{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> cnn_ensemble.py\n",
    "* model architecture\n",
    "* cnn training\n",
    "* feature extraction for training + test\n",
    "* training svm, random forest on training\n",
    "* classification using heuristic ensembling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation\n",
    "from keras.layers import Dropout, LeakyReLU       \n",
    "from keras.activations import relu, softmax, tanh, hard_sigmoid\n",
    "\n",
    "import pydot, graphviz\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "import h5py\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hist_equ(img_path = '', show = False, img = None):\n",
    "    if img is None:\n",
    "        img = cv2.imread(data_path + img_path)\n",
    "\n",
    "    img_yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV) # RGB to YUB color space\n",
    "\n",
    "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0]) # histogram equalization in Y channel\n",
    "\n",
    "    equ = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2BGR) # inverse transform\n",
    "    \n",
    "    if show:    \n",
    "        comp = np.hstack((img,equ)) # horizontal stack\n",
    "        cv2.imwrite('original.bmp',img)\n",
    "        cv2.imwrite('hist_equ.bmp',equ)\n",
    "        cv2.imwrite('comp.bmp',comp)\n",
    "        plt.imshow(comp)\n",
    "        plt.show()\n",
    "    return equ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    # build a lookup table mapping the pixel values [0, 255] to\n",
    "    # their adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    " \n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_segmented_images(_class,_id,data_split,flag_show):\n",
    "\n",
    "    _class = str(_class)\n",
    "    _id = str(_id)\n",
    "    \n",
    "    imgpath = 'PH2 Dataset images/'+'/IMD'+_id+'/IMD'+_id+'_Dermoscopic_Image/IMD'+_id+'.bmp'  \n",
    "    maskpath = 'PH2 Dataset images/'+'/IMD'+_id+'/IMD'+_id+'_lesion/IMD'+_id+'_lesion.bmp'\n",
    "    img2 = cv2.imread(imgpath)\n",
    "    img = hist_equ(img = img2)\n",
    "    img = adjust_gamma(img, 1.55)\n",
    "    msk = cv2.imread(maskpath)\n",
    "    \n",
    "    #cv2.imshow('Main Image',img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    if flag_show:\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        #print(img.shape)\n",
    "\n",
    "        plt.imshow(msk)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    msk = msk/255 # normalizing\n",
    "    max_channels = np.amax([np.amax(msk[:,:,0]), np.amax(msk[:,:,1]), np.amax(msk[:,:,2])])\n",
    "    \n",
    "    msk = np.ndarray.astype(msk, dtype='uint8')\n",
    "    \n",
    "    gen_img = img*msk\n",
    "    dirc = 'F:/Research important/Thesis/Active Phase/Part1 Scin Cancer/PH2Dataset experiments/classification/' + data_split\n",
    "    dirc = os.path.join(dirc,_class)\n",
    "    if not os.path.exists(dirc):\n",
    "        os.makedirs(dirc)\n",
    "        \n",
    "    gen_img_name =  dirc + '/' + _id + '.png'\n",
    "    cv2.imwrite(gen_img_name,gen_img)\n",
    "    if flag_show:\n",
    "        print(max_channels)\n",
    "        plt.imshow(gen_img)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Image Name', 'Histological Diagnosis', 'Common Nevus',\n",
      "       'Atypical Nevus', 'Melanoma', 'Asymmetry\\n(0/1/2)',\n",
      "       'Pigment Network\\n(AT/T)', 'Dots/Globules\\n(A/AT/T)', 'Streaks\\n(A/P)',\n",
      "       'Regression Areas\\n(A/P)', 'Blue-Whitish Veil\\n(A/P)', 'White', 'Red',\n",
      "       'Light-Brown', 'Dark-Brown', 'Blue-Gray', 'Black'],\n",
      "      dtype='object')\n",
      "IMD003\n",
      "X\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image Name</th>\n",
       "      <th>Common Nevus</th>\n",
       "      <th>Atypical Nevus</th>\n",
       "      <th>Melanoma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMD009</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMD016</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMD022</td>\n",
       "      <td>X</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Image Name Common Nevus Atypical Nevus Melanoma\n",
       "1     IMD009            X            NaN      NaN\n",
       "2     IMD016            X            NaN      NaN\n",
       "3     IMD022            X            NaN      NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df = pandas.read_excel('PH2_dataset.xlsx') # The excel file was modified, unnecessary info was removed\n",
    "#print the column names\n",
    "print (df.columns)\n",
    "#get the values for a given column\n",
    "values = df['Image Name'].values\n",
    "#get a data frame with selected columns\n",
    "FORMAT = ['Image Name', 'Common Nevus', 'Atypical Nevus', 'Melanoma']\n",
    "df_selected = df[FORMAT]\n",
    "print(df_selected.loc[0][0])\n",
    "print(df_selected.loc[0][1])\n",
    "print(type(df_selected.loc[0][0]))\n",
    "df_selected[1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'003'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_selected.loc[0][0][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 200\n",
    "class0 = []\n",
    "class1 = []\n",
    "class2 = []\n",
    "for ns in range(num_samples):\n",
    "    img_id = df_selected.loc[ns][0][-3:]\n",
    "    com_nev = df_selected.loc[ns][1]\n",
    "    atyp_nev = df_selected.loc[ns][2]\n",
    "    melan = df_selected.loc[ns][3]\n",
    "    _class = -1\n",
    "    if com_nev == 'X':\n",
    "        _class = 0\n",
    "        class0.append(img_id)\n",
    "    elif atyp_nev == 'X':\n",
    "        _class = 1\n",
    "        class1.append(img_id)\n",
    "    elif melan == 'X':\n",
    "        _class = 2\n",
    "        class2.append(img_id)\n",
    "    #print(_class)\n",
    "    #generate_segmented_images(_class,img_id,flag_show = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['045', '156', '044', '372', '146', '147', '105', '125', '379', '381', '025', '020', '107', '198', '204', '134', '159', '150', '203', '374', '010', '050', '390', '200', '160', '389', '385', '173', '003', '009', '152', '400', '176', '197', '112', '402', '035', '364', '132', '142', '384', '133', '392', '367', '182', '375', '395', '394', '092', '371', '016', '039', '206', '022', '041', '397', '380', '135', '143', '103', '365', '207', '378', '101', '383', '024', '144', '017']\n",
      "['199', '038', '162', '177', '208', '108', '196', '399', '118', '161', '042', '175']\n",
      "['032', '149', '437', '434', '398', '393', '076', '386', '037', '251', '036', '049', '370', '210', '433', '431', '356', '328', '013', '388', '171', '002', '040', '164', '153', '256', '278', '120', '436', '008', '157', '043', '057', '304', '139', '368', '018', '279', '004', '033', '312', '369', '031', '047', '019', '280', '138', '305', '427', '030', '396', '166', '169', '155', '126', '027', '140', '339', '430', '226', '306', '014', '021', '075', '137', '006', '015', '382']\n",
      "['154', '376', '243', '078', '432', '331', '347', '048', '023', '170', '360', '254']\n",
      "['426', '284', '420', '405', '088', '240', '211', '219', '408', '168', '091', '348', '413', '407', '425', '429', '065', '090', '085', '409', '080', '424', '063', '423', '410', '419', '404', '058', '349', '406', '435', '064', '403', '061']\n",
      "['421', '417', '242', '285', '411', '418']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1997) # to reproduce results\n",
    "\n",
    "train_split = 0.85\n",
    "random.shuffle(class0)\n",
    "random.shuffle(class1)\n",
    "random.shuffle(class2)\n",
    "\n",
    "train_0 = class0[0:int(len(class0)*train_split)]\n",
    "test_0 = class0[int(len(class0)*train_split):]\n",
    "print(train_0)\n",
    "print(test_0)\n",
    "\n",
    "train_1 = class1[0:int(len(class1)*train_split)]\n",
    "test_1 = class1[int(len(class1)*train_split):]\n",
    "print(train_1)\n",
    "print(test_1)\n",
    "\n",
    "train_2 = class2[0:int(len(class2)*train_split)]\n",
    "test_2 = class2[int(len(class2)*train_split):]\n",
    "print(train_2)\n",
    "print(test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "40\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(class0))\n",
    "print(len(train_0) + len(test_0))\n",
    "\n",
    "print(len(class1))\n",
    "print(len(train_1) + len(test_1))\n",
    "\n",
    "print(len(class2))\n",
    "print(len(train_2) + len(test_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_0)):\n",
    "    generate_segmented_images(0,train_0[i],'train',flag_show = False)\n",
    "\n",
    "for i in range(len(test_0)):\n",
    "    generate_segmented_images(0,test_0[i],'test',flag_show = False)\n",
    "    \n",
    "for i in range(len(train_1)):\n",
    "    generate_segmented_images(1,train_1[i],'train',flag_show = False)\n",
    "\n",
    "for i in range(len(test_1)):\n",
    "    generate_segmented_images(1,test_1[i],'test',flag_show = False)\n",
    "    \n",
    "    \n",
    "for i in range(len(train_2)):\n",
    "    generate_segmented_images(2,train_2[i],'train',flag_show = False)\n",
    "\n",
    "for i in range(len(test_2)):\n",
    "    generate_segmented_images(2,test_2[i],'test',flag_show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "model.add(BatchNormalization()) # BN on :      BN off : \n",
    "model.add(LeakyReLU(alpha=0.1)) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5)))     # kernel size :\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(BatchNormalization()) # BN on :      BN off : \n",
    "model.add(LeakyReLU(alpha=0.1)) # sigmoid :      relu :      tanh :\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 221, 221, 16)      64        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 217, 217, 32)      12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 108, 108, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 108, 108, 32)      128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 108, 108, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 373248)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               47775872  \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 47,790,255\n",
      "Trainable params: 47,789,897\n",
      "Non-trainable params: 358\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DIR = 'train'\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Found 145 images belonging to 3 classes.\n",
      "Found 25 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum = 0.9, nesterov = True),\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorboard\n",
    "> tensorboard --logdir ./Graph\n",
    "\n",
    "> htttp://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.0943 - acc: 0.5203 - val_loss: 0.7161 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.56757 to 0.59459, saving model to best_weight.hdf5\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 158s 3s/step - loss: 1.0757 - acc: 0.5349 - val_loss: 0.6813 - val_acc: 0.7297\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59459 to 0.72973, saving model to best_weight.hdf5\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 165s 3s/step - loss: 1.0519 - acc: 0.5302 - val_loss: 0.7100 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 152s 3s/step - loss: 1.0066 - acc: 0.5253 - val_loss: 0.7110 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 146s 3s/step - loss: 0.9445 - acc: 0.5699 - val_loss: 0.8151 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.0121 - acc: 0.5652 - val_loss: 0.6837 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 151s 3s/step - loss: 0.8781 - acc: 0.6054 - val_loss: 1.0018 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 132s 3s/step - loss: 0.9299 - acc: 0.6252 - val_loss: 0.7559 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 131s 3s/step - loss: 0.8868 - acc: 0.6501 - val_loss: 0.7803 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 134s 3s/step - loss: 0.9981 - acc: 0.6100 - val_loss: 0.8723 - val_acc: 0.6216\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2600000390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                       steps_per_epoch=50,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=10,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 2\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "model2.add(BatchNormalization()) # BN on :      BN off : \n",
    "model2.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model2.add(Conv2D(32, (3, 3)))     # kernel size :\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model2.add(BatchNormalization()) # BN on :      BN off : \n",
    "model2.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(128))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.2))\n",
    "\n",
    "model2.add(Dense(3))\n",
    "model2.add(BatchNormalization())\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 221, 221, 16)      64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 219, 219, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 109, 109, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 380192)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               48664704  \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 3)                 12        \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 48,670,895\n",
      "Trainable params: 48,670,537\n",
      "Non-trainable params: 358\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 3 classes.\n",
      "Found 25 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "model2.compile(loss=categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum = 0.9, nesterov = True),\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight2.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 156s 3s/step - loss: 1.2870 - acc: 0.3902 - val_loss: 0.9559 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48649, saving model to best_weight2.hdf5\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 136s 3s/step - loss: 1.0451 - acc: 0.4851 - val_loss: 0.8594 - val_acc: 0.2973\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 144s 3s/step - loss: 1.0871 - acc: 0.5252 - val_loss: 0.7211 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.48649 to 0.59459, saving model to best_weight2.hdf5\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 131s 3s/step - loss: 0.9484 - acc: 0.6004 - val_loss: 0.8611 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 142s 3s/step - loss: 0.8213 - acc: 0.6849 - val_loss: 0.7041 - val_acc: 0.7568\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59459 to 0.75676, saving model to best_weight2.hdf5\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 155s 3s/step - loss: 0.8605 - acc: 0.6502 - val_loss: 0.8346 - val_acc: 0.6216\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 158s 3s/step - loss: 0.8300 - acc: 0.6899 - val_loss: 1.7235 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 120s 2s/step - loss: 0.7154 - acc: 0.7602 - val_loss: 1.1805 - val_acc: 0.4324\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 133s 3s/step - loss: 0.6086 - acc: 0.7752 - val_loss: 0.8353 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 128s 3s/step - loss: 0.5625 - acc: 0.7852 - val_loss: 0.8909 - val_acc: 0.4324\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2600a1cda0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model2.fit_generator(train_generator,\n",
    "                       steps_per_epoch=50,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=10,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 3\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model3.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model3.add(Conv2D(32, (3, 3)))     # kernel size :\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model3.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Dense(3))\n",
    "#model2.add(BatchNormalization())\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 219, 219, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 380192)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               48664704  \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 48,670,179\n",
      "Trainable params: 48,670,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 3 classes.\n",
      "Found 25 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "optm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model3.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optm,\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 2/50 [>.............................] - ETA: 8:31 - loss: 8.6502 - acc: 0.1250 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (3.935483). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 294s 6s/step - loss: 5.8614 - acc: 0.4348 - val_loss: 0.9407 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56757, saving model to best_weight3.hdf5\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 103s 2s/step - loss: 1.2575 - acc: 0.5951 - val_loss: 0.8725 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 102s 2s/step - loss: 0.3148 - acc: 0.9100 - val_loss: 0.8215 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 118s 2s/step - loss: 0.0275 - acc: 1.0000 - val_loss: 1.1000 - val_acc: 0.4595\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 0.0524 - acc: 0.9850 - val_loss: 1.4301 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 124s 2s/step - loss: 0.0121 - acc: 0.9950 - val_loss: 1.3986 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.56757 to 0.64865, saving model to best_weight3.hdf5\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 0.0027 - acc: 1.0000 - val_loss: 1.2461 - val_acc: 0.5135\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 9.4326e-04 - acc: 1.0000 - val_loss: 1.2813 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 9.6808e-04 - acc: 1.0000 - val_loss: 1.2527 - val_acc: 0.4595\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 7.0398e-04 - acc: 1.0000 - val_loss: 1.5094 - val_acc: 0.4054\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26003497b8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model3.fit_generator(train_generator,\n",
    "                       steps_per_epoch=50,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=10,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 4\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model4.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "model4.add(Conv2D(32, (3, 3)))     # kernel size :\n",
    "model4.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model4.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(128))\n",
    "#model3.add(BatchNormalization())\n",
    "model4.add(Activation('sigmoid'))\n",
    "model4.add(Dropout(0.5))\n",
    "\n",
    "model4.add(Dense(3))\n",
    "#model2.add(BatchNormalization())\n",
    "model4.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 219, 219, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 380192)            0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               48664704  \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 48,670,179\n",
      "Trainable params: 48,670,179\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 3 classes.\n",
      "Found 25 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "optm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model4.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optm,\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight4.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 150s 3s/step - loss: 1.5557 - acc: 0.3999 - val_loss: 1.0864 - val_acc: 0.4865\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48649, saving model to best_weight4.hdf5\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 123s 2s/step - loss: 1.4427 - acc: 0.3850 - val_loss: 1.0994 - val_acc: 0.3784\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 123s 2s/step - loss: 1.6505 - acc: 0.3252 - val_loss: 1.0872 - val_acc: 0.3243\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 122s 2s/step - loss: 1.5165 - acc: 0.3749 - val_loss: 0.9820 - val_acc: 0.4324\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 133s 3s/step - loss: 1.3585 - acc: 0.5202 - val_loss: 0.8356 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.48649 to 0.56757, saving model to best_weight4.hdf5\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 125s 3s/step - loss: 1.1643 - acc: 0.5149 - val_loss: 0.8558 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 119s 2s/step - loss: 1.2600 - acc: 0.4447 - val_loss: 0.7521 - val_acc: 0.6757\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.56757 to 0.67568, saving model to best_weight4.hdf5\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 125s 2s/step - loss: 1.2615 - acc: 0.4701 - val_loss: 0.7859 - val_acc: 0.5946\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 126s 3s/step - loss: 1.2304 - acc: 0.4848 - val_loss: 0.8458 - val_acc: 0.5405\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 120s 2s/step - loss: 1.3261 - acc: 0.4700 - val_loss: 0.8410 - val_acc: 0.5676\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x260137ad68>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model4.fit_generator(train_generator,\n",
    "                       steps_per_epoch=50,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=10,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 5\n",
    "model5 = Sequential()\n",
    "\n",
    "model5.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "#model5.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model5.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "model5.add(Dropout(0.1))\n",
    "\n",
    "model5.add(Conv2D(32, (3, 3)))     # kernel size :\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model5.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "model5.add(Dropout(0.1))\n",
    "\n",
    "model5.add(Conv2D(64, (3, 3)))     # kernel size :\n",
    "model5.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model5.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "model5.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(64))\n",
    "#model3.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model5.add(Dense(128))\n",
    "#model3.add(BatchNormalization())\n",
    "model5.add(Activation('relu'))\n",
    "model5.add(Dropout(0.4))\n",
    "\n",
    "model5.add(Dense(3))\n",
    "#model2.add(BatchNormalization())\n",
    "model5.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 222, 222, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 222, 222, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 220, 220, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 110, 110, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 108, 108, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 54, 54, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 186624)            0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 64)                11944000  \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 11,976,291\n",
      "Trainable params: 11,976,291\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 3 classes.\n",
      "Found 25 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "optm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model5.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optm,\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight5.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE*2, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE*2, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 118s 2s/step - loss: 1.2389 - acc: 0.2553 - val_loss: 0.9896 - val_acc: 0.5909\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59091, saving model to best_weight5.hdf5\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 125s 2s/step - loss: 1.0243 - acc: 0.5545 - val_loss: 0.7764 - val_acc: 0.5606\n",
      "\n",
      "Epoch 00002: val_acc did not improve\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 95s 2s/step - loss: 0.7134 - acc: 0.6523 - val_loss: 1.1981 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 95s 2s/step - loss: 0.7397 - acc: 0.6876 - val_loss: 0.9024 - val_acc: 0.5303\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 92s 2s/step - loss: 0.4160 - acc: 0.8098 - val_loss: 1.1161 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.59091 to 0.69697, saving model to best_weight5.hdf5\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 97s 2s/step - loss: 0.2734 - acc: 0.8799 - val_loss: 1.4433 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 95s 2s/step - loss: 0.2399 - acc: 0.8978 - val_loss: 1.4653 - val_acc: 0.5000\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 98s 2s/step - loss: 0.2118 - acc: 0.9350 - val_loss: 1.3423 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 96s 2s/step - loss: 0.1719 - acc: 0.9203 - val_loss: 1.6427 - val_acc: 0.4697\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 110s 2s/step - loss: 0.1628 - acc: 0.9399 - val_loss: 1.7736 - val_acc: 0.5152\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2612700f98>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model5.fit_generator(train_generator,\n",
    "                       steps_per_epoch=50,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=10,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv2d_15\n",
      "1 activation_23\n",
      "2 dropout_15\n",
      "3 conv2d_16\n",
      "4 max_pooling2d_13\n",
      "5 activation_24\n",
      "6 dropout_16\n",
      "7 conv2d_17\n",
      "8 max_pooling2d_14\n",
      "9 activation_25\n",
      "10 dropout_17\n",
      "11 flatten_8\n",
      "12 dense_12\n",
      "13 activation_26\n",
      "14 dropout_18\n",
      "15 dense_13\n",
      "16 activation_27\n",
      "17 dropout_19\n",
      "18 dense_14\n",
      "19 activation_28\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(model5.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract features from an arbitrary intermediate layer\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "model_fe = Model(inputs=model5.input, outputs=model5.get_layer('dense_13').output)\n",
    "\n",
    "# load an image and preprocess it\n",
    "img_path = 'test.bmp'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "# get the features \n",
    "features = model_fe.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -258.47922    -168.22876      32.303875    628.96124     298.69937\n",
      "   -190.73158    -369.96774      86.12593      54.811954    134.03952\n",
      "   -128.10455     497.63858     804.23627    -313.0369     -820.4245\n",
      "    279.30426    1205.5005      324.50034     -16.94054    -661.7363\n",
      "    497.626      -486.84073     173.10646    -479.46912    -401.12036\n",
      "    504.56284     336.819       150.16965    -433.0042        7.2921257\n",
      "    428.5451       -1.2626623   203.04216     899.2101     -442.60178\n",
      "     16.11631     552.1154     -612.38477    -328.64893    -350.64658\n",
      "   -463.8479     -213.48932    1206.1561       10.02821     803.36426\n",
      "    -17.090204   -740.31384    -751.6819     -144.64534     686.55676\n",
      "   -336.5051     -787.98816     405.46695     657.82855      12.142002\n",
      "    146.71025    -555.3851      493.86185      86.37762     -17.741146\n",
      "   -493.24414    -323.51         32.564068   -831.3529      194.96825\n",
      "   -306.36255     262.69498     539.1898      320.4866      269.64627\n",
      "   -276.20078     224.54323     736.5716       49.415497   -201.01369\n",
      "    755.37445     560.21704     174.93394    -869.37823     329.05496\n",
      "    830.43243      63.21299     294.67984    -187.88655     293.50827\n",
      "    -74.766304   -184.37909     258.462      -634.9188      735.2281\n",
      "   -379.93814     681.2806      254.49792     -22.279951    626.3175\n",
      "    345.79794    -593.06995     626.2414     -218.56322    -196.56447\n",
      "   -650.1303      -86.73957    -376.40994    -220.79825     470.64932\n",
      "   -190.30638     -57.822247    170.39023   -1085.1068       27.982033\n",
      "    328.02237    -761.2037       27.05963    -269.93448    -129.41405\n",
      "    972.30505     213.69882       7.3268924  -255.10338     206.76054\n",
      "    171.36188    -211.51163     338.5872     -757.8446      949.69147\n",
      "   -386.8615     -601.29004     214.70766  ]]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 170 image(s) found.\n",
      "Output directory set to train\\output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=764x576 at 0xC02F696550>: 100%|█| 2720/2720 [08:48<00:00,  3.73 Samples/s]\n"
     ]
    }
   ],
   "source": [
    "import Augmentor\n",
    "p = Augmentor.Pipeline(\"train\")\n",
    "p.random_distortion(probability = 1, grid_width = 3, grid_height = 3, magnitude = 10)\n",
    "p.flip_random(probability = 1)\n",
    "p.rotate_without_crop(probability=1, max_left_rotation=360, max_right_rotation=360, expand=False)\n",
    "p.skew(probability=1, magnitude=0.8)\n",
    "p.sample(2720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 6\n",
    "model6 = Sequential()\n",
    "\n",
    "model6.add(Conv2D(16, kernel_size=(3, 3),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model6.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "model6.add(Conv2D(32, (3, 3)))     # kernel size :\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model6.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "model6.add(Dropout(0.1))\n",
    "\n",
    "model6.add(Conv2D(64, (3, 3)))     # kernel size :\n",
    "model6.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model6.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "model6.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(64))\n",
    "#model3.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Dropout(0.4))\n",
    "\n",
    "\n",
    "model6.add(Dense(128))\n",
    "#model3.add(BatchNormalization())\n",
    "model6.add(Activation('relu'))\n",
    "model6.add(Dropout(0.5))\n",
    "\n",
    "model6.add(Dense(3))\n",
    "#model2.add(BatchNormalization())\n",
    "model6.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 222, 222, 16)      448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 221, 221, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 219, 219, 32)      4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 107, 107, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 53, 53, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 179776)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                11505728  \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 387       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 11,538,019\n",
      "Trainable params: 11,538,019\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Found 2457 images belonging to 3 classes.\n",
      "Found 433 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "optm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2 = 0.999, epsilon=None, decay=0.0, amsgrad=True)\n",
    "model6.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optm,\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight6.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE*2, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=1997,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE*2, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "204/307 [==================>...........] - ETA: 12:45 - loss: 1.1022 - acc: 0.50 - ETA: 11:54 - loss: 0.8405 - acc: 0.62 - ETA: 11:25 - loss: 0.8849 - acc: 0.58 - ETA: 11:05 - loss: 0.8804 - acc: 0.56 - ETA: 10:49 - loss: 0.8927 - acc: 0.55 - ETA: 10:49 - loss: 0.8597 - acc: 0.54 - ETA: 10:48 - loss: 0.8753 - acc: 0.53 - ETA: 10:46 - loss: 0.9555 - acc: 0.53 - ETA: 10:40 - loss: 0.9392 - acc: 0.52 - ETA: 10:34 - loss: 0.9486 - acc: 0.52 - ETA: 10:28 - loss: 0.9638 - acc: 0.48 - ETA: 10:23 - loss: 0.9799 - acc: 0.50 - ETA: 10:17 - loss: 1.0533 - acc: 0.48 - ETA: 10:13 - loss: 1.0739 - acc: 0.47 - ETA: 10:11 - loss: 1.0464 - acc: 0.48 - ETA: 10:07 - loss: 1.0485 - acc: 0.48 - ETA: 10:05 - loss: 1.0305 - acc: 0.50 - ETA: 10:02 - loss: 1.0344 - acc: 0.51 - ETA: 9:59 - loss: 1.0519 - acc: 0.5000 - ETA: 9:56 - loss: 1.0382 - acc: 0.506 - ETA: 9:52 - loss: 1.0326 - acc: 0.494 - ETA: 9:49 - loss: 1.0358 - acc: 0.494 - ETA: 9:46 - loss: 1.0322 - acc: 0.483 - ETA: 9:44 - loss: 1.0349 - acc: 0.489 - ETA: 9:42 - loss: 1.0322 - acc: 0.495 - ETA: 9:40 - loss: 1.0411 - acc: 0.495 - ETA: 9:37 - loss: 1.0431 - acc: 0.486 - ETA: 9:34 - loss: 1.0420 - acc: 0.482 - ETA: 9:32 - loss: 1.0377 - acc: 0.491 - ETA: 9:29 - loss: 1.0307 - acc: 0.491 - ETA: 9:26 - loss: 1.0217 - acc: 0.500 - ETA: 9:24 - loss: 1.0290 - acc: 0.503 - ETA: 9:22 - loss: 1.0275 - acc: 0.515 - ETA: 9:21 - loss: 1.0263 - acc: 0.514 - ETA: 9:22 - loss: 1.0270 - acc: 0.517 - ETA: 9:21 - loss: 1.0245 - acc: 0.527 - ETA: 9:22 - loss: 1.0224 - acc: 0.537 - ETA: 9:22 - loss: 1.0286 - acc: 0.542 - ETA: 9:20 - loss: 1.0290 - acc: 0.541 - ETA: 9:18 - loss: 1.0267 - acc: 0.540 - ETA: 9:16 - loss: 1.0292 - acc: 0.533 - ETA: 9:15 - loss: 1.0274 - acc: 0.532 - ETA: 9:13 - loss: 1.0244 - acc: 0.534 - ETA: 9:11 - loss: 1.0338 - acc: 0.534 - ETA: 9:08 - loss: 1.0330 - acc: 0.530 - ETA: 9:06 - loss: 1.0368 - acc: 0.527 - ETA: 9:04 - loss: 1.0298 - acc: 0.529 - ETA: 9:02 - loss: 1.0210 - acc: 0.531 - ETA: 8:59 - loss: 1.0239 - acc: 0.530 - ETA: 8:58 - loss: 1.0336 - acc: 0.525 - ETA: 8:58 - loss: 1.0269 - acc: 0.527 - ETA: 8:57 - loss: 1.0187 - acc: 0.531 - ETA: 8:56 - loss: 1.0140 - acc: 0.533 - ETA: 8:55 - loss: 1.0118 - acc: 0.525 - ETA: 8:54 - loss: 1.0142 - acc: 0.525 - ETA: 8:53 - loss: 1.0135 - acc: 0.520 - ETA: 8:54 - loss: 1.0127 - acc: 0.521 - ETA: 8:52 - loss: 1.0112 - acc: 0.523 - ETA: 8:51 - loss: 1.0131 - acc: 0.521 - ETA: 8:51 - loss: 1.0121 - acc: 0.522 - ETA: 8:52 - loss: 1.0109 - acc: 0.522 - ETA: 8:51 - loss: 1.0142 - acc: 0.518 - ETA: 8:50 - loss: 1.0166 - acc: 0.515 - ETA: 8:48 - loss: 1.0197 - acc: 0.517 - ETA: 8:46 - loss: 1.0276 - acc: 0.515 - ETA: 8:46 - loss: 1.0254 - acc: 0.515 - ETA: 8:44 - loss: 1.0238 - acc: 0.518 - ETA: 8:42 - loss: 1.0242 - acc: 0.523 - ETA: 8:39 - loss: 1.0246 - acc: 0.523 - ETA: 8:36 - loss: 1.0215 - acc: 0.525 - ETA: 8:34 - loss: 1.0278 - acc: 0.524 - ETA: 8:32 - loss: 1.0270 - acc: 0.526 - ETA: 8:29 - loss: 1.0314 - acc: 0.524 - ETA: 8:27 - loss: 1.0363 - acc: 0.522 - ETA: 8:25 - loss: 1.0320 - acc: 0.523 - ETA: 8:22 - loss: 1.0279 - acc: 0.524 - ETA: 8:20 - loss: 1.0230 - acc: 0.530 - ETA: 8:19 - loss: 1.0209 - acc: 0.530 - ETA: 8:18 - loss: 1.0177 - acc: 0.531 - ETA: 8:16 - loss: 1.0149 - acc: 0.528 - ETA: 8:15 - loss: 1.0110 - acc: 0.526 - ETA: 8:13 - loss: 1.0088 - acc: 0.527 - ETA: 8:11 - loss: 1.0083 - acc: 0.525 - ETA: 8:09 - loss: 1.0063 - acc: 0.523 - ETA: 8:08 - loss: 1.0057 - acc: 0.523 - ETA: 8:07 - loss: 1.0096 - acc: 0.526 - ETA: 8:01 - loss: 1.0091 - acc: 0.520 - ETA: 8:00 - loss: 1.0173 - acc: 0.517 - ETA: 7:59 - loss: 1.0166 - acc: 0.519 - ETA: 7:59 - loss: 1.0152 - acc: 0.519 - ETA: 7:59 - loss: 1.0150 - acc: 0.519 - ETA: 7:57 - loss: 1.0125 - acc: 0.520 - ETA: 7:55 - loss: 1.0138 - acc: 0.518 - ETA: 7:54 - loss: 1.0097 - acc: 0.521 - ETA: 7:51 - loss: 1.0086 - acc: 0.521 - ETA: 7:49 - loss: 1.0092 - acc: 0.519 - ETA: 7:47 - loss: 1.0066 - acc: 0.520 - ETA: 7:45 - loss: 1.0078 - acc: 0.517 - ETA: 7:43 - loss: 1.0048 - acc: 0.518 - ETA: 7:41 - loss: 1.0024 - acc: 0.520 - ETA: 7:39 - loss: 1.0004 - acc: 0.521 - ETA: 7:37 - loss: 0.9969 - acc: 0.524 - ETA: 7:35 - loss: 0.9955 - acc: 0.524 - ETA: 7:33 - loss: 0.9940 - acc: 0.524 - ETA: 7:31 - loss: 0.9918 - acc: 0.523 - ETA: 7:29 - loss: 0.9937 - acc: 0.522 - ETA: 7:26 - loss: 0.9981 - acc: 0.522 - ETA: 7:24 - loss: 0.9961 - acc: 0.523 - ETA: 7:21 - loss: 0.9963 - acc: 0.522 - ETA: 7:19 - loss: 0.9968 - acc: 0.522 - ETA: 7:16 - loss: 0.9983 - acc: 0.522 - ETA: 7:14 - loss: 0.9981 - acc: 0.522 - ETA: 7:12 - loss: 0.9954 - acc: 0.524 - ETA: 7:09 - loss: 0.9940 - acc: 0.526 - ETA: 7:07 - loss: 0.9930 - acc: 0.525 - ETA: 7:05 - loss: 0.9935 - acc: 0.524 - ETA: 7:02 - loss: 0.9913 - acc: 0.525 - ETA: 7:00 - loss: 0.9873 - acc: 0.527 - ETA: 6:58 - loss: 0.9866 - acc: 0.528 - ETA: 6:56 - loss: 0.9873 - acc: 0.527 - ETA: 6:54 - loss: 0.9848 - acc: 0.527 - ETA: 6:51 - loss: 0.9824 - acc: 0.529 - ETA: 6:49 - loss: 0.9846 - acc: 0.529 - ETA: 6:47 - loss: 0.9836 - acc: 0.531 - ETA: 6:45 - loss: 0.9818 - acc: 0.531 - ETA: 6:43 - loss: 0.9797 - acc: 0.532 - ETA: 6:40 - loss: 0.9791 - acc: 0.532 - ETA: 6:38 - loss: 0.9806 - acc: 0.530 - ETA: 6:36 - loss: 0.9779 - acc: 0.530 - ETA: 6:34 - loss: 0.9793 - acc: 0.530 - ETA: 6:31 - loss: 0.9772 - acc: 0.531 - ETA: 6:29 - loss: 0.9746 - acc: 0.532 - ETA: 6:27 - loss: 0.9723 - acc: 0.534 - ETA: 6:25 - loss: 0.9737 - acc: 0.530 - ETA: 6:23 - loss: 0.9714 - acc: 0.529 - ETA: 6:20 - loss: 0.9694 - acc: 0.530 - ETA: 6:18 - loss: 0.9719 - acc: 0.527 - ETA: 6:16 - loss: 0.9715 - acc: 0.526 - ETA: 6:14 - loss: 0.9690 - acc: 0.526 - ETA: 6:12 - loss: 0.9693 - acc: 0.525 - ETA: 6:09 - loss: 0.9682 - acc: 0.526 - ETA: 6:07 - loss: 0.9674 - acc: 0.527 - ETA: 6:05 - loss: 0.9683 - acc: 0.526 - ETA: 6:02 - loss: 0.9688 - acc: 0.527 - ETA: 6:00 - loss: 0.9671 - acc: 0.529 - ETA: 5:58 - loss: 0.9645 - acc: 0.530 - ETA: 5:56 - loss: 0.9629 - acc: 0.534 - ETA: 5:54 - loss: 0.9638 - acc: 0.531 - ETA: 5:51 - loss: 0.9631 - acc: 0.531 - ETA: 5:49 - loss: 0.9628 - acc: 0.530 - ETA: 5:47 - loss: 0.9626 - acc: 0.531 - ETA: 5:45 - loss: 0.9611 - acc: 0.532 - ETA: 5:43 - loss: 0.9601 - acc: 0.532 - ETA: 5:40 - loss: 0.9582 - acc: 0.532 - ETA: 5:38 - loss: 0.9551 - acc: 0.533 - ETA: 5:36 - loss: 0.9540 - acc: 0.532 - ETA: 5:34 - loss: 0.9544 - acc: 0.532 - ETA: 5:31 - loss: 0.9530 - acc: 0.534 - ETA: 5:29 - loss: 0.9518 - acc: 0.532 - ETA: 5:27 - loss: 0.9496 - acc: 0.533 - ETA: 5:25 - loss: 0.9507 - acc: 0.532 - ETA: 5:22 - loss: 0.9490 - acc: 0.532 - ETA: 5:20 - loss: 0.9475 - acc: 0.532 - ETA: 5:19 - loss: 0.9474 - acc: 0.531 - ETA: 5:17 - loss: 0.9449 - acc: 0.532 - ETA: 5:15 - loss: 0.9424 - acc: 0.534 - ETA: 5:13 - loss: 0.9431 - acc: 0.533 - ETA: 5:11 - loss: 0.9417 - acc: 0.534 - ETA: 5:09 - loss: 0.9404 - acc: 0.535 - ETA: 5:07 - loss: 0.9386 - acc: 0.536 - ETA: 5:05 - loss: 0.9375 - acc: 0.536 - ETA: 5:03 - loss: 0.9372 - acc: 0.536 - ETA: 5:01 - loss: 0.9363 - acc: 0.535 - ETA: 4:59 - loss: 0.9356 - acc: 0.535 - ETA: 4:56 - loss: 0.9376 - acc: 0.534 - ETA: 4:54 - loss: 0.9368 - acc: 0.534 - ETA: 4:52 - loss: 0.9407 - acc: 0.533 - ETA: 4:50 - loss: 0.9402 - acc: 0.532 - ETA: 4:48 - loss: 0.9396 - acc: 0.531 - ETA: 4:46 - loss: 0.9407 - acc: 0.531 - ETA: 4:44 - loss: 0.9404 - acc: 0.531 - ETA: 4:42 - loss: 0.9386 - acc: 0.532 - ETA: 4:40 - loss: 0.9376 - acc: 0.532 - ETA: 4:38 - loss: 0.9406 - acc: 0.531 - ETA: 4:36 - loss: 0.9403 - acc: 0.531 - ETA: 4:34 - loss: 0.9397 - acc: 0.532 - ETA: 4:32 - loss: 0.9397 - acc: 0.533 - ETA: 4:30 - loss: 0.9390 - acc: 0.533 - ETA: 4:28 - loss: 0.9394 - acc: 0.533 - ETA: 4:25 - loss: 0.9397 - acc: 0.534 - ETA: 4:23 - loss: 0.9399 - acc: 0.533 - ETA: 4:21 - loss: 0.9402 - acc: 0.533 - ETA: 4:19 - loss: 0.9434 - acc: 0.532 - ETA: 4:17 - loss: 0.9416 - acc: 0.534 - ETA: 4:15 - loss: 0.9416 - acc: 0.535 - ETA: 4:13 - loss: 0.9413 - acc: 0.536 - ETA: 4:10 - loss: 0.9408 - acc: 0.536 - ETA: 4:08 - loss: 0.9417 - acc: 0.536 - ETA: 4:06 - loss: 0.9443 - acc: 0.534 - ETA: 4:04 - loss: 0.9457 - acc: 0.533 - ETA: 4:02 - loss: 0.9448 - acc: 0.533 - ETA: 4:00 - loss: 0.9441 - acc: 0.532 - ETA: 3:58 - loss: 0.9442 - acc: 0.533 - ETA: 3:56 - loss: 0.9451 - acc: 0.534307/307 [==============================] - ETA: 3:54 - loss: 0.9448 - acc: 0.533 - ETA: 3:51 - loss: 0.9436 - acc: 0.533 - ETA: 3:49 - loss: 0.9435 - acc: 0.532 - ETA: 3:47 - loss: 0.9436 - acc: 0.531 - ETA: 3:45 - loss: 0.9428 - acc: 0.532 - ETA: 3:42 - loss: 0.9424 - acc: 0.531 - ETA: 3:40 - loss: 0.9417 - acc: 0.531 - ETA: 3:38 - loss: 0.9412 - acc: 0.531 - ETA: 3:35 - loss: 0.9409 - acc: 0.530 - ETA: 3:33 - loss: 0.9408 - acc: 0.531 - ETA: 3:31 - loss: 0.9402 - acc: 0.532 - ETA: 3:29 - loss: 0.9394 - acc: 0.532 - ETA: 3:26 - loss: 0.9388 - acc: 0.532 - ETA: 3:24 - loss: 0.9370 - acc: 0.532 - ETA: 3:22 - loss: 0.9359 - acc: 0.533 - ETA: 3:20 - loss: 0.9351 - acc: 0.533 - ETA: 3:18 - loss: 0.9337 - acc: 0.532 - ETA: 3:16 - loss: 0.9331 - acc: 0.532 - ETA: 3:14 - loss: 0.9323 - acc: 0.533 - ETA: 3:11 - loss: 0.9306 - acc: 0.532 - ETA: 3:09 - loss: 0.9295 - acc: 0.533 - ETA: 3:07 - loss: 0.9309 - acc: 0.534 - ETA: 3:04 - loss: 0.9304 - acc: 0.534 - ETA: 3:02 - loss: 0.9292 - acc: 0.534 - ETA: 3:00 - loss: 0.9279 - acc: 0.534 - ETA: 2:58 - loss: 0.9269 - acc: 0.533 - ETA: 2:55 - loss: 0.9257 - acc: 0.534 - ETA: 2:53 - loss: 0.9254 - acc: 0.533 - ETA: 2:51 - loss: 0.9258 - acc: 0.534 - ETA: 2:49 - loss: 0.9246 - acc: 0.534 - ETA: 2:47 - loss: 0.9254 - acc: 0.533 - ETA: 2:44 - loss: 0.9236 - acc: 0.533 - ETA: 2:42 - loss: 0.9257 - acc: 0.533 - ETA: 2:40 - loss: 0.9257 - acc: 0.532 - ETA: 2:37 - loss: 0.9291 - acc: 0.530 - ETA: 2:35 - loss: 0.9320 - acc: 0.530 - ETA: 2:33 - loss: 0.9321 - acc: 0.529 - ETA: 2:30 - loss: 0.9348 - acc: 0.529 - ETA: 2:28 - loss: 0.9333 - acc: 0.530 - ETA: 2:26 - loss: 0.9338 - acc: 0.531 - ETA: 2:23 - loss: 0.9341 - acc: 0.532 - ETA: 2:21 - loss: 0.9336 - acc: 0.531 - ETA: 2:19 - loss: 0.9334 - acc: 0.530 - ETA: 2:16 - loss: 0.9338 - acc: 0.529 - ETA: 2:14 - loss: 0.9331 - acc: 0.530 - ETA: 2:12 - loss: 0.9336 - acc: 0.530 - ETA: 2:09 - loss: 0.9336 - acc: 0.530 - ETA: 2:07 - loss: 0.9336 - acc: 0.530 - ETA: 2:05 - loss: 0.9337 - acc: 0.530 - ETA: 2:02 - loss: 0.9328 - acc: 0.530 - ETA: 2:00 - loss: 0.9323 - acc: 0.530 - ETA: 1:58 - loss: 0.9325 - acc: 0.531 - ETA: 1:55 - loss: 0.9319 - acc: 0.531 - ETA: 1:53 - loss: 0.9310 - acc: 0.532 - ETA: 1:51 - loss: 0.9314 - acc: 0.531 - ETA: 1:49 - loss: 0.9307 - acc: 0.532 - ETA: 1:46 - loss: 0.9306 - acc: 0.531 - ETA: 1:44 - loss: 0.9304 - acc: 0.531 - ETA: 1:42 - loss: 0.9302 - acc: 0.530 - ETA: 1:39 - loss: 0.9294 - acc: 0.531 - ETA: 1:37 - loss: 0.9297 - acc: 0.531 - ETA: 1:35 - loss: 0.9297 - acc: 0.530 - ETA: 1:32 - loss: 0.9282 - acc: 0.531 - ETA: 1:30 - loss: 0.9288 - acc: 0.531 - ETA: 1:28 - loss: 0.9287 - acc: 0.530 - ETA: 1:25 - loss: 0.9285 - acc: 0.530 - ETA: 1:23 - loss: 0.9276 - acc: 0.530 - ETA: 1:21 - loss: 0.9271 - acc: 0.530 - ETA: 1:18 - loss: 0.9264 - acc: 0.530 - ETA: 1:16 - loss: 0.9251 - acc: 0.532 - ETA: 1:14 - loss: 0.9246 - acc: 0.532 - ETA: 1:12 - loss: 0.9234 - acc: 0.533 - ETA: 1:09 - loss: 0.9224 - acc: 0.533 - ETA: 1:07 - loss: 0.9220 - acc: 0.534 - ETA: 1:05 - loss: 0.9230 - acc: 0.533 - ETA: 1:02 - loss: 0.9224 - acc: 0.534 - ETA: 1:00 - loss: 0.9212 - acc: 0.534 - ETA: 57s - loss: 0.9212 - acc: 0.533 - ETA: 55s - loss: 0.9199 - acc: 0.53 - ETA: 53s - loss: 0.9195 - acc: 0.53 - ETA: 51s - loss: 0.9187 - acc: 0.53 - ETA: 48s - loss: 0.9173 - acc: 0.53 - ETA: 46s - loss: 0.9178 - acc: 0.53 - ETA: 44s - loss: 0.9174 - acc: 0.53 - ETA: 41s - loss: 0.9162 - acc: 0.53 - ETA: 39s - loss: 0.9155 - acc: 0.53 - ETA: 37s - loss: 0.9156 - acc: 0.53 - ETA: 34s - loss: 0.9147 - acc: 0.53 - ETA: 32s - loss: 0.9134 - acc: 0.53 - ETA: 30s - loss: 0.9127 - acc: 0.53 - ETA: 27s - loss: 0.9146 - acc: 0.53 - ETA: 25s - loss: 0.9146 - acc: 0.53 - ETA: 23s - loss: 0.9144 - acc: 0.53 - ETA: 20s - loss: 0.9207 - acc: 0.53 - ETA: 18s - loss: 0.9196 - acc: 0.53 - ETA: 16s - loss: 0.9190 - acc: 0.53 - ETA: 13s - loss: 0.9200 - acc: 0.53 - ETA: 11s - loss: 0.9191 - acc: 0.53 - ETA: 9s - loss: 0.9185 - acc: 0.5342 - ETA: 6s - loss: 0.9182 - acc: 0.533 - ETA: 4s - loss: 0.9172 - acc: 0.534 - ETA: 2s - loss: 0.9178 - acc: 0.534 - 778s 3s/step - loss: 0.9180 - acc: 0.5338 - val_loss: 0.8076 - val_acc: 0.5324\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.53241, saving model to best_weight6.hdf5\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/307 [==================>...........] - ETA: 13:11 - loss: 0.5959 - acc: 0.75 - ETA: 20:03 - loss: 0.6000 - acc: 0.75 - ETA: 20:40 - loss: 0.6317 - acc: 0.70 - ETA: 19:37 - loss: 0.7203 - acc: 0.65 - ETA: 18:00 - loss: 0.7497 - acc: 0.62 - ETA: 16:51 - loss: 0.8045 - acc: 0.58 - ETA: 21:47 - loss: 0.8085 - acc: 0.58 - ETA: 20:27 - loss: 0.8135 - acc: 0.54 - ETA: 19:42 - loss: 0.8132 - acc: 0.52 - ETA: 19:54 - loss: 0.8407 - acc: 0.52 - ETA: 19:31 - loss: 0.8268 - acc: 0.53 - ETA: 18:57 - loss: 0.8502 - acc: 0.52 - ETA: 18:34 - loss: 0.8621 - acc: 0.51 - ETA: 18:12 - loss: 0.8694 - acc: 0.53 - ETA: 17:50 - loss: 0.8679 - acc: 0.52 - ETA: 17:26 - loss: 0.8646 - acc: 0.53 - ETA: 17:01 - loss: 0.8627 - acc: 0.52 - ETA: 16:36 - loss: 0.8529 - acc: 0.54 - ETA: 16:12 - loss: 0.8705 - acc: 0.52 - ETA: 15:51 - loss: 0.8641 - acc: 0.52 - ETA: 15:31 - loss: 0.8633 - acc: 0.51 - ETA: 15:17 - loss: 0.8563 - acc: 0.51 - ETA: 14:59 - loss: 0.8580 - acc: 0.52 - ETA: 14:47 - loss: 0.8644 - acc: 0.52 - ETA: 14:35 - loss: 0.8558 - acc: 0.54 - ETA: 14:24 - loss: 0.8441 - acc: 0.54 - ETA: 14:19 - loss: 0.8420 - acc: 0.55 - ETA: 14:07 - loss: 0.8372 - acc: 0.55 - ETA: 14:01 - loss: 0.8335 - acc: 0.55 - ETA: 13:54 - loss: 0.8498 - acc: 0.54 - ETA: 13:49 - loss: 0.8544 - acc: 0.54 - ETA: 13:45 - loss: 0.8520 - acc: 0.55 - ETA: 13:36 - loss: 0.8478 - acc: 0.55 - ETA: 13:28 - loss: 0.8438 - acc: 0.55 - ETA: 13:18 - loss: 0.8364 - acc: 0.55 - ETA: 13:09 - loss: 0.8358 - acc: 0.55 - ETA: 13:01 - loss: 0.8364 - acc: 0.55 - ETA: 12:53 - loss: 0.8364 - acc: 0.54 - ETA: 12:44 - loss: 0.8344 - acc: 0.54 - ETA: 12:36 - loss: 0.8292 - acc: 0.54 - ETA: 12:28 - loss: 0.8219 - acc: 0.54 - ETA: 12:21 - loss: 0.8182 - acc: 0.55 - ETA: 12:13 - loss: 0.8162 - acc: 0.55 - ETA: 12:06 - loss: 0.8066 - acc: 0.55 - ETA: 12:00 - loss: 0.8217 - acc: 0.55 - ETA: 11:54 - loss: 0.8241 - acc: 0.54 - ETA: 11:48 - loss: 0.8237 - acc: 0.54 - ETA: 11:42 - loss: 0.8387 - acc: 0.54 - ETA: 11:36 - loss: 0.8397 - acc: 0.54 - ETA: 11:30 - loss: 0.8439 - acc: 0.54 - ETA: 11:24 - loss: 0.8419 - acc: 0.54 - ETA: 11:18 - loss: 0.8421 - acc: 0.53 - ETA: 11:13 - loss: 0.8363 - acc: 0.54 - ETA: 11:07 - loss: 0.8413 - acc: 0.53 - ETA: 11:02 - loss: 0.8363 - acc: 0.54 - ETA: 10:57 - loss: 0.8395 - acc: 0.54 - ETA: 10:52 - loss: 0.8379 - acc: 0.54 - ETA: 10:47 - loss: 0.8528 - acc: 0.54 - ETA: 10:42 - loss: 0.8525 - acc: 0.54 - ETA: 10:38 - loss: 0.8522 - acc: 0.54 - ETA: 10:33 - loss: 0.8526 - acc: 0.54 - ETA: 10:28 - loss: 0.8588 - acc: 0.54 - ETA: 10:24 - loss: 0.8556 - acc: 0.54 - ETA: 10:19 - loss: 0.8517 - acc: 0.54 - ETA: 10:15 - loss: 0.8505 - acc: 0.54 - ETA: 10:10 - loss: 0.8497 - acc: 0.54 - ETA: 10:06 - loss: 0.8518 - acc: 0.54 - ETA: 10:02 - loss: 0.8470 - acc: 0.54 - ETA: 9:57 - loss: 0.8535 - acc: 0.5471 - ETA: 9:53 - loss: 0.8531 - acc: 0.548 - ETA: 9:49 - loss: 0.8528 - acc: 0.549 - ETA: 9:45 - loss: 0.8700 - acc: 0.546 - ETA: 9:40 - loss: 0.8755 - acc: 0.542 - ETA: 9:36 - loss: 0.8771 - acc: 0.542 - ETA: 9:32 - loss: 0.8753 - acc: 0.545 - ETA: 9:29 - loss: 0.8717 - acc: 0.549 - ETA: 9:25 - loss: 0.8690 - acc: 0.548 - ETA: 9:21 - loss: 0.8688 - acc: 0.548 - ETA: 9:17 - loss: 0.8726 - acc: 0.544 - ETA: 9:13 - loss: 0.8703 - acc: 0.546 - ETA: 9:10 - loss: 0.8703 - acc: 0.547 - ETA: 9:06 - loss: 0.8696 - acc: 0.548 - ETA: 9:02 - loss: 0.8666 - acc: 0.549 - ETA: 8:59 - loss: 0.8646 - acc: 0.549 - ETA: 8:55 - loss: 0.8637 - acc: 0.551 - ETA: 8:53 - loss: 0.8636 - acc: 0.555 - ETA: 8:52 - loss: 0.8626 - acc: 0.553 - ETA: 8:49 - loss: 0.8597 - acc: 0.554 - ETA: 8:46 - loss: 0.8573 - acc: 0.554 - ETA: 8:44 - loss: 0.8556 - acc: 0.556 - ETA: 8:41 - loss: 0.8532 - acc: 0.556 - ETA: 8:40 - loss: 0.8509 - acc: 0.555 - ETA: 8:37 - loss: 0.8499 - acc: 0.557 - ETA: 8:34 - loss: 0.8497 - acc: 0.558 - ETA: 8:30 - loss: 0.8471 - acc: 0.560 - ETA: 8:27 - loss: 0.8450 - acc: 0.562 - ETA: 8:24 - loss: 0.8514 - acc: 0.560 - ETA: 8:21 - loss: 0.8516 - acc: 0.559 - ETA: 8:18 - loss: 0.8496 - acc: 0.561 - ETA: 8:15 - loss: 0.8490 - acc: 0.560 - ETA: 8:12 - loss: 0.8491 - acc: 0.559 - ETA: 8:08 - loss: 0.8492 - acc: 0.558 - ETA: 8:05 - loss: 0.8493 - acc: 0.557 - ETA: 8:03 - loss: 0.8460 - acc: 0.558 - ETA: 8:00 - loss: 0.8438 - acc: 0.559 - ETA: 7:57 - loss: 0.8420 - acc: 0.561 - ETA: 7:54 - loss: 0.8398 - acc: 0.564 - ETA: 7:51 - loss: 0.8454 - acc: 0.562 - ETA: 7:48 - loss: 0.8474 - acc: 0.560 - ETA: 7:45 - loss: 0.8469 - acc: 0.560 - ETA: 7:42 - loss: 0.8437 - acc: 0.560 - ETA: 7:39 - loss: 0.8415 - acc: 0.564 - ETA: 7:36 - loss: 0.8434 - acc: 0.561 - ETA: 7:33 - loss: 0.8426 - acc: 0.563 - ETA: 7:30 - loss: 0.8415 - acc: 0.563 - ETA: 7:27 - loss: 0.8389 - acc: 0.563 - ETA: 7:24 - loss: 0.8381 - acc: 0.564 - ETA: 7:22 - loss: 0.8394 - acc: 0.564 - ETA: 7:19 - loss: 0.8476 - acc: 0.563 - ETA: 7:16 - loss: 0.8461 - acc: 0.562 - ETA: 7:13 - loss: 0.8458 - acc: 0.565 - ETA: 7:10 - loss: 0.8451 - acc: 0.563 - ETA: 7:08 - loss: 0.8440 - acc: 0.564 - ETA: 7:05 - loss: 0.8424 - acc: 0.565 - ETA: 7:03 - loss: 0.8409 - acc: 0.566 - ETA: 7:00 - loss: 0.8388 - acc: 0.567 - ETA: 6:57 - loss: 0.8375 - acc: 0.566 - ETA: 6:54 - loss: 0.8352 - acc: 0.569 - ETA: 6:52 - loss: 0.8371 - acc: 0.566 - ETA: 6:49 - loss: 0.8363 - acc: 0.568 - ETA: 6:46 - loss: 0.8395 - acc: 0.566 - ETA: 6:44 - loss: 0.8424 - acc: 0.565 - ETA: 6:41 - loss: 0.8413 - acc: 0.564 - ETA: 6:38 - loss: 0.8390 - acc: 0.567 - ETA: 6:36 - loss: 0.8396 - acc: 0.567 - ETA: 6:34 - loss: 0.8396 - acc: 0.568 - ETA: 6:31 - loss: 0.8395 - acc: 0.569 - ETA: 6:29 - loss: 0.8364 - acc: 0.572 - ETA: 6:26 - loss: 0.8362 - acc: 0.571 - ETA: 6:24 - loss: 0.8378 - acc: 0.572 - ETA: 6:21 - loss: 0.8379 - acc: 0.571 - ETA: 6:18 - loss: 0.8374 - acc: 0.572 - ETA: 6:16 - loss: 0.8409 - acc: 0.573 - ETA: 6:13 - loss: 0.8394 - acc: 0.574 - ETA: 6:11 - loss: 0.8379 - acc: 0.576 - ETA: 6:08 - loss: 0.8368 - acc: 0.577 - ETA: 6:06 - loss: 0.8374 - acc: 0.579 - ETA: 6:04 - loss: 0.8381 - acc: 0.579 - ETA: 6:02 - loss: 0.8360 - acc: 0.580 - ETA: 6:00 - loss: 0.8353 - acc: 0.580 - ETA: 5:58 - loss: 0.8381 - acc: 0.580 - ETA: 5:56 - loss: 0.8375 - acc: 0.579 - ETA: 5:54 - loss: 0.8367 - acc: 0.580 - ETA: 5:52 - loss: 0.8371 - acc: 0.581 - ETA: 5:50 - loss: 0.8368 - acc: 0.581 - ETA: 5:48 - loss: 0.8361 - acc: 0.581 - ETA: 5:46 - loss: 0.8352 - acc: 0.581 - ETA: 5:44 - loss: 0.8362 - acc: 0.580 - ETA: 5:42 - loss: 0.8379 - acc: 0.578 - ETA: 5:40 - loss: 0.8352 - acc: 0.580 - ETA: 5:38 - loss: 0.8340 - acc: 0.580 - ETA: 5:36 - loss: 0.8327 - acc: 0.581 - ETA: 5:34 - loss: 0.8308 - acc: 0.582 - ETA: 5:32 - loss: 0.8310 - acc: 0.583 - ETA: 5:29 - loss: 0.8298 - acc: 0.583 - ETA: 5:27 - loss: 0.8314 - acc: 0.582 - ETA: 5:24 - loss: 0.8308 - acc: 0.583 - ETA: 5:22 - loss: 0.8315 - acc: 0.583 - ETA: 5:19 - loss: 0.8345 - acc: 0.583 - ETA: 5:16 - loss: 0.8337 - acc: 0.584 - ETA: 5:14 - loss: 0.8332 - acc: 0.584 - ETA: 5:11 - loss: 0.8335 - acc: 0.583 - ETA: 5:09 - loss: 0.8313 - acc: 0.583 - ETA: 5:07 - loss: 0.8304 - acc: 0.584 - ETA: 5:04 - loss: 0.8315 - acc: 0.584 - ETA: 5:02 - loss: 0.8302 - acc: 0.585 - ETA: 4:59 - loss: 0.8300 - acc: 0.585 - ETA: 4:57 - loss: 0.8340 - acc: 0.584 - ETA: 4:54 - loss: 0.8337 - acc: 0.583 - ETA: 4:52 - loss: 0.8336 - acc: 0.584 - ETA: 4:49 - loss: 0.8351 - acc: 0.584 - ETA: 4:47 - loss: 0.8354 - acc: 0.583 - ETA: 4:45 - loss: 0.8343 - acc: 0.582 - ETA: 4:42 - loss: 0.8348 - acc: 0.582 - ETA: 4:40 - loss: 0.8345 - acc: 0.581 - ETA: 4:38 - loss: 0.8332 - acc: 0.582 - ETA: 4:35 - loss: 0.8338 - acc: 0.580 - ETA: 4:33 - loss: 0.8341 - acc: 0.581 - ETA: 4:31 - loss: 0.8349 - acc: 0.580 - ETA: 4:28 - loss: 0.8365 - acc: 0.580 - ETA: 4:26 - loss: 0.8354 - acc: 0.581 - ETA: 4:23 - loss: 0.8357 - acc: 0.580 - ETA: 4:21 - loss: 0.8355 - acc: 0.581 - ETA: 4:18 - loss: 0.8348 - acc: 0.581 - ETA: 4:16 - loss: 0.8357 - acc: 0.580 - ETA: 4:14 - loss: 0.8363 - acc: 0.579 - ETA: 4:12 - loss: 0.8349 - acc: 0.580 - ETA: 4:09 - loss: 0.8308 - acc: 0.582 - ETA: 4:06 - loss: 0.8298 - acc: 0.582 - ETA: 4:04 - loss: 0.8299 - acc: 0.581 - ETA: 4:02 - loss: 0.8287 - acc: 0.584 - ETA: 3:59 - loss: 0.8282 - acc: 0.584 - ETA: 3:57 - loss: 0.8268 - acc: 0.585 - ETA: 3:54 - loss: 0.8306 - acc: 0.5846"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 3:52 - loss: 0.8311 - acc: 0.583 - ETA: 3:50 - loss: 0.8328 - acc: 0.582 - ETA: 3:47 - loss: 0.8324 - acc: 0.582 - ETA: 3:45 - loss: 0.8340 - acc: 0.581 - ETA: 3:43 - loss: 0.8324 - acc: 0.582 - ETA: 3:40 - loss: 0.8353 - acc: 0.582 - ETA: 3:38 - loss: 0.8352 - acc: 0.581 - ETA: 3:36 - loss: 0.8364 - acc: 0.579 - ETA: 3:33 - loss: 0.8378 - acc: 0.579 - ETA: 3:31 - loss: 0.8382 - acc: 0.578 - ETA: 3:29 - loss: 0.8397 - acc: 0.577 - ETA: 3:26 - loss: 0.8378 - acc: 0.579 - ETA: 3:24 - loss: 0.8370 - acc: 0.579 - ETA: 3:22 - loss: 0.8392 - acc: 0.578 - ETA: 3:20 - loss: 0.8393 - acc: 0.577 - ETA: 3:17 - loss: 0.8382 - acc: 0.577 - ETA: 3:15 - loss: 0.8385 - acc: 0.576 - ETA: 3:13 - loss: 0.8400 - acc: 0.576 - ETA: 3:10 - loss: 0.8396 - acc: 0.575 - ETA: 3:08 - loss: 0.8412 - acc: 0.574 - ETA: 3:06 - loss: 0.8410 - acc: 0.574 - ETA: 3:03 - loss: 0.8420 - acc: 0.574 - ETA: 3:01 - loss: 0.8422 - acc: 0.572 - ETA: 2:59 - loss: 0.8434 - acc: 0.572 - ETA: 2:56 - loss: 0.8433 - acc: 0.573 - ETA: 2:54 - loss: 0.8426 - acc: 0.572 - ETA: 2:52 - loss: 0.8431 - acc: 0.572 - ETA: 2:49 - loss: 0.8423 - acc: 0.573 - ETA: 2:47 - loss: 0.8429 - acc: 0.571 - ETA: 2:45 - loss: 0.8432 - acc: 0.571 - ETA: 2:42 - loss: 0.8437 - acc: 0.570 - ETA: 2:40 - loss: 0.8431 - acc: 0.571 - ETA: 2:38 - loss: 0.8431 - acc: 0.570 - ETA: 2:36 - loss: 0.8441 - acc: 0.569 - ETA: 2:33 - loss: 0.8438 - acc: 0.569 - ETA: 2:31 - loss: 0.8434 - acc: 0.569 - ETA: 2:29 - loss: 0.8432 - acc: 0.570 - ETA: 2:26 - loss: 0.8428 - acc: 0.570 - ETA: 2:24 - loss: 0.8421 - acc: 0.571 - ETA: 2:22 - loss: 0.8410 - acc: 0.572 - ETA: 2:19 - loss: 0.8409 - acc: 0.570 - ETA: 2:17 - loss: 0.8408 - acc: 0.571 - ETA: 2:15 - loss: 0.8423 - acc: 0.570 - ETA: 2:13 - loss: 0.8417 - acc: 0.570 - ETA: 2:10 - loss: 0.8411 - acc: 0.570 - ETA: 2:08 - loss: 0.8411 - acc: 0.570 - ETA: 2:06 - loss: 0.8401 - acc: 0.570 - ETA: 2:03 - loss: 0.8390 - acc: 0.571 - ETA: 2:01 - loss: 0.8391 - acc: 0.571 - ETA: 1:59 - loss: 0.8385 - acc: 0.570 - ETA: 1:56 - loss: 0.8375 - acc: 0.571 - ETA: 1:54 - loss: 0.8370 - acc: 0.572 - ETA: 1:52 - loss: 0.8364 - acc: 0.572 - ETA: 1:50 - loss: 0.8370 - acc: 0.571 - ETA: 1:47 - loss: 0.8410 - acc: 0.571 - ETA: 1:45 - loss: 0.8406 - acc: 0.571 - ETA: 1:43 - loss: 0.8407 - acc: 0.570 - ETA: 1:41 - loss: 0.8409 - acc: 0.570 - ETA: 1:38 - loss: 0.8409 - acc: 0.569 - ETA: 1:36 - loss: 0.8416 - acc: 0.569 - ETA: 1:34 - loss: 0.8417 - acc: 0.570 - ETA: 1:32 - loss: 0.8417 - acc: 0.569 - ETA: 1:29 - loss: 0.8420 - acc: 0.569 - ETA: 1:27 - loss: 0.8411 - acc: 0.570 - ETA: 1:25 - loss: 0.8411 - acc: 0.569 - ETA: 1:23 - loss: 0.8414 - acc: 0.568 - ETA: 1:20 - loss: 0.8408 - acc: 0.568 - ETA: 1:18 - loss: 0.8407 - acc: 0.567 - ETA: 1:16 - loss: 0.8403 - acc: 0.567 - ETA: 1:14 - loss: 0.8396 - acc: 0.568 - ETA: 1:11 - loss: 0.8386 - acc: 0.568 - ETA: 1:09 - loss: 0.8389 - acc: 0.567 - ETA: 1:07 - loss: 0.8377 - acc: 0.567 - ETA: 1:05 - loss: 0.8405 - acc: 0.567 - ETA: 1:02 - loss: 0.8423 - acc: 0.566 - ETA: 1:00 - loss: 0.8413 - acc: 0.567 - ETA: 58s - loss: 0.8410 - acc: 0.567 - ETA: 56s - loss: 0.8421 - acc: 0.56 - ETA: 53s - loss: 0.8416 - acc: 0.56 - ETA: 51s - loss: 0.8413 - acc: 0.56 - ETA: 49s - loss: 0.8424 - acc: 0.56 - ETA: 47s - loss: 0.8417 - acc: 0.56 - ETA: 44s - loss: 0.8419 - acc: 0.56 - ETA: 42s - loss: 0.8422 - acc: 0.56 - ETA: 40s - loss: 0.8420 - acc: 0.56 - ETA: 38s - loss: 0.8423 - acc: 0.56 - ETA: 35s - loss: 0.8414 - acc: 0.56 - ETA: 33s - loss: 0.8411 - acc: 0.56 - ETA: 31s - loss: 0.8410 - acc: 0.56 - ETA: 29s - loss: 0.8420 - acc: 0.56 - ETA: 26s - loss: 0.8417 - acc: 0.56 - ETA: 24s - loss: 0.8416 - acc: 0.56 - ETA: 22s - loss: 0.8412 - acc: 0.56 - ETA: 20s - loss: 0.8418 - acc: 0.56 - ETA: 17s - loss: 0.8410 - acc: 0.56 - ETA: 15s - loss: 0.8410 - acc: 0.56 - ETA: 13s - loss: 0.8418 - acc: 0.56 - ETA: 11s - loss: 0.8411 - acc: 0.56 - ETA: 8s - loss: 0.8411 - acc: 0.5648 - ETA: 6s - loss: 0.8418 - acc: 0.564 - ETA: 4s - loss: 0.8402 - acc: 0.564 - ETA: 2s - loss: 0.8406 - acc: 0.564 - 716s 2s/step - loss: 0.8402 - acc: 0.5643 - val_loss: 0.7243 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.53241 to 0.53704, saving model to best_weight6.hdf5\n",
      "Epoch 3/10\n",
      "204/307 [==================>...........] - ETA: 12:06 - loss: 0.6638 - acc: 0.37 - ETA: 12:15 - loss: 0.6433 - acc: 0.43 - ETA: 12:16 - loss: 0.6554 - acc: 0.41 - ETA: 12:08 - loss: 0.6866 - acc: 0.43 - ETA: 11:53 - loss: 0.6751 - acc: 0.50 - ETA: 12:04 - loss: 0.7085 - acc: 0.50 - ETA: 12:21 - loss: 0.7319 - acc: 0.46 - ETA: 12:17 - loss: 0.7393 - acc: 0.46 - ETA: 12:04 - loss: 0.7417 - acc: 0.45 - ETA: 11:52 - loss: 0.7659 - acc: 0.43 - ETA: 11:44 - loss: 0.7644 - acc: 0.43 - ETA: 11:36 - loss: 0.7828 - acc: 0.44 - ETA: 11:31 - loss: 0.8054 - acc: 0.47 - ETA: 11:24 - loss: 0.7899 - acc: 0.49 - ETA: 11:16 - loss: 0.7834 - acc: 0.50 - ETA: 11:09 - loss: 0.7900 - acc: 0.50 - ETA: 11:03 - loss: 0.7717 - acc: 0.51 - ETA: 10:59 - loss: 0.7627 - acc: 0.52 - ETA: 10:54 - loss: 0.7846 - acc: 0.51 - ETA: 10:49 - loss: 0.7820 - acc: 0.51 - ETA: 10:44 - loss: 0.7871 - acc: 0.51 - ETA: 10:41 - loss: 0.7938 - acc: 0.50 - ETA: 10:36 - loss: 0.7888 - acc: 0.50 - ETA: 10:32 - loss: 0.7997 - acc: 0.50 - ETA: 10:28 - loss: 0.7935 - acc: 0.51 - ETA: 10:24 - loss: 0.7953 - acc: 0.52 - ETA: 10:20 - loss: 0.7918 - acc: 0.52 - ETA: 10:15 - loss: 0.7852 - acc: 0.52 - ETA: 10:12 - loss: 0.7796 - acc: 0.53 - ETA: 10:08 - loss: 0.7803 - acc: 0.53 - ETA: 10:05 - loss: 0.7829 - acc: 0.52 - ETA: 10:01 - loss: 0.7849 - acc: 0.51 - ETA: 9:58 - loss: 0.7884 - acc: 0.5152 - ETA: 9:55 - loss: 0.7770 - acc: 0.529 - ETA: 9:51 - loss: 0.7831 - acc: 0.517 - ETA: 9:48 - loss: 0.7836 - acc: 0.513 - ETA: 9:45 - loss: 0.7873 - acc: 0.510 - ETA: 9:42 - loss: 0.7911 - acc: 0.509 - ETA: 9:38 - loss: 0.7917 - acc: 0.509 - ETA: 9:36 - loss: 0.7943 - acc: 0.506 - ETA: 9:34 - loss: 0.7900 - acc: 0.509 - ETA: 9:32 - loss: 0.7896 - acc: 0.508 - ETA: 9:32 - loss: 0.7896 - acc: 0.502 - ETA: 9:30 - loss: 0.7851 - acc: 0.508 - ETA: 9:27 - loss: 0.7967 - acc: 0.505 - ETA: 9:25 - loss: 0.7886 - acc: 0.513 - ETA: 9:23 - loss: 0.7927 - acc: 0.510 - ETA: 9:20 - loss: 0.7918 - acc: 0.510 - ETA: 9:19 - loss: 0.7912 - acc: 0.507 - ETA: 9:18 - loss: 0.8022 - acc: 0.502 - ETA: 9:16 - loss: 0.8026 - acc: 0.502 - ETA: 9:15 - loss: 0.8025 - acc: 0.507 - ETA: 9:14 - loss: 0.8022 - acc: 0.511 - ETA: 9:13 - loss: 0.8054 - acc: 0.513 - ETA: 9:13 - loss: 0.8035 - acc: 0.518 - ETA: 9:13 - loss: 0.8034 - acc: 0.515 - ETA: 9:12 - loss: 0.8035 - acc: 0.515 - ETA: 9:10 - loss: 0.8107 - acc: 0.519 - ETA: 9:07 - loss: 0.8077 - acc: 0.523 - ETA: 9:05 - loss: 0.8089 - acc: 0.522 - ETA: 9:02 - loss: 0.8106 - acc: 0.520 - ETA: 8:59 - loss: 0.8103 - acc: 0.526 - ETA: 8:57 - loss: 0.8088 - acc: 0.527 - ETA: 8:54 - loss: 0.8085 - acc: 0.527 - ETA: 8:52 - loss: 0.8071 - acc: 0.528 - ETA: 8:49 - loss: 0.8072 - acc: 0.530 - ETA: 8:46 - loss: 0.8024 - acc: 0.533 - ETA: 8:43 - loss: 0.7989 - acc: 0.534 - ETA: 8:41 - loss: 0.7984 - acc: 0.538 - ETA: 8:38 - loss: 0.7971 - acc: 0.535 - ETA: 8:35 - loss: 0.7965 - acc: 0.535 - ETA: 8:33 - loss: 0.7956 - acc: 0.534 - ETA: 8:30 - loss: 0.7965 - acc: 0.534 - ETA: 8:27 - loss: 0.7966 - acc: 0.537 - ETA: 8:25 - loss: 0.7956 - acc: 0.538 - ETA: 8:22 - loss: 0.7947 - acc: 0.539 - ETA: 8:20 - loss: 0.7945 - acc: 0.539 - ETA: 8:19 - loss: 0.7943 - acc: 0.540 - ETA: 8:18 - loss: 0.7918 - acc: 0.541 - ETA: 8:16 - loss: 0.7971 - acc: 0.540 - ETA: 8:13 - loss: 0.7980 - acc: 0.540 - ETA: 8:12 - loss: 0.7983 - acc: 0.541 - ETA: 8:11 - loss: 0.8006 - acc: 0.536 - ETA: 8:10 - loss: 0.7970 - acc: 0.538 - ETA: 8:08 - loss: 0.7935 - acc: 0.542 - ETA: 8:06 - loss: 0.7934 - acc: 0.540 - ETA: 8:04 - loss: 0.7915 - acc: 0.540 - ETA: 8:02 - loss: 0.7939 - acc: 0.538 - ETA: 8:00 - loss: 0.7919 - acc: 0.536 - ETA: 7:57 - loss: 0.7975 - acc: 0.534 - ETA: 7:55 - loss: 0.8002 - acc: 0.533 - ETA: 7:54 - loss: 0.7994 - acc: 0.535 - ETA: 7:52 - loss: 0.7968 - acc: 0.534 - ETA: 7:50 - loss: 0.7969 - acc: 0.535 - ETA: 7:48 - loss: 0.7952 - acc: 0.539 - ETA: 7:45 - loss: 0.7984 - acc: 0.539 - ETA: 7:43 - loss: 0.7996 - acc: 0.542 - ETA: 7:40 - loss: 0.8057 - acc: 0.543 - ETA: 7:38 - loss: 0.8033 - acc: 0.544 - ETA: 7:36 - loss: 0.8030 - acc: 0.542 - ETA: 7:33 - loss: 0.8035 - acc: 0.542 - ETA: 7:31 - loss: 0.8054 - acc: 0.541 - ETA: 7:28 - loss: 0.8032 - acc: 0.543 - ETA: 7:26 - loss: 0.8019 - acc: 0.544 - ETA: 7:24 - loss: 0.8020 - acc: 0.544 - ETA: 7:22 - loss: 0.8019 - acc: 0.544 - ETA: 7:19 - loss: 0.8002 - acc: 0.547 - ETA: 7:17 - loss: 0.8015 - acc: 0.547 - ETA: 7:15 - loss: 0.8010 - acc: 0.547 - ETA: 7:12 - loss: 0.7995 - acc: 0.548 - ETA: 7:11 - loss: 0.7989 - acc: 0.549 - ETA: 7:09 - loss: 0.7987 - acc: 0.550 - ETA: 7:07 - loss: 0.7985 - acc: 0.548 - ETA: 7:05 - loss: 0.8005 - acc: 0.547 - ETA: 7:03 - loss: 0.7995 - acc: 0.546 - ETA: 7:01 - loss: 0.7974 - acc: 0.549 - ETA: 7:00 - loss: 0.7982 - acc: 0.550 - ETA: 6:58 - loss: 0.7953 - acc: 0.553 - ETA: 6:56 - loss: 0.7947 - acc: 0.553 - ETA: 6:54 - loss: 0.7970 - acc: 0.552 - ETA: 6:52 - loss: 0.7958 - acc: 0.552 - ETA: 6:51 - loss: 0.7959 - acc: 0.553 - ETA: 6:49 - loss: 0.7968 - acc: 0.551 - ETA: 6:47 - loss: 0.7970 - acc: 0.552 - ETA: 6:45 - loss: 0.7947 - acc: 0.555 - ETA: 6:43 - loss: 0.7963 - acc: 0.555 - ETA: 6:41 - loss: 0.7962 - acc: 0.556 - ETA: 6:39 - loss: 0.7956 - acc: 0.555 - ETA: 6:37 - loss: 0.7939 - acc: 0.557 - ETA: 6:35 - loss: 0.7917 - acc: 0.558 - ETA: 6:33 - loss: 0.7910 - acc: 0.560 - ETA: 6:31 - loss: 0.7910 - acc: 0.560 - ETA: 6:29 - loss: 0.7898 - acc: 0.560 - ETA: 6:27 - loss: 0.7898 - acc: 0.558 - ETA: 6:25 - loss: 0.7888 - acc: 0.558 - ETA: 6:23 - loss: 0.7868 - acc: 0.560 - ETA: 6:21 - loss: 0.7867 - acc: 0.561 - ETA: 6:19 - loss: 0.7851 - acc: 0.562 - ETA: 6:17 - loss: 0.7825 - acc: 0.564 - ETA: 6:15 - loss: 0.7809 - acc: 0.565 - ETA: 6:13 - loss: 0.7798 - acc: 0.565 - ETA: 6:11 - loss: 0.7783 - acc: 0.566 - ETA: 6:09 - loss: 0.7773 - acc: 0.565 - ETA: 6:07 - loss: 0.7762 - acc: 0.566 - ETA: 6:05 - loss: 0.7791 - acc: 0.567 - ETA: 6:02 - loss: 0.7813 - acc: 0.567 - ETA: 6:00 - loss: 0.7818 - acc: 0.566 - ETA: 5:58 - loss: 0.7824 - acc: 0.565 - ETA: 5:56 - loss: 0.7824 - acc: 0.565 - ETA: 5:54 - loss: 0.7820 - acc: 0.565 - ETA: 5:52 - loss: 0.7822 - acc: 0.564 - ETA: 5:49 - loss: 0.7822 - acc: 0.565 - ETA: 5:47 - loss: 0.7809 - acc: 0.565 - ETA: 5:45 - loss: 0.7824 - acc: 0.563 - ETA: 5:43 - loss: 0.7827 - acc: 0.563 - ETA: 5:41 - loss: 0.7826 - acc: 0.561 - ETA: 5:38 - loss: 0.7839 - acc: 0.561 - ETA: 5:36 - loss: 0.7840 - acc: 0.562 - ETA: 5:34 - loss: 0.7841 - acc: 0.562 - ETA: 5:32 - loss: 0.7862 - acc: 0.562 - ETA: 5:29 - loss: 0.7863 - acc: 0.564 - ETA: 5:27 - loss: 0.7867 - acc: 0.563 - ETA: 5:25 - loss: 0.7855 - acc: 0.565 - ETA: 5:23 - loss: 0.7852 - acc: 0.565 - ETA: 5:21 - loss: 0.7858 - acc: 0.565 - ETA: 5:18 - loss: 0.7883 - acc: 0.565 - ETA: 5:16 - loss: 0.7872 - acc: 0.565 - ETA: 5:14 - loss: 0.7884 - acc: 0.563 - ETA: 5:12 - loss: 0.7882 - acc: 0.562 - ETA: 5:10 - loss: 0.7887 - acc: 0.562 - ETA: 5:07 - loss: 0.7870 - acc: 0.564 - ETA: 5:05 - loss: 0.7875 - acc: 0.564 - ETA: 5:03 - loss: 0.7867 - acc: 0.565 - ETA: 5:01 - loss: 0.7859 - acc: 0.565 - ETA: 4:58 - loss: 0.7872 - acc: 0.565 - ETA: 4:56 - loss: 0.7868 - acc: 0.566 - ETA: 4:54 - loss: 0.7860 - acc: 0.566 - ETA: 4:52 - loss: 0.7868 - acc: 0.566 - ETA: 4:50 - loss: 0.7863 - acc: 0.566 - ETA: 4:47 - loss: 0.7868 - acc: 0.565 - ETA: 4:45 - loss: 0.7868 - acc: 0.565 - ETA: 4:43 - loss: 0.7856 - acc: 0.566 - ETA: 4:41 - loss: 0.7849 - acc: 0.567 - ETA: 4:38 - loss: 0.7840 - acc: 0.568 - ETA: 4:36 - loss: 0.7844 - acc: 0.567 - ETA: 4:34 - loss: 0.7826 - acc: 0.567 - ETA: 4:32 - loss: 0.7826 - acc: 0.568 - ETA: 4:29 - loss: 0.7833 - acc: 0.567 - ETA: 4:27 - loss: 0.7820 - acc: 0.568 - ETA: 4:25 - loss: 0.7832 - acc: 0.567 - ETA: 4:23 - loss: 0.7827 - acc: 0.568 - ETA: 4:20 - loss: 0.7821 - acc: 0.568 - ETA: 4:18 - loss: 0.7830 - acc: 0.567 - ETA: 4:16 - loss: 0.7854 - acc: 0.565 - ETA: 4:14 - loss: 0.7880 - acc: 0.564 - ETA: 4:12 - loss: 0.7866 - acc: 0.565 - ETA: 4:10 - loss: 0.7856 - acc: 0.565 - ETA: 4:08 - loss: 0.7848 - acc: 0.565 - ETA: 4:06 - loss: 0.7838 - acc: 0.567 - ETA: 4:03 - loss: 0.7836 - acc: 0.565 - ETA: 4:01 - loss: 0.7829 - acc: 0.565 - ETA: 3:59 - loss: 0.7815 - acc: 0.566 - ETA: 3:56 - loss: 0.7804 - acc: 0.566 - ETA: 3:54 - loss: 0.7807 - acc: 0.5674307/307 [==============================] - ETA: 3:52 - loss: 0.7818 - acc: 0.567 - ETA: 3:49 - loss: 0.7824 - acc: 0.568 - ETA: 3:47 - loss: 0.7824 - acc: 0.568 - ETA: 3:45 - loss: 0.7818 - acc: 0.569 - ETA: 3:42 - loss: 0.7818 - acc: 0.568 - ETA: 3:40 - loss: 0.7816 - acc: 0.568 - ETA: 3:38 - loss: 0.7825 - acc: 0.568 - ETA: 3:35 - loss: 0.7814 - acc: 0.570 - ETA: 3:33 - loss: 0.7813 - acc: 0.570 - ETA: 3:30 - loss: 0.7821 - acc: 0.570 - ETA: 3:28 - loss: 0.7827 - acc: 0.570 - ETA: 3:26 - loss: 0.7822 - acc: 0.571 - ETA: 3:24 - loss: 0.7838 - acc: 0.569 - ETA: 3:21 - loss: 0.7826 - acc: 0.569 - ETA: 3:19 - loss: 0.7837 - acc: 0.568 - ETA: 3:17 - loss: 0.7827 - acc: 0.568 - ETA: 3:14 - loss: 0.7833 - acc: 0.570 - ETA: 3:12 - loss: 0.7828 - acc: 0.570 - ETA: 3:10 - loss: 0.7826 - acc: 0.570 - ETA: 3:08 - loss: 0.7832 - acc: 0.569 - ETA: 3:06 - loss: 0.7845 - acc: 0.570 - ETA: 3:03 - loss: 0.7838 - acc: 0.571 - ETA: 3:01 - loss: 0.7841 - acc: 0.570 - ETA: 2:59 - loss: 0.7837 - acc: 0.570 - ETA: 2:56 - loss: 0.7831 - acc: 0.571 - ETA: 2:54 - loss: 0.7830 - acc: 0.570 - ETA: 2:52 - loss: 0.7833 - acc: 0.569 - ETA: 2:50 - loss: 0.7826 - acc: 0.570 - ETA: 2:47 - loss: 0.7825 - acc: 0.570 - ETA: 2:45 - loss: 0.7836 - acc: 0.569 - ETA: 2:43 - loss: 0.7827 - acc: 0.570 - ETA: 2:40 - loss: 0.7847 - acc: 0.571 - ETA: 2:38 - loss: 0.7865 - acc: 0.570 - ETA: 2:36 - loss: 0.7869 - acc: 0.569 - ETA: 2:33 - loss: 0.7861 - acc: 0.570 - ETA: 2:31 - loss: 0.7867 - acc: 0.569 - ETA: 2:29 - loss: 0.7864 - acc: 0.568 - ETA: 2:26 - loss: 0.7876 - acc: 0.567 - ETA: 2:24 - loss: 0.7880 - acc: 0.567 - ETA: 2:22 - loss: 0.7886 - acc: 0.567 - ETA: 2:20 - loss: 0.7875 - acc: 0.568 - ETA: 2:17 - loss: 0.7876 - acc: 0.567 - ETA: 2:15 - loss: 0.7887 - acc: 0.566 - ETA: 2:13 - loss: 0.7883 - acc: 0.566 - ETA: 2:11 - loss: 0.7867 - acc: 0.567 - ETA: 2:08 - loss: 0.7895 - acc: 0.567 - ETA: 2:06 - loss: 0.7893 - acc: 0.567 - ETA: 2:04 - loss: 0.7890 - acc: 0.567 - ETA: 2:02 - loss: 0.7896 - acc: 0.567 - ETA: 1:59 - loss: 0.7888 - acc: 0.567 - ETA: 1:57 - loss: 0.7888 - acc: 0.567 - ETA: 1:55 - loss: 0.7886 - acc: 0.567 - ETA: 1:53 - loss: 0.7885 - acc: 0.567 - ETA: 1:50 - loss: 0.7884 - acc: 0.567 - ETA: 1:48 - loss: 0.7872 - acc: 0.568 - ETA: 1:46 - loss: 0.7869 - acc: 0.568 - ETA: 1:44 - loss: 0.7868 - acc: 0.569 - ETA: 1:42 - loss: 0.7859 - acc: 0.568 - ETA: 1:39 - loss: 0.7864 - acc: 0.567 - ETA: 1:37 - loss: 0.7867 - acc: 0.566 - ETA: 1:35 - loss: 0.7864 - acc: 0.566 - ETA: 1:33 - loss: 0.7874 - acc: 0.566 - ETA: 1:30 - loss: 0.7868 - acc: 0.567 - ETA: 1:28 - loss: 0.7869 - acc: 0.567 - ETA: 1:26 - loss: 0.7865 - acc: 0.567 - ETA: 1:24 - loss: 0.7869 - acc: 0.566 - ETA: 1:21 - loss: 0.7861 - acc: 0.567 - ETA: 1:19 - loss: 0.7860 - acc: 0.567 - ETA: 1:17 - loss: 0.7851 - acc: 0.567 - ETA: 1:14 - loss: 0.7875 - acc: 0.566 - ETA: 1:12 - loss: 0.7880 - acc: 0.565 - ETA: 1:10 - loss: 0.7880 - acc: 0.566 - ETA: 1:08 - loss: 0.7873 - acc: 0.566 - ETA: 1:05 - loss: 0.7876 - acc: 0.566 - ETA: 1:03 - loss: 0.7872 - acc: 0.566 - ETA: 1:01 - loss: 0.7875 - acc: 0.565 - ETA: 59s - loss: 0.7891 - acc: 0.565 - ETA: 56s - loss: 0.7890 - acc: 0.56 - ETA: 54s - loss: 0.7924 - acc: 0.56 - ETA: 52s - loss: 0.7915 - acc: 0.56 - ETA: 50s - loss: 0.7951 - acc: 0.56 - ETA: 47s - loss: 0.7949 - acc: 0.56 - ETA: 45s - loss: 0.7939 - acc: 0.56 - ETA: 43s - loss: 0.7938 - acc: 0.56 - ETA: 40s - loss: 0.7934 - acc: 0.56 - ETA: 38s - loss: 0.7929 - acc: 0.56 - ETA: 36s - loss: 0.7926 - acc: 0.56 - ETA: 34s - loss: 0.7921 - acc: 0.56 - ETA: 31s - loss: 0.7914 - acc: 0.56 - ETA: 29s - loss: 0.7915 - acc: 0.56 - ETA: 27s - loss: 0.7906 - acc: 0.56 - ETA: 25s - loss: 0.7910 - acc: 0.56 - ETA: 22s - loss: 0.7902 - acc: 0.56 - ETA: 20s - loss: 0.7920 - acc: 0.56 - ETA: 18s - loss: 0.7916 - acc: 0.56 - ETA: 15s - loss: 0.7921 - acc: 0.56 - ETA: 13s - loss: 0.7916 - acc: 0.56 - ETA: 11s - loss: 0.7916 - acc: 0.56 - ETA: 9s - loss: 0.7920 - acc: 0.5652 - ETA: 6s - loss: 0.7916 - acc: 0.565 - ETA: 4s - loss: 0.7910 - acc: 0.566 - ETA: 2s - loss: 0.7903 - acc: 0.565 - 729s 2s/step - loss: 0.7901 - acc: 0.5660 - val_loss: 0.7110 - val_acc: 0.5486\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.53704 to 0.54861, saving model to best_weight6.hdf5\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/307 [==================>...........] - ETA: 12:43 - loss: 0.6942 - acc: 0.62 - ETA: 12:06 - loss: 0.7436 - acc: 0.68 - ETA: 12:22 - loss: 0.7620 - acc: 0.66 - ETA: 12:34 - loss: 0.8401 - acc: 0.59 - ETA: 12:18 - loss: 0.7958 - acc: 0.60 - ETA: 12:05 - loss: 0.7906 - acc: 0.56 - ETA: 11:53 - loss: 0.7419 - acc: 0.60 - ETA: 11:55 - loss: 0.7217 - acc: 0.60 - ETA: 11:52 - loss: 0.7369 - acc: 0.59 - ETA: 11:52 - loss: 0.7716 - acc: 0.60 - ETA: 11:42 - loss: 0.7744 - acc: 0.57 - ETA: 11:40 - loss: 0.7526 - acc: 0.58 - ETA: 11:43 - loss: 0.7892 - acc: 0.57 - ETA: 11:46 - loss: 0.7816 - acc: 0.57 - ETA: 12:04 - loss: 0.8034 - acc: 0.57 - ETA: 12:01 - loss: 0.7928 - acc: 0.57 - ETA: 11:55 - loss: 0.7797 - acc: 0.56 - ETA: 11:51 - loss: 0.7770 - acc: 0.56 - ETA: 11:48 - loss: 0.7667 - acc: 0.57 - ETA: 11:41 - loss: 0.7626 - acc: 0.57 - ETA: 11:36 - loss: 0.7636 - acc: 0.57 - ETA: 11:31 - loss: 0.7468 - acc: 0.58 - ETA: 11:26 - loss: 0.7376 - acc: 0.58 - ETA: 11:19 - loss: 0.7331 - acc: 0.59 - ETA: 11:14 - loss: 0.7358 - acc: 0.58 - ETA: 11:09 - loss: 0.7402 - acc: 0.56 - ETA: 11:03 - loss: 0.7454 - acc: 0.56 - ETA: 10:58 - loss: 0.7471 - acc: 0.56 - ETA: 10:53 - loss: 0.7490 - acc: 0.56 - ETA: 10:48 - loss: 0.7540 - acc: 0.56 - ETA: 10:43 - loss: 0.7508 - acc: 0.56 - ETA: 10:39 - loss: 0.7480 - acc: 0.57 - ETA: 10:34 - loss: 0.7452 - acc: 0.57 - ETA: 10:30 - loss: 0.7408 - acc: 0.57 - ETA: 10:26 - loss: 0.7474 - acc: 0.57 - ETA: 10:22 - loss: 0.7460 - acc: 0.57 - ETA: 10:19 - loss: 0.7349 - acc: 0.58 - ETA: 10:15 - loss: 0.7345 - acc: 0.57 - ETA: 10:11 - loss: 0.7354 - acc: 0.57 - ETA: 10:10 - loss: 0.7351 - acc: 0.57 - ETA: 10:09 - loss: 0.7328 - acc: 0.57 - ETA: 10:09 - loss: 0.7344 - acc: 0.57 - ETA: 10:08 - loss: 0.7422 - acc: 0.56 - ETA: 10:05 - loss: 0.7519 - acc: 0.56 - ETA: 10:02 - loss: 0.7582 - acc: 0.57 - ETA: 9:59 - loss: 0.7550 - acc: 0.5734 - ETA: 9:55 - loss: 0.7680 - acc: 0.569 - ETA: 9:52 - loss: 0.7646 - acc: 0.572 - ETA: 9:49 - loss: 0.7641 - acc: 0.568 - ETA: 9:45 - loss: 0.7662 - acc: 0.567 - ETA: 9:43 - loss: 0.7601 - acc: 0.573 - ETA: 9:41 - loss: 0.7610 - acc: 0.569 - ETA: 9:38 - loss: 0.7592 - acc: 0.568 - ETA: 9:35 - loss: 0.7590 - acc: 0.569 - ETA: 9:34 - loss: 0.7579 - acc: 0.563 - ETA: 9:33 - loss: 0.7583 - acc: 0.567 - ETA: 9:32 - loss: 0.7561 - acc: 0.572 - ETA: 9:29 - loss: 0.7554 - acc: 0.573 - ETA: 9:26 - loss: 0.7531 - acc: 0.578 - ETA: 9:23 - loss: 0.7483 - acc: 0.585 - ETA: 9:20 - loss: 0.7637 - acc: 0.579 - ETA: 9:17 - loss: 0.7732 - acc: 0.578 - ETA: 9:14 - loss: 0.7728 - acc: 0.577 - ETA: 9:11 - loss: 0.7746 - acc: 0.580 - ETA: 9:08 - loss: 0.7709 - acc: 0.578 - ETA: 9:05 - loss: 0.7720 - acc: 0.579 - ETA: 9:02 - loss: 0.7740 - acc: 0.574 - ETA: 9:00 - loss: 0.7719 - acc: 0.573 - ETA: 8:53 - loss: 0.7735 - acc: 0.565 - ETA: 8:50 - loss: 0.7734 - acc: 0.566 - ETA: 8:47 - loss: 0.7722 - acc: 0.568 - ETA: 8:44 - loss: 0.7704 - acc: 0.569 - ETA: 8:41 - loss: 0.7680 - acc: 0.571 - ETA: 8:39 - loss: 0.7658 - acc: 0.572 - ETA: 8:36 - loss: 0.7639 - acc: 0.573 - ETA: 8:34 - loss: 0.7682 - acc: 0.570 - ETA: 8:31 - loss: 0.7698 - acc: 0.569 - ETA: 8:28 - loss: 0.7681 - acc: 0.572 - ETA: 8:26 - loss: 0.7793 - acc: 0.566 - ETA: 8:24 - loss: 0.7751 - acc: 0.568 - ETA: 8:21 - loss: 0.7725 - acc: 0.569 - ETA: 8:19 - loss: 0.7718 - acc: 0.570 - ETA: 8:16 - loss: 0.7708 - acc: 0.570 - ETA: 8:14 - loss: 0.7701 - acc: 0.571 - ETA: 8:11 - loss: 0.7706 - acc: 0.569 - ETA: 8:09 - loss: 0.7686 - acc: 0.571 - ETA: 8:07 - loss: 0.7673 - acc: 0.570 - ETA: 8:06 - loss: 0.7638 - acc: 0.571 - ETA: 8:05 - loss: 0.7651 - acc: 0.567 - ETA: 8:05 - loss: 0.7630 - acc: 0.570 - ETA: 8:03 - loss: 0.7614 - acc: 0.571 - ETA: 8:03 - loss: 0.7589 - acc: 0.574 - ETA: 8:03 - loss: 0.7631 - acc: 0.572 - ETA: 8:02 - loss: 0.7654 - acc: 0.571 - ETA: 8:00 - loss: 0.7652 - acc: 0.572 - ETA: 8:00 - loss: 0.7631 - acc: 0.574 - ETA: 7:59 - loss: 0.7626 - acc: 0.574 - ETA: 7:58 - loss: 0.7597 - acc: 0.575 - ETA: 7:57 - loss: 0.7605 - acc: 0.577 - ETA: 7:56 - loss: 0.7723 - acc: 0.577 - ETA: 7:54 - loss: 0.7722 - acc: 0.576 - ETA: 7:51 - loss: 0.7762 - acc: 0.576 - ETA: 7:49 - loss: 0.7749 - acc: 0.577 - ETA: 7:46 - loss: 0.7827 - acc: 0.573 - ETA: 7:45 - loss: 0.7831 - acc: 0.571 - ETA: 7:43 - loss: 0.7830 - acc: 0.573 - ETA: 7:42 - loss: 0.7824 - acc: 0.573 - ETA: 7:40 - loss: 0.7814 - acc: 0.574 - ETA: 7:38 - loss: 0.7816 - acc: 0.574 - ETA: 7:35 - loss: 0.7896 - acc: 0.575 - ETA: 7:33 - loss: 0.7912 - acc: 0.576 - ETA: 7:32 - loss: 0.7922 - acc: 0.578 - ETA: 7:30 - loss: 0.7932 - acc: 0.577 - ETA: 7:28 - loss: 0.7926 - acc: 0.577 - ETA: 7:26 - loss: 0.7928 - acc: 0.579 - ETA: 7:23 - loss: 0.7948 - acc: 0.578 - ETA: 7:20 - loss: 0.7935 - acc: 0.578 - ETA: 7:18 - loss: 0.7933 - acc: 0.579 - ETA: 7:15 - loss: 0.7943 - acc: 0.580 - ETA: 7:13 - loss: 0.7929 - acc: 0.581 - ETA: 7:10 - loss: 0.7933 - acc: 0.583 - ETA: 7:07 - loss: 0.7936 - acc: 0.584 - ETA: 7:05 - loss: 0.7935 - acc: 0.583 - ETA: 7:02 - loss: 0.7920 - acc: 0.585 - ETA: 7:00 - loss: 0.7912 - acc: 0.587 - ETA: 6:57 - loss: 0.7903 - acc: 0.585 - ETA: 6:54 - loss: 0.7889 - acc: 0.586 - ETA: 6:52 - loss: 0.7870 - acc: 0.586 - ETA: 6:49 - loss: 0.7853 - acc: 0.587 - ETA: 6:47 - loss: 0.7843 - acc: 0.588 - ETA: 6:44 - loss: 0.7817 - acc: 0.590 - ETA: 6:42 - loss: 0.7811 - acc: 0.590 - ETA: 6:39 - loss: 0.7830 - acc: 0.590 - ETA: 6:36 - loss: 0.7812 - acc: 0.591 - ETA: 6:34 - loss: 0.7791 - acc: 0.591 - ETA: 6:32 - loss: 0.7772 - acc: 0.592 - ETA: 6:29 - loss: 0.7744 - acc: 0.595 - ETA: 6:27 - loss: 0.7722 - acc: 0.597 - ETA: 6:25 - loss: 0.7728 - acc: 0.596 - ETA: 6:23 - loss: 0.7747 - acc: 0.594 - ETA: 6:22 - loss: 0.7771 - acc: 0.594 - ETA: 6:20 - loss: 0.7769 - acc: 0.595 - ETA: 6:18 - loss: 0.7833 - acc: 0.592 - ETA: 6:16 - loss: 0.7833 - acc: 0.592 - ETA: 6:14 - loss: 0.7853 - acc: 0.594 - ETA: 6:12 - loss: 0.7868 - acc: 0.594 - ETA: 6:10 - loss: 0.7880 - acc: 0.591 - ETA: 6:08 - loss: 0.7852 - acc: 0.592 - ETA: 6:06 - loss: 0.7836 - acc: 0.591 - ETA: 6:04 - loss: 0.7828 - acc: 0.591 - ETA: 6:03 - loss: 0.7810 - acc: 0.591 - ETA: 6:01 - loss: 0.7798 - acc: 0.591 - ETA: 5:59 - loss: 0.7781 - acc: 0.592 - ETA: 5:57 - loss: 0.7778 - acc: 0.592 - ETA: 5:55 - loss: 0.7776 - acc: 0.591 - ETA: 5:52 - loss: 0.7774 - acc: 0.590 - ETA: 5:49 - loss: 0.7761 - acc: 0.590 - ETA: 5:47 - loss: 0.7757 - acc: 0.591 - ETA: 5:44 - loss: 0.7747 - acc: 0.592 - ETA: 5:42 - loss: 0.7755 - acc: 0.589 - ETA: 5:39 - loss: 0.7748 - acc: 0.589 - ETA: 5:37 - loss: 0.7742 - acc: 0.588 - ETA: 5:34 - loss: 0.7758 - acc: 0.588 - ETA: 5:32 - loss: 0.7742 - acc: 0.589 - ETA: 5:29 - loss: 0.7725 - acc: 0.590 - ETA: 5:27 - loss: 0.7724 - acc: 0.591 - ETA: 5:24 - loss: 0.7721 - acc: 0.590 - ETA: 5:22 - loss: 0.7749 - acc: 0.591 - ETA: 5:19 - loss: 0.7769 - acc: 0.589 - ETA: 5:17 - loss: 0.7743 - acc: 0.591 - ETA: 5:14 - loss: 0.7750 - acc: 0.590 - ETA: 5:12 - loss: 0.7769 - acc: 0.590 - ETA: 5:09 - loss: 0.7760 - acc: 0.591 - ETA: 5:07 - loss: 0.7755 - acc: 0.591 - ETA: 5:04 - loss: 0.7746 - acc: 0.590 - ETA: 5:02 - loss: 0.7734 - acc: 0.592 - ETA: 4:59 - loss: 0.7728 - acc: 0.593 - ETA: 4:57 - loss: 0.7726 - acc: 0.594 - ETA: 4:54 - loss: 0.7741 - acc: 0.594 - ETA: 4:52 - loss: 0.7741 - acc: 0.593 - ETA: 4:49 - loss: 0.7753 - acc: 0.593 - ETA: 4:47 - loss: 0.7771 - acc: 0.592 - ETA: 4:44 - loss: 0.7771 - acc: 0.591 - ETA: 4:42 - loss: 0.7760 - acc: 0.591 - ETA: 4:40 - loss: 0.7763 - acc: 0.589 - ETA: 4:37 - loss: 0.7760 - acc: 0.590 - ETA: 4:35 - loss: 0.7788 - acc: 0.590 - ETA: 4:33 - loss: 0.7810 - acc: 0.590 - ETA: 4:31 - loss: 0.7794 - acc: 0.591 - ETA: 4:29 - loss: 0.7789 - acc: 0.591 - ETA: 4:27 - loss: 0.7784 - acc: 0.592 - ETA: 4:24 - loss: 0.7775 - acc: 0.593 - ETA: 4:22 - loss: 0.7785 - acc: 0.592 - ETA: 4:20 - loss: 0.7775 - acc: 0.594 - ETA: 4:17 - loss: 0.7768 - acc: 0.594 - ETA: 4:15 - loss: 0.7777 - acc: 0.594 - ETA: 4:12 - loss: 0.7793 - acc: 0.592 - ETA: 4:10 - loss: 0.7778 - acc: 0.592 - ETA: 4:08 - loss: 0.7759 - acc: 0.594 - ETA: 4:05 - loss: 0.7753 - acc: 0.594 - ETA: 4:03 - loss: 0.7750 - acc: 0.595 - ETA: 4:00 - loss: 0.7751 - acc: 0.595 - ETA: 3:58 - loss: 0.7738 - acc: 0.596 - ETA: 3:55 - loss: 0.7726 - acc: 0.5974"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 3:53 - loss: 0.7719 - acc: 0.597 - ETA: 3:51 - loss: 0.7718 - acc: 0.596 - ETA: 3:48 - loss: 0.7704 - acc: 0.597 - ETA: 3:46 - loss: 0.7702 - acc: 0.597 - ETA: 3:43 - loss: 0.7703 - acc: 0.597 - ETA: 3:41 - loss: 0.7696 - acc: 0.598 - ETA: 3:39 - loss: 0.7684 - acc: 0.598 - ETA: 3:36 - loss: 0.7679 - acc: 0.599 - ETA: 3:34 - loss: 0.7720 - acc: 0.599 - ETA: 3:31 - loss: 0.7724 - acc: 0.598 - ETA: 3:29 - loss: 0.7719 - acc: 0.598 - ETA: 3:27 - loss: 0.7718 - acc: 0.597 - ETA: 3:24 - loss: 0.7693 - acc: 0.598 - ETA: 3:22 - loss: 0.7689 - acc: 0.598 - ETA: 3:20 - loss: 0.7684 - acc: 0.598 - ETA: 3:18 - loss: 0.7676 - acc: 0.598 - ETA: 3:16 - loss: 0.7675 - acc: 0.597 - ETA: 3:13 - loss: 0.7661 - acc: 0.598 - ETA: 3:11 - loss: 0.7660 - acc: 0.598 - ETA: 3:09 - loss: 0.7653 - acc: 0.598 - ETA: 3:06 - loss: 0.7653 - acc: 0.597 - ETA: 3:04 - loss: 0.7650 - acc: 0.597 - ETA: 3:02 - loss: 0.7648 - acc: 0.598 - ETA: 3:00 - loss: 0.7653 - acc: 0.597 - ETA: 2:58 - loss: 0.7681 - acc: 0.596 - ETA: 2:56 - loss: 0.7687 - acc: 0.596 - ETA: 2:54 - loss: 0.7688 - acc: 0.596 - ETA: 2:52 - loss: 0.7673 - acc: 0.597 - ETA: 2:49 - loss: 0.7675 - acc: 0.596 - ETA: 2:47 - loss: 0.7666 - acc: 0.596 - ETA: 2:45 - loss: 0.7658 - acc: 0.596 - ETA: 2:42 - loss: 0.7664 - acc: 0.595 - ETA: 2:40 - loss: 0.7663 - acc: 0.595 - ETA: 2:38 - loss: 0.7683 - acc: 0.594 - ETA: 2:35 - loss: 0.7680 - acc: 0.594 - ETA: 2:33 - loss: 0.7685 - acc: 0.593 - ETA: 2:31 - loss: 0.7688 - acc: 0.593 - ETA: 2:28 - loss: 0.7695 - acc: 0.594 - ETA: 2:26 - loss: 0.7715 - acc: 0.593 - ETA: 2:24 - loss: 0.7706 - acc: 0.594 - ETA: 2:21 - loss: 0.7701 - acc: 0.594 - ETA: 2:19 - loss: 0.7698 - acc: 0.595 - ETA: 2:17 - loss: 0.7712 - acc: 0.595 - ETA: 2:14 - loss: 0.7708 - acc: 0.594 - ETA: 2:12 - loss: 0.7707 - acc: 0.593 - ETA: 2:10 - loss: 0.7709 - acc: 0.594 - ETA: 2:07 - loss: 0.7701 - acc: 0.594 - ETA: 2:05 - loss: 0.7699 - acc: 0.593 - ETA: 2:03 - loss: 0.7700 - acc: 0.592 - ETA: 2:00 - loss: 0.7697 - acc: 0.592 - ETA: 1:58 - loss: 0.7699 - acc: 0.592 - ETA: 1:56 - loss: 0.7709 - acc: 0.593 - ETA: 1:53 - loss: 0.7712 - acc: 0.592 - ETA: 1:51 - loss: 0.7706 - acc: 0.594 - ETA: 1:49 - loss: 0.7701 - acc: 0.594 - ETA: 1:46 - loss: 0.7701 - acc: 0.595 - ETA: 1:44 - loss: 0.7691 - acc: 0.595 - ETA: 1:42 - loss: 0.7694 - acc: 0.595 - ETA: 1:40 - loss: 0.7690 - acc: 0.595 - ETA: 1:37 - loss: 0.7685 - acc: 0.596 - ETA: 1:35 - loss: 0.7692 - acc: 0.595 - ETA: 1:33 - loss: 0.7698 - acc: 0.595 - ETA: 1:31 - loss: 0.7698 - acc: 0.595 - ETA: 1:29 - loss: 0.7699 - acc: 0.594 - ETA: 1:26 - loss: 0.7723 - acc: 0.593 - ETA: 1:24 - loss: 0.7720 - acc: 0.593 - ETA: 1:22 - loss: 0.7726 - acc: 0.592 - ETA: 1:19 - loss: 0.7727 - acc: 0.592 - ETA: 1:17 - loss: 0.7724 - acc: 0.592 - ETA: 1:15 - loss: 0.7723 - acc: 0.593 - ETA: 1:13 - loss: 0.7717 - acc: 0.593 - ETA: 1:10 - loss: 0.7718 - acc: 0.594 - ETA: 1:08 - loss: 0.7724 - acc: 0.594 - ETA: 1:06 - loss: 0.7715 - acc: 0.594 - ETA: 1:04 - loss: 0.7711 - acc: 0.595 - ETA: 1:02 - loss: 0.7724 - acc: 0.595 - ETA: 59s - loss: 0.7712 - acc: 0.596 - ETA: 57s - loss: 0.7720 - acc: 0.59 - ETA: 55s - loss: 0.7717 - acc: 0.59 - ETA: 53s - loss: 0.7715 - acc: 0.59 - ETA: 50s - loss: 0.7718 - acc: 0.59 - ETA: 48s - loss: 0.7712 - acc: 0.59 - ETA: 46s - loss: 0.7703 - acc: 0.59 - ETA: 43s - loss: 0.7692 - acc: 0.59 - ETA: 41s - loss: 0.7677 - acc: 0.59 - ETA: 39s - loss: 0.7675 - acc: 0.59 - ETA: 36s - loss: 0.7674 - acc: 0.59 - ETA: 34s - loss: 0.7674 - acc: 0.59 - ETA: 32s - loss: 0.7672 - acc: 0.59 - ETA: 29s - loss: 0.7667 - acc: 0.59 - ETA: 27s - loss: 0.7670 - acc: 0.59 - ETA: 25s - loss: 0.7673 - acc: 0.59 - ETA: 23s - loss: 0.7675 - acc: 0.59 - ETA: 20s - loss: 0.7669 - acc: 0.59 - ETA: 18s - loss: 0.7661 - acc: 0.59 - ETA: 16s - loss: 0.7678 - acc: 0.59 - ETA: 13s - loss: 0.7676 - acc: 0.59 - ETA: 11s - loss: 0.7669 - acc: 0.59 - ETA: 9s - loss: 0.7685 - acc: 0.5953 - ETA: 6s - loss: 0.7686 - acc: 0.596 - ETA: 4s - loss: 0.7731 - acc: 0.596 - ETA: 2s - loss: 0.7724 - acc: 0.596 - 744s 2s/step - loss: 0.7722 - acc: 0.5969 - val_loss: 0.6894 - val_acc: 0.5301\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/10\n",
      "204/307 [==================>...........] - ETA: 13:49 - loss: 0.6011 - acc: 0.62 - ETA: 13:09 - loss: 0.5913 - acc: 0.68 - ETA: 12:51 - loss: 0.6315 - acc: 0.66 - ETA: 13:34 - loss: 0.6508 - acc: 0.68 - ETA: 13:06 - loss: 0.6384 - acc: 0.67 - ETA: 13:11 - loss: 0.6334 - acc: 0.66 - ETA: 13:06 - loss: 0.6651 - acc: 0.58 - ETA: 12:45 - loss: 0.6605 - acc: 0.57 - ETA: 12:32 - loss: 0.6689 - acc: 0.56 - ETA: 12:20 - loss: 0.6588 - acc: 0.57 - ETA: 12:07 - loss: 0.6561 - acc: 0.57 - ETA: 11:55 - loss: 0.6532 - acc: 0.58 - ETA: 11:49 - loss: 0.6642 - acc: 0.57 - ETA: 11:40 - loss: 0.6830 - acc: 0.57 - ETA: 11:31 - loss: 0.6802 - acc: 0.58 - ETA: 11:30 - loss: 0.6756 - acc: 0.59 - ETA: 11:29 - loss: 0.6689 - acc: 0.60 - ETA: 11:27 - loss: 0.6921 - acc: 0.59 - ETA: 11:27 - loss: 0.6919 - acc: 0.59 - ETA: 11:26 - loss: 0.6815 - acc: 0.61 - ETA: 11:25 - loss: 0.6829 - acc: 0.60 - ETA: 11:21 - loss: 0.6784 - acc: 0.60 - ETA: 11:18 - loss: 0.6745 - acc: 0.60 - ETA: 11:13 - loss: 0.7012 - acc: 0.60 - ETA: 11:12 - loss: 0.6893 - acc: 0.62 - ETA: 11:07 - loss: 0.6894 - acc: 0.62 - ETA: 11:03 - loss: 0.7011 - acc: 0.62 - ETA: 11:00 - loss: 0.6903 - acc: 0.62 - ETA: 10:56 - loss: 0.7210 - acc: 0.62 - ETA: 10:52 - loss: 0.7139 - acc: 0.62 - ETA: 10:48 - loss: 0.7037 - acc: 0.63 - ETA: 10:43 - loss: 0.7011 - acc: 0.62 - ETA: 10:39 - loss: 0.6994 - acc: 0.63 - ETA: 10:35 - loss: 0.7057 - acc: 0.62 - ETA: 10:32 - loss: 0.7124 - acc: 0.62 - ETA: 10:28 - loss: 0.7078 - acc: 0.62 - ETA: 10:25 - loss: 0.7108 - acc: 0.62 - ETA: 10:22 - loss: 0.7075 - acc: 0.61 - ETA: 10:18 - loss: 0.7028 - acc: 0.62 - ETA: 10:15 - loss: 0.6991 - acc: 0.62 - ETA: 10:11 - loss: 0.6957 - acc: 0.62 - ETA: 10:08 - loss: 0.6998 - acc: 0.62 - ETA: 10:05 - loss: 0.6924 - acc: 0.63 - ETA: 10:01 - loss: 0.6905 - acc: 0.63 - ETA: 9:59 - loss: 0.7007 - acc: 0.6306 - ETA: 9:55 - loss: 0.7164 - acc: 0.627 - ETA: 9:52 - loss: 0.7118 - acc: 0.627 - ETA: 9:49 - loss: 0.7105 - acc: 0.625 - ETA: 9:45 - loss: 0.7084 - acc: 0.630 - ETA: 9:42 - loss: 0.7050 - acc: 0.632 - ETA: 9:39 - loss: 0.7080 - acc: 0.629 - ETA: 9:35 - loss: 0.7014 - acc: 0.634 - ETA: 9:33 - loss: 0.6965 - acc: 0.634 - ETA: 9:30 - loss: 0.7010 - acc: 0.629 - ETA: 9:27 - loss: 0.6972 - acc: 0.629 - ETA: 9:25 - loss: 0.7011 - acc: 0.629 - ETA: 9:22 - loss: 0.6991 - acc: 0.631 - ETA: 9:19 - loss: 0.6959 - acc: 0.631 - ETA: 9:16 - loss: 0.7029 - acc: 0.629 - ETA: 9:14 - loss: 0.7008 - acc: 0.629 - ETA: 9:11 - loss: 0.6964 - acc: 0.627 - ETA: 9:08 - loss: 0.7026 - acc: 0.627 - ETA: 9:05 - loss: 0.7025 - acc: 0.627 - ETA: 9:02 - loss: 0.6994 - acc: 0.627 - ETA: 9:00 - loss: 0.7023 - acc: 0.623 - ETA: 8:58 - loss: 0.7045 - acc: 0.623 - ETA: 8:55 - loss: 0.7055 - acc: 0.623 - ETA: 8:53 - loss: 0.7150 - acc: 0.619 - ETA: 8:50 - loss: 0.7129 - acc: 0.621 - ETA: 8:47 - loss: 0.7143 - acc: 0.623 - ETA: 8:45 - loss: 0.7153 - acc: 0.621 - ETA: 8:42 - loss: 0.7153 - acc: 0.621 - ETA: 8:39 - loss: 0.7139 - acc: 0.621 - ETA: 8:37 - loss: 0.7150 - acc: 0.616 - ETA: 8:35 - loss: 0.7125 - acc: 0.618 - ETA: 8:32 - loss: 0.7080 - acc: 0.621 - ETA: 8:31 - loss: 0.7081 - acc: 0.621 - ETA: 8:28 - loss: 0.7078 - acc: 0.623 - ETA: 8:25 - loss: 0.7091 - acc: 0.625 - ETA: 8:23 - loss: 0.7091 - acc: 0.626 - ETA: 8:20 - loss: 0.7081 - acc: 0.628 - ETA: 8:18 - loss: 0.7121 - acc: 0.626 - ETA: 8:15 - loss: 0.7120 - acc: 0.625 - ETA: 8:13 - loss: 0.7124 - acc: 0.625 - ETA: 8:10 - loss: 0.7136 - acc: 0.625 - ETA: 8:07 - loss: 0.7120 - acc: 0.625 - ETA: 8:05 - loss: 0.7124 - acc: 0.623 - ETA: 8:02 - loss: 0.7139 - acc: 0.623 - ETA: 7:59 - loss: 0.7098 - acc: 0.625 - ETA: 7:57 - loss: 0.7108 - acc: 0.623 - ETA: 7:54 - loss: 0.7075 - acc: 0.626 - ETA: 7:51 - loss: 0.7070 - acc: 0.623 - ETA: 7:49 - loss: 0.7050 - acc: 0.623 - ETA: 7:47 - loss: 0.7059 - acc: 0.622 - ETA: 7:45 - loss: 0.7035 - acc: 0.622 - ETA: 7:43 - loss: 0.7024 - acc: 0.622 - ETA: 7:41 - loss: 0.7017 - acc: 0.623 - ETA: 7:38 - loss: 0.6997 - acc: 0.625 - ETA: 7:36 - loss: 0.7128 - acc: 0.622 - ETA: 7:33 - loss: 0.7121 - acc: 0.620 - ETA: 7:31 - loss: 0.7114 - acc: 0.621 - ETA: 7:29 - loss: 0.7117 - acc: 0.618 - ETA: 7:26 - loss: 0.7129 - acc: 0.616 - ETA: 7:24 - loss: 0.7128 - acc: 0.614 - ETA: 7:22 - loss: 0.7141 - acc: 0.614 - ETA: 7:19 - loss: 0.7126 - acc: 0.614 - ETA: 7:17 - loss: 0.7129 - acc: 0.614 - ETA: 7:15 - loss: 0.7152 - acc: 0.610 - ETA: 7:14 - loss: 0.7141 - acc: 0.608 - ETA: 7:13 - loss: 0.7165 - acc: 0.608 - ETA: 7:13 - loss: 0.7150 - acc: 0.609 - ETA: 7:12 - loss: 0.7131 - acc: 0.610 - ETA: 7:09 - loss: 0.7107 - acc: 0.611 - ETA: 7:07 - loss: 0.7111 - acc: 0.611 - ETA: 7:05 - loss: 0.7097 - acc: 0.615 - ETA: 7:02 - loss: 0.7096 - acc: 0.616 - ETA: 7:00 - loss: 0.7109 - acc: 0.617 - ETA: 6:58 - loss: 0.7114 - acc: 0.617 - ETA: 6:56 - loss: 0.7105 - acc: 0.617 - ETA: 6:54 - loss: 0.7099 - acc: 0.617 - ETA: 6:51 - loss: 0.7087 - acc: 0.619 - ETA: 6:49 - loss: 0.7069 - acc: 0.620 - ETA: 6:47 - loss: 0.7056 - acc: 0.620 - ETA: 6:44 - loss: 0.7055 - acc: 0.619 - ETA: 6:42 - loss: 0.7069 - acc: 0.617 - ETA: 6:40 - loss: 0.7058 - acc: 0.617 - ETA: 6:37 - loss: 0.7103 - acc: 0.617 - ETA: 6:35 - loss: 0.7128 - acc: 0.615 - ETA: 6:33 - loss: 0.7098 - acc: 0.616 - ETA: 6:31 - loss: 0.7089 - acc: 0.617 - ETA: 6:29 - loss: 0.7090 - acc: 0.618 - ETA: 6:27 - loss: 0.7105 - acc: 0.616 - ETA: 6:24 - loss: 0.7101 - acc: 0.617 - ETA: 6:22 - loss: 0.7080 - acc: 0.618 - ETA: 6:20 - loss: 0.7073 - acc: 0.619 - ETA: 6:17 - loss: 0.7071 - acc: 0.619 - ETA: 6:15 - loss: 0.7076 - acc: 0.618 - ETA: 6:13 - loss: 0.7074 - acc: 0.619 - ETA: 6:11 - loss: 0.7089 - acc: 0.620 - ETA: 6:08 - loss: 0.7089 - acc: 0.619 - ETA: 6:06 - loss: 0.7082 - acc: 0.618 - ETA: 6:04 - loss: 0.7083 - acc: 0.619 - ETA: 6:01 - loss: 0.7089 - acc: 0.618 - ETA: 5:59 - loss: 0.7090 - acc: 0.618 - ETA: 5:57 - loss: 0.7087 - acc: 0.619 - ETA: 5:55 - loss: 0.7082 - acc: 0.619 - ETA: 5:52 - loss: 0.7069 - acc: 0.620 - ETA: 5:50 - loss: 0.7083 - acc: 0.618 - ETA: 5:48 - loss: 0.7089 - acc: 0.617 - ETA: 5:46 - loss: 0.7090 - acc: 0.617 - ETA: 5:43 - loss: 0.7067 - acc: 0.620 - ETA: 5:41 - loss: 0.7103 - acc: 0.620 - ETA: 5:39 - loss: 0.7107 - acc: 0.620 - ETA: 5:36 - loss: 0.7104 - acc: 0.621 - ETA: 5:34 - loss: 0.7087 - acc: 0.622 - ETA: 5:32 - loss: 0.7104 - acc: 0.623 - ETA: 5:29 - loss: 0.7101 - acc: 0.622 - ETA: 5:27 - loss: 0.7095 - acc: 0.622 - ETA: 5:25 - loss: 0.7100 - acc: 0.621 - ETA: 5:23 - loss: 0.7095 - acc: 0.621 - ETA: 5:20 - loss: 0.7091 - acc: 0.621 - ETA: 5:18 - loss: 0.7090 - acc: 0.621 - ETA: 5:16 - loss: 0.7087 - acc: 0.621 - ETA: 5:13 - loss: 0.7078 - acc: 0.620 - ETA: 5:11 - loss: 0.7087 - acc: 0.620 - ETA: 5:09 - loss: 0.7058 - acc: 0.622 - ETA: 5:06 - loss: 0.7053 - acc: 0.622 - ETA: 5:04 - loss: 0.7052 - acc: 0.622 - ETA: 5:02 - loss: 0.7065 - acc: 0.621 - ETA: 5:00 - loss: 0.7049 - acc: 0.622 - ETA: 4:56 - loss: 0.7073 - acc: 0.619 - ETA: 4:54 - loss: 0.7071 - acc: 0.618 - ETA: 4:52 - loss: 0.7083 - acc: 0.617 - ETA: 4:50 - loss: 0.7071 - acc: 0.618 - ETA: 4:47 - loss: 0.7061 - acc: 0.620 - ETA: 4:45 - loss: 0.7083 - acc: 0.618 - ETA: 4:43 - loss: 0.7101 - acc: 0.617 - ETA: 4:41 - loss: 0.7120 - acc: 0.617 - ETA: 4:38 - loss: 0.7108 - acc: 0.618 - ETA: 4:36 - loss: 0.7120 - acc: 0.619 - ETA: 4:34 - loss: 0.7127 - acc: 0.619 - ETA: 4:32 - loss: 0.7125 - acc: 0.618 - ETA: 4:30 - loss: 0.7151 - acc: 0.617 - ETA: 4:27 - loss: 0.7170 - acc: 0.615 - ETA: 4:25 - loss: 0.7176 - acc: 0.614 - ETA: 4:23 - loss: 0.7184 - acc: 0.612 - ETA: 4:21 - loss: 0.7172 - acc: 0.613 - ETA: 4:19 - loss: 0.7173 - acc: 0.612 - ETA: 4:16 - loss: 0.7160 - acc: 0.613 - ETA: 4:14 - loss: 0.7175 - acc: 0.611 - ETA: 4:12 - loss: 0.7176 - acc: 0.610 - ETA: 4:10 - loss: 0.7180 - acc: 0.610 - ETA: 4:08 - loss: 0.7175 - acc: 0.611 - ETA: 4:05 - loss: 0.7175 - acc: 0.610 - ETA: 4:03 - loss: 0.7175 - acc: 0.610 - ETA: 4:01 - loss: 0.7164 - acc: 0.611 - ETA: 3:59 - loss: 0.7165 - acc: 0.611 - ETA: 3:57 - loss: 0.7150 - acc: 0.611 - ETA: 3:55 - loss: 0.7137 - acc: 0.613 - ETA: 3:52 - loss: 0.7154 - acc: 0.612 - ETA: 3:50 - loss: 0.7142 - acc: 0.613 - ETA: 3:48 - loss: 0.7141 - acc: 0.613 - ETA: 3:46 - loss: 0.7144 - acc: 0.613 - ETA: 3:44 - loss: 0.7135 - acc: 0.6152307/307 [==============================] - ETA: 3:41 - loss: 0.7145 - acc: 0.614 - ETA: 3:39 - loss: 0.7139 - acc: 0.614 - ETA: 3:37 - loss: 0.7141 - acc: 0.614 - ETA: 3:35 - loss: 0.7150 - acc: 0.612 - ETA: 3:32 - loss: 0.7141 - acc: 0.612 - ETA: 3:30 - loss: 0.7150 - acc: 0.612 - ETA: 3:28 - loss: 0.7132 - acc: 0.614 - ETA: 3:26 - loss: 0.7131 - acc: 0.614 - ETA: 3:24 - loss: 0.7127 - acc: 0.614 - ETA: 3:21 - loss: 0.7139 - acc: 0.613 - ETA: 3:19 - loss: 0.7141 - acc: 0.613 - ETA: 3:17 - loss: 0.7136 - acc: 0.614 - ETA: 3:15 - loss: 0.7129 - acc: 0.614 - ETA: 3:13 - loss: 0.7116 - acc: 0.615 - ETA: 3:10 - loss: 0.7122 - acc: 0.614 - ETA: 3:08 - loss: 0.7110 - acc: 0.615 - ETA: 3:06 - loss: 0.7114 - acc: 0.614 - ETA: 3:04 - loss: 0.7121 - acc: 0.613 - ETA: 3:02 - loss: 0.7129 - acc: 0.613 - ETA: 3:00 - loss: 0.7135 - acc: 0.612 - ETA: 2:57 - loss: 0.7132 - acc: 0.612 - ETA: 2:55 - loss: 0.7128 - acc: 0.611 - ETA: 2:53 - loss: 0.7128 - acc: 0.611 - ETA: 2:51 - loss: 0.7120 - acc: 0.611 - ETA: 2:48 - loss: 0.7106 - acc: 0.613 - ETA: 2:46 - loss: 0.7100 - acc: 0.612 - ETA: 2:44 - loss: 0.7098 - acc: 0.612 - ETA: 2:42 - loss: 0.7103 - acc: 0.612 - ETA: 2:40 - loss: 0.7102 - acc: 0.612 - ETA: 2:37 - loss: 0.7094 - acc: 0.613 - ETA: 2:35 - loss: 0.7106 - acc: 0.612 - ETA: 2:33 - loss: 0.7102 - acc: 0.612 - ETA: 2:31 - loss: 0.7094 - acc: 0.612 - ETA: 2:29 - loss: 0.7091 - acc: 0.612 - ETA: 2:26 - loss: 0.7129 - acc: 0.611 - ETA: 2:24 - loss: 0.7130 - acc: 0.611 - ETA: 2:22 - loss: 0.7148 - acc: 0.611 - ETA: 2:20 - loss: 0.7147 - acc: 0.611 - ETA: 2:18 - loss: 0.7156 - acc: 0.609 - ETA: 2:15 - loss: 0.7152 - acc: 0.609 - ETA: 2:13 - loss: 0.7152 - acc: 0.608 - ETA: 2:11 - loss: 0.7153 - acc: 0.609 - ETA: 2:09 - loss: 0.7172 - acc: 0.608 - ETA: 2:07 - loss: 0.7171 - acc: 0.607 - ETA: 2:05 - loss: 0.7183 - acc: 0.607 - ETA: 2:03 - loss: 0.7184 - acc: 0.608 - ETA: 2:01 - loss: 0.7183 - acc: 0.607 - ETA: 1:59 - loss: 0.7177 - acc: 0.608 - ETA: 1:57 - loss: 0.7173 - acc: 0.608 - ETA: 1:54 - loss: 0.7180 - acc: 0.608 - ETA: 1:52 - loss: 0.7177 - acc: 0.608 - ETA: 1:50 - loss: 0.7172 - acc: 0.609 - ETA: 1:48 - loss: 0.7179 - acc: 0.608 - ETA: 1:46 - loss: 0.7171 - acc: 0.610 - ETA: 1:44 - loss: 0.7163 - acc: 0.611 - ETA: 1:41 - loss: 0.7183 - acc: 0.611 - ETA: 1:39 - loss: 0.7183 - acc: 0.611 - ETA: 1:37 - loss: 0.7188 - acc: 0.611 - ETA: 1:35 - loss: 0.7183 - acc: 0.611 - ETA: 1:33 - loss: 0.7169 - acc: 0.612 - ETA: 1:31 - loss: 0.7165 - acc: 0.612 - ETA: 1:29 - loss: 0.7162 - acc: 0.613 - ETA: 1:26 - loss: 0.7177 - acc: 0.612 - ETA: 1:24 - loss: 0.7171 - acc: 0.612 - ETA: 1:22 - loss: 0.7175 - acc: 0.611 - ETA: 1:20 - loss: 0.7168 - acc: 0.611 - ETA: 1:18 - loss: 0.7156 - acc: 0.611 - ETA: 1:16 - loss: 0.7156 - acc: 0.611 - ETA: 1:14 - loss: 0.7161 - acc: 0.611 - ETA: 1:11 - loss: 0.7151 - acc: 0.611 - ETA: 1:09 - loss: 0.7156 - acc: 0.610 - ETA: 1:07 - loss: 0.7150 - acc: 0.611 - ETA: 1:05 - loss: 0.7146 - acc: 0.611 - ETA: 1:03 - loss: 0.7143 - acc: 0.611 - ETA: 1:01 - loss: 0.7144 - acc: 0.610 - ETA: 58s - loss: 0.7137 - acc: 0.610 - ETA: 56s - loss: 0.7129 - acc: 0.61 - ETA: 54s - loss: 0.7123 - acc: 0.61 - ETA: 52s - loss: 0.7117 - acc: 0.61 - ETA: 50s - loss: 0.7120 - acc: 0.61 - ETA: 48s - loss: 0.7120 - acc: 0.61 - ETA: 45s - loss: 0.7117 - acc: 0.61 - ETA: 43s - loss: 0.7111 - acc: 0.61 - ETA: 41s - loss: 0.7118 - acc: 0.61 - ETA: 39s - loss: 0.7126 - acc: 0.61 - ETA: 37s - loss: 0.7120 - acc: 0.61 - ETA: 35s - loss: 0.7116 - acc: 0.61 - ETA: 33s - loss: 0.7114 - acc: 0.61 - ETA: 30s - loss: 0.7117 - acc: 0.61 - ETA: 28s - loss: 0.7110 - acc: 0.61 - ETA: 26s - loss: 0.7106 - acc: 0.61 - ETA: 24s - loss: 0.7101 - acc: 0.61 - ETA: 22s - loss: 0.7107 - acc: 0.61 - ETA: 19s - loss: 0.7106 - acc: 0.61 - ETA: 17s - loss: 0.7108 - acc: 0.61 - ETA: 15s - loss: 0.7100 - acc: 0.61 - ETA: 13s - loss: 0.7094 - acc: 0.61 - ETA: 11s - loss: 0.7089 - acc: 0.61 - ETA: 8s - loss: 0.7098 - acc: 0.6130 - ETA: 6s - loss: 0.7103 - acc: 0.612 - ETA: 4s - loss: 0.7105 - acc: 0.612 - ETA: 2s - loss: 0.7105 - acc: 0.611 - 707s 2s/step - loss: 0.7102 - acc: 0.6124 - val_loss: 0.8137 - val_acc: 0.5185\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/307 [==================>...........] - ETA: 11:04 - loss: 0.7398 - acc: 0.62 - ETA: 10:41 - loss: 0.8816 - acc: 0.56 - ETA: 10:44 - loss: 0.7986 - acc: 0.58 - ETA: 10:35 - loss: 0.7598 - acc: 0.56 - ETA: 10:42 - loss: 0.7023 - acc: 0.60 - ETA: 10:45 - loss: 0.7019 - acc: 0.60 - ETA: 10:40 - loss: 0.6744 - acc: 0.62 - ETA: 10:36 - loss: 0.7121 - acc: 0.57 - ETA: 10:33 - loss: 0.6997 - acc: 0.58 - ETA: 10:35 - loss: 0.6766 - acc: 0.61 - ETA: 10:33 - loss: 0.6689 - acc: 0.62 - ETA: 10:30 - loss: 0.6577 - acc: 0.63 - ETA: 10:27 - loss: 0.6416 - acc: 0.64 - ETA: 10:24 - loss: 0.6534 - acc: 0.63 - ETA: 10:20 - loss: 0.6665 - acc: 0.63 - ETA: 10:17 - loss: 0.6636 - acc: 0.64 - ETA: 10:14 - loss: 0.6966 - acc: 0.62 - ETA: 10:13 - loss: 0.6767 - acc: 0.64 - ETA: 10:15 - loss: 0.6659 - acc: 0.64 - ETA: 10:12 - loss: 0.6557 - acc: 0.65 - ETA: 10:09 - loss: 0.6442 - acc: 0.64 - ETA: 10:06 - loss: 0.6673 - acc: 0.63 - ETA: 10:03 - loss: 0.6704 - acc: 0.61 - ETA: 10:01 - loss: 0.6671 - acc: 0.61 - ETA: 9:59 - loss: 0.6639 - acc: 0.6150 - ETA: 9:56 - loss: 0.7363 - acc: 0.620 - ETA: 9:54 - loss: 0.7315 - acc: 0.620 - ETA: 9:52 - loss: 0.7277 - acc: 0.625 - ETA: 9:50 - loss: 0.7260 - acc: 0.629 - ETA: 9:46 - loss: 0.7263 - acc: 0.625 - ETA: 9:43 - loss: 0.7305 - acc: 0.629 - ETA: 9:41 - loss: 0.7333 - acc: 0.628 - ETA: 9:38 - loss: 0.7234 - acc: 0.636 - ETA: 9:35 - loss: 0.7302 - acc: 0.636 - ETA: 9:32 - loss: 0.7249 - acc: 0.639 - ETA: 9:31 - loss: 0.7213 - acc: 0.642 - ETA: 9:29 - loss: 0.7198 - acc: 0.641 - ETA: 9:27 - loss: 0.7095 - acc: 0.651 - ETA: 9:26 - loss: 0.7058 - acc: 0.650 - ETA: 9:24 - loss: 0.6980 - acc: 0.653 - ETA: 9:22 - loss: 0.6992 - acc: 0.649 - ETA: 9:19 - loss: 0.6943 - acc: 0.645 - ETA: 9:17 - loss: 0.6981 - acc: 0.639 - ETA: 9:15 - loss: 0.7033 - acc: 0.636 - ETA: 9:14 - loss: 0.6957 - acc: 0.644 - ETA: 9:12 - loss: 0.6868 - acc: 0.649 - ETA: 9:09 - loss: 0.6803 - acc: 0.654 - ETA: 9:06 - loss: 0.6792 - acc: 0.658 - ETA: 9:05 - loss: 0.6802 - acc: 0.653 - ETA: 9:03 - loss: 0.6784 - acc: 0.652 - ETA: 9:00 - loss: 0.6784 - acc: 0.652 - ETA: 8:57 - loss: 0.6749 - acc: 0.651 - ETA: 8:55 - loss: 0.6766 - acc: 0.648 - ETA: 8:52 - loss: 0.6760 - acc: 0.648 - ETA: 8:50 - loss: 0.6822 - acc: 0.647 - ETA: 8:47 - loss: 0.6823 - acc: 0.645 - ETA: 8:44 - loss: 0.6786 - acc: 0.642 - ETA: 8:42 - loss: 0.6724 - acc: 0.648 - ETA: 8:39 - loss: 0.6661 - acc: 0.654 - ETA: 8:37 - loss: 0.6689 - acc: 0.650 - ETA: 8:34 - loss: 0.6751 - acc: 0.645 - ETA: 8:33 - loss: 0.6764 - acc: 0.645 - ETA: 8:32 - loss: 0.6763 - acc: 0.648 - ETA: 8:31 - loss: 0.6824 - acc: 0.642 - ETA: 8:30 - loss: 0.6862 - acc: 0.642 - ETA: 8:29 - loss: 0.6840 - acc: 0.642 - ETA: 8:27 - loss: 0.6860 - acc: 0.638 - ETA: 8:25 - loss: 0.6820 - acc: 0.641 - ETA: 8:23 - loss: 0.6777 - acc: 0.644 - ETA: 8:21 - loss: 0.6727 - acc: 0.648 - ETA: 8:19 - loss: 0.6721 - acc: 0.646 - ETA: 8:17 - loss: 0.6689 - acc: 0.649 - ETA: 8:15 - loss: 0.6676 - acc: 0.650 - ETA: 8:13 - loss: 0.6686 - acc: 0.650 - ETA: 8:11 - loss: 0.6706 - acc: 0.648 - ETA: 8:08 - loss: 0.6711 - acc: 0.648 - ETA: 8:06 - loss: 0.6701 - acc: 0.651 - ETA: 8:04 - loss: 0.6699 - acc: 0.649 - ETA: 8:02 - loss: 0.6683 - acc: 0.650 - ETA: 7:59 - loss: 0.6672 - acc: 0.651 - ETA: 7:57 - loss: 0.6618 - acc: 0.655 - ETA: 7:54 - loss: 0.6584 - acc: 0.657 - ETA: 7:52 - loss: 0.6572 - acc: 0.658 - ETA: 7:50 - loss: 0.6578 - acc: 0.656 - ETA: 7:47 - loss: 0.6566 - acc: 0.658 - ETA: 7:45 - loss: 0.6560 - acc: 0.659 - ETA: 7:42 - loss: 0.6531 - acc: 0.660 - ETA: 7:40 - loss: 0.6519 - acc: 0.661 - ETA: 7:38 - loss: 0.6512 - acc: 0.661 - ETA: 7:36 - loss: 0.6495 - acc: 0.661 - ETA: 7:33 - loss: 0.6490 - acc: 0.662 - ETA: 7:31 - loss: 0.6487 - acc: 0.664 - ETA: 7:29 - loss: 0.6466 - acc: 0.665 - ETA: 7:26 - loss: 0.6465 - acc: 0.664 - ETA: 7:24 - loss: 0.6481 - acc: 0.661 - ETA: 7:22 - loss: 0.6459 - acc: 0.664 - ETA: 7:19 - loss: 0.6432 - acc: 0.663 - ETA: 7:17 - loss: 0.6407 - acc: 0.664 - ETA: 7:15 - loss: 0.6507 - acc: 0.661 - ETA: 7:13 - loss: 0.6513 - acc: 0.660 - ETA: 7:10 - loss: 0.6507 - acc: 0.660 - ETA: 7:06 - loss: 0.6508 - acc: 0.664 - ETA: 7:03 - loss: 0.6549 - acc: 0.662 - ETA: 7:01 - loss: 0.6528 - acc: 0.662 - ETA: 6:59 - loss: 0.6500 - acc: 0.665 - ETA: 6:57 - loss: 0.6539 - acc: 0.663 - ETA: 6:54 - loss: 0.6542 - acc: 0.662 - ETA: 6:52 - loss: 0.6582 - acc: 0.660 - ETA: 6:50 - loss: 0.6613 - acc: 0.657 - ETA: 6:48 - loss: 0.6607 - acc: 0.654 - ETA: 6:46 - loss: 0.6576 - acc: 0.656 - ETA: 6:44 - loss: 0.6574 - acc: 0.654 - ETA: 6:41 - loss: 0.6556 - acc: 0.656 - ETA: 6:39 - loss: 0.6562 - acc: 0.653 - ETA: 6:37 - loss: 0.6559 - acc: 0.654 - ETA: 6:35 - loss: 0.6537 - acc: 0.657 - ETA: 6:33 - loss: 0.6529 - acc: 0.656 - ETA: 6:31 - loss: 0.6531 - acc: 0.655 - ETA: 6:29 - loss: 0.6583 - acc: 0.656 - ETA: 6:27 - loss: 0.6571 - acc: 0.656 - ETA: 6:24 - loss: 0.6543 - acc: 0.659 - ETA: 6:22 - loss: 0.6540 - acc: 0.658 - ETA: 6:20 - loss: 0.6546 - acc: 0.658 - ETA: 6:18 - loss: 0.6545 - acc: 0.659 - ETA: 6:16 - loss: 0.6540 - acc: 0.660 - ETA: 6:14 - loss: 0.6547 - acc: 0.658 - ETA: 6:12 - loss: 0.6557 - acc: 0.655 - ETA: 6:10 - loss: 0.6563 - acc: 0.654 - ETA: 6:07 - loss: 0.6544 - acc: 0.656 - ETA: 6:05 - loss: 0.6544 - acc: 0.655 - ETA: 6:03 - loss: 0.6546 - acc: 0.654 - ETA: 6:01 - loss: 0.6545 - acc: 0.654 - ETA: 5:59 - loss: 0.6546 - acc: 0.653 - ETA: 5:57 - loss: 0.6555 - acc: 0.652 - ETA: 5:55 - loss: 0.6549 - acc: 0.650 - ETA: 5:52 - loss: 0.6563 - acc: 0.650 - ETA: 5:50 - loss: 0.6537 - acc: 0.652 - ETA: 5:48 - loss: 0.6534 - acc: 0.651 - ETA: 5:46 - loss: 0.6524 - acc: 0.652 - ETA: 5:44 - loss: 0.6522 - acc: 0.652 - ETA: 5:42 - loss: 0.6508 - acc: 0.653 - ETA: 5:40 - loss: 0.6505 - acc: 0.653 - ETA: 5:38 - loss: 0.6513 - acc: 0.653 - ETA: 5:35 - loss: 0.6495 - acc: 0.654 - ETA: 5:33 - loss: 0.6496 - acc: 0.656 - ETA: 5:31 - loss: 0.6471 - acc: 0.657 - ETA: 5:29 - loss: 0.6466 - acc: 0.658 - ETA: 5:27 - loss: 0.6504 - acc: 0.656 - ETA: 5:25 - loss: 0.6509 - acc: 0.654 - ETA: 5:23 - loss: 0.6580 - acc: 0.653 - ETA: 5:21 - loss: 0.6582 - acc: 0.653 - ETA: 5:19 - loss: 0.6589 - acc: 0.653 - ETA: 5:16 - loss: 0.6591 - acc: 0.654 - ETA: 5:14 - loss: 0.6596 - acc: 0.654 - ETA: 5:12 - loss: 0.6584 - acc: 0.654 - ETA: 5:10 - loss: 0.6584 - acc: 0.654 - ETA: 5:08 - loss: 0.6574 - acc: 0.656 - ETA: 5:06 - loss: 0.6568 - acc: 0.658 - ETA: 5:04 - loss: 0.6565 - acc: 0.657 - ETA: 5:02 - loss: 0.6567 - acc: 0.657 - ETA: 4:59 - loss: 0.6547 - acc: 0.657 - ETA: 4:57 - loss: 0.6551 - acc: 0.658 - ETA: 4:55 - loss: 0.6549 - acc: 0.658 - ETA: 4:53 - loss: 0.6572 - acc: 0.656 - ETA: 4:51 - loss: 0.6564 - acc: 0.655 - ETA: 4:49 - loss: 0.6569 - acc: 0.655 - ETA: 4:47 - loss: 0.6563 - acc: 0.655 - ETA: 4:45 - loss: 0.6553 - acc: 0.657 - ETA: 4:43 - loss: 0.6559 - acc: 0.656 - ETA: 4:40 - loss: 0.6545 - acc: 0.658 - ETA: 4:38 - loss: 0.6546 - acc: 0.658 - ETA: 4:36 - loss: 0.6551 - acc: 0.657 - ETA: 4:34 - loss: 0.6547 - acc: 0.656 - ETA: 4:32 - loss: 0.6556 - acc: 0.655 - ETA: 4:30 - loss: 0.6544 - acc: 0.656 - ETA: 4:28 - loss: 0.6539 - acc: 0.655 - ETA: 4:26 - loss: 0.6524 - acc: 0.656 - ETA: 4:24 - loss: 0.6525 - acc: 0.656 - ETA: 4:22 - loss: 0.6518 - acc: 0.657 - ETA: 4:19 - loss: 0.6519 - acc: 0.656 - ETA: 4:17 - loss: 0.6515 - acc: 0.657 - ETA: 4:15 - loss: 0.6518 - acc: 0.657 - ETA: 4:13 - loss: 0.6510 - acc: 0.658 - ETA: 4:11 - loss: 0.6488 - acc: 0.660 - ETA: 4:09 - loss: 0.6501 - acc: 0.660 - ETA: 4:07 - loss: 0.6496 - acc: 0.660 - ETA: 4:05 - loss: 0.6492 - acc: 0.661 - ETA: 4:03 - loss: 0.6490 - acc: 0.661 - ETA: 4:01 - loss: 0.6488 - acc: 0.662 - ETA: 3:59 - loss: 0.6479 - acc: 0.663 - ETA: 3:57 - loss: 0.6481 - acc: 0.662 - ETA: 3:54 - loss: 0.6473 - acc: 0.662 - ETA: 3:52 - loss: 0.6475 - acc: 0.663 - ETA: 3:50 - loss: 0.6463 - acc: 0.663 - ETA: 3:48 - loss: 0.6455 - acc: 0.664 - ETA: 3:46 - loss: 0.6452 - acc: 0.665 - ETA: 3:44 - loss: 0.6443 - acc: 0.666 - ETA: 3:42 - loss: 0.6440 - acc: 0.667 - ETA: 3:40 - loss: 0.6431 - acc: 0.667 - ETA: 3:38 - loss: 0.6427 - acc: 0.668 - ETA: 3:36 - loss: 0.6422 - acc: 0.667 - ETA: 3:34 - loss: 0.6427 - acc: 0.667 - ETA: 3:32 - loss: 0.6426 - acc: 0.666 - ETA: 3:30 - loss: 0.6421 - acc: 0.6667"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 3:27 - loss: 0.6423 - acc: 0.667 - ETA: 3:25 - loss: 0.6412 - acc: 0.667 - ETA: 3:23 - loss: 0.6415 - acc: 0.667 - ETA: 3:21 - loss: 0.6407 - acc: 0.667 - ETA: 3:19 - loss: 0.6416 - acc: 0.667 - ETA: 3:17 - loss: 0.6399 - acc: 0.669 - ETA: 3:15 - loss: 0.6433 - acc: 0.668 - ETA: 3:13 - loss: 0.6433 - acc: 0.668 - ETA: 3:11 - loss: 0.6436 - acc: 0.669 - ETA: 3:09 - loss: 0.6425 - acc: 0.669 - ETA: 3:07 - loss: 0.6420 - acc: 0.669 - ETA: 3:05 - loss: 0.6415 - acc: 0.670 - ETA: 3:03 - loss: 0.6418 - acc: 0.668 - ETA: 3:01 - loss: 0.6409 - acc: 0.668 - ETA: 2:59 - loss: 0.6417 - acc: 0.668 - ETA: 2:57 - loss: 0.6418 - acc: 0.668 - ETA: 2:55 - loss: 0.6406 - acc: 0.669 - ETA: 2:52 - loss: 0.6407 - acc: 0.670 - ETA: 2:50 - loss: 0.6405 - acc: 0.669 - ETA: 2:48 - loss: 0.6388 - acc: 0.671 - ETA: 2:46 - loss: 0.6391 - acc: 0.670 - ETA: 2:44 - loss: 0.6391 - acc: 0.669 - ETA: 2:42 - loss: 0.6390 - acc: 0.671 - ETA: 2:40 - loss: 0.6387 - acc: 0.670 - ETA: 2:38 - loss: 0.6387 - acc: 0.670 - ETA: 2:36 - loss: 0.6381 - acc: 0.671 - ETA: 2:34 - loss: 0.6382 - acc: 0.671 - ETA: 2:32 - loss: 0.6388 - acc: 0.671 - ETA: 2:30 - loss: 0.6394 - acc: 0.672 - ETA: 2:28 - loss: 0.6393 - acc: 0.672 - ETA: 2:26 - loss: 0.6394 - acc: 0.672 - ETA: 2:24 - loss: 0.6396 - acc: 0.672 - ETA: 2:22 - loss: 0.6416 - acc: 0.671 - ETA: 2:20 - loss: 0.6416 - acc: 0.671 - ETA: 2:18 - loss: 0.6423 - acc: 0.670 - ETA: 2:16 - loss: 0.6415 - acc: 0.670 - ETA: 2:14 - loss: 0.6416 - acc: 0.669 - ETA: 2:11 - loss: 0.6418 - acc: 0.668 - ETA: 2:09 - loss: 0.6418 - acc: 0.668 - ETA: 2:07 - loss: 0.6430 - acc: 0.668 - ETA: 2:05 - loss: 0.6434 - acc: 0.667 - ETA: 2:03 - loss: 0.6436 - acc: 0.668 - ETA: 2:01 - loss: 0.6435 - acc: 0.668 - ETA: 1:59 - loss: 0.6444 - acc: 0.668 - ETA: 1:57 - loss: 0.6435 - acc: 0.668 - ETA: 1:55 - loss: 0.6437 - acc: 0.668 - ETA: 1:53 - loss: 0.6437 - acc: 0.668 - ETA: 1:51 - loss: 0.6429 - acc: 0.668 - ETA: 1:49 - loss: 0.6436 - acc: 0.667 - ETA: 1:47 - loss: 0.6429 - acc: 0.667 - ETA: 1:45 - loss: 0.6424 - acc: 0.668 - ETA: 1:43 - loss: 0.6420 - acc: 0.668 - ETA: 1:41 - loss: 0.6418 - acc: 0.668 - ETA: 1:39 - loss: 0.6411 - acc: 0.669 - ETA: 1:37 - loss: 0.6416 - acc: 0.668 - ETA: 1:35 - loss: 0.6404 - acc: 0.669 - ETA: 1:33 - loss: 0.6404 - acc: 0.669 - ETA: 1:31 - loss: 0.6396 - acc: 0.669 - ETA: 1:29 - loss: 0.6394 - acc: 0.669 - ETA: 1:27 - loss: 0.6395 - acc: 0.670 - ETA: 1:25 - loss: 0.6396 - acc: 0.668 - ETA: 1:23 - loss: 0.6394 - acc: 0.669 - ETA: 1:21 - loss: 0.6396 - acc: 0.669 - ETA: 1:19 - loss: 0.6393 - acc: 0.669 - ETA: 1:16 - loss: 0.6393 - acc: 0.669 - ETA: 1:14 - loss: 0.6379 - acc: 0.670 - ETA: 1:12 - loss: 0.6362 - acc: 0.671 - ETA: 1:10 - loss: 0.6358 - acc: 0.671 - ETA: 1:08 - loss: 0.6357 - acc: 0.671 - ETA: 1:06 - loss: 0.6356 - acc: 0.671 - ETA: 1:04 - loss: 0.6361 - acc: 0.671 - ETA: 1:02 - loss: 0.6359 - acc: 0.671 - ETA: 1:00 - loss: 0.6344 - acc: 0.672 - ETA: 58s - loss: 0.6336 - acc: 0.673 - ETA: 56s - loss: 0.6339 - acc: 0.67 - ETA: 54s - loss: 0.6338 - acc: 0.67 - ETA: 52s - loss: 0.6344 - acc: 0.67 - ETA: 50s - loss: 0.6331 - acc: 0.67 - ETA: 48s - loss: 0.6322 - acc: 0.67 - ETA: 46s - loss: 0.6324 - acc: 0.67 - ETA: 44s - loss: 0.6326 - acc: 0.67 - ETA: 42s - loss: 0.6324 - acc: 0.67 - ETA: 40s - loss: 0.6326 - acc: 0.67 - ETA: 38s - loss: 0.6331 - acc: 0.67 - ETA: 36s - loss: 0.6324 - acc: 0.67 - ETA: 34s - loss: 0.6324 - acc: 0.67 - ETA: 32s - loss: 0.6316 - acc: 0.67 - ETA: 30s - loss: 0.6319 - acc: 0.67 - ETA: 28s - loss: 0.6317 - acc: 0.67 - ETA: 26s - loss: 0.6313 - acc: 0.67 - ETA: 24s - loss: 0.6324 - acc: 0.67 - ETA: 22s - loss: 0.6325 - acc: 0.67 - ETA: 20s - loss: 0.6320 - acc: 0.67 - ETA: 18s - loss: 0.6319 - acc: 0.67 - ETA: 16s - loss: 0.6321 - acc: 0.67 - ETA: 14s - loss: 0.6318 - acc: 0.67 - ETA: 12s - loss: 0.6315 - acc: 0.67 - ETA: 10s - loss: 0.6334 - acc: 0.67 - ETA: 8s - loss: 0.6336 - acc: 0.6724 - ETA: 6s - loss: 0.6336 - acc: 0.672 - ETA: 4s - loss: 0.6334 - acc: 0.672 - ETA: 2s - loss: 0.6332 - acc: 0.672 - 647s 2s/step - loss: 0.6337 - acc: 0.6722 - val_loss: 0.7462 - val_acc: 0.5440\n",
      "\n",
      "Epoch 00006: val_acc did not improve\n",
      "Epoch 7/10\n",
      "204/307 [==================>...........] - ETA: 10:07 - loss: 0.6157 - acc: 0.75 - ETA: 9:55 - loss: 0.7139 - acc: 0.6875 - ETA: 9:55 - loss: 0.6858 - acc: 0.583 - ETA: 9:52 - loss: 0.6908 - acc: 0.593 - ETA: 9:49 - loss: 0.6122 - acc: 0.650 - ETA: 9:54 - loss: 0.5847 - acc: 0.687 - ETA: 9:57 - loss: 0.5832 - acc: 0.714 - ETA: 9:58 - loss: 0.5752 - acc: 0.718 - ETA: 9:54 - loss: 0.6061 - acc: 0.708 - ETA: 9:51 - loss: 0.5692 - acc: 0.737 - ETA: 9:49 - loss: 0.5721 - acc: 0.727 - ETA: 9:46 - loss: 0.5631 - acc: 0.729 - ETA: 9:44 - loss: 0.5703 - acc: 0.711 - ETA: 9:41 - loss: 0.5547 - acc: 0.723 - ETA: 9:40 - loss: 0.5543 - acc: 0.716 - ETA: 9:38 - loss: 0.5458 - acc: 0.718 - ETA: 9:36 - loss: 0.5412 - acc: 0.735 - ETA: 9:33 - loss: 0.5354 - acc: 0.743 - ETA: 9:31 - loss: 0.5281 - acc: 0.750 - ETA: 9:29 - loss: 0.5361 - acc: 0.743 - ETA: 9:27 - loss: 0.5872 - acc: 0.738 - ETA: 9:25 - loss: 0.5859 - acc: 0.738 - ETA: 9:22 - loss: 0.6109 - acc: 0.728 - ETA: 9:21 - loss: 0.6053 - acc: 0.734 - ETA: 9:19 - loss: 0.5969 - acc: 0.735 - ETA: 9:17 - loss: 0.6038 - acc: 0.726 - ETA: 9:15 - loss: 0.5916 - acc: 0.736 - ETA: 9:13 - loss: 0.5853 - acc: 0.741 - ETA: 9:11 - loss: 0.5832 - acc: 0.741 - ETA: 9:09 - loss: 0.5777 - acc: 0.745 - ETA: 9:07 - loss: 0.5739 - acc: 0.750 - ETA: 9:06 - loss: 0.5784 - acc: 0.746 - ETA: 9:04 - loss: 0.5882 - acc: 0.738 - ETA: 8:51 - loss: 0.5801 - acc: 0.746 - ETA: 8:49 - loss: 0.5777 - acc: 0.746 - ETA: 8:47 - loss: 0.5635 - acc: 0.753 - ETA: 8:46 - loss: 0.5635 - acc: 0.750 - ETA: 8:44 - loss: 0.5608 - acc: 0.750 - ETA: 8:42 - loss: 0.5540 - acc: 0.753 - ETA: 8:40 - loss: 0.5481 - acc: 0.756 - ETA: 8:39 - loss: 0.5598 - acc: 0.750 - ETA: 8:37 - loss: 0.5561 - acc: 0.750 - ETA: 8:35 - loss: 0.5580 - acc: 0.752 - ETA: 8:33 - loss: 0.5569 - acc: 0.750 - ETA: 8:31 - loss: 0.5485 - acc: 0.752 - ETA: 8:30 - loss: 0.5534 - acc: 0.747 - ETA: 8:28 - loss: 0.5595 - acc: 0.742 - ETA: 8:26 - loss: 0.5576 - acc: 0.742 - ETA: 8:24 - loss: 0.5556 - acc: 0.744 - ETA: 8:22 - loss: 0.5689 - acc: 0.745 - ETA: 8:20 - loss: 0.5668 - acc: 0.745 - ETA: 8:18 - loss: 0.5717 - acc: 0.740 - ETA: 8:16 - loss: 0.5679 - acc: 0.742 - ETA: 8:15 - loss: 0.5748 - acc: 0.740 - ETA: 8:13 - loss: 0.5810 - acc: 0.738 - ETA: 8:11 - loss: 0.5822 - acc: 0.736 - ETA: 8:09 - loss: 0.5796 - acc: 0.739 - ETA: 8:07 - loss: 0.5773 - acc: 0.741 - ETA: 8:05 - loss: 0.5741 - acc: 0.745 - ETA: 8:03 - loss: 0.5773 - acc: 0.743 - ETA: 8:02 - loss: 0.5782 - acc: 0.739 - ETA: 8:00 - loss: 0.5780 - acc: 0.737 - ETA: 7:58 - loss: 0.5812 - acc: 0.736 - ETA: 7:57 - loss: 0.5788 - acc: 0.738 - ETA: 7:56 - loss: 0.5791 - acc: 0.736 - ETA: 7:54 - loss: 0.5787 - acc: 0.734 - ETA: 7:52 - loss: 0.5839 - acc: 0.733 - ETA: 7:50 - loss: 0.5822 - acc: 0.731 - ETA: 7:49 - loss: 0.5803 - acc: 0.731 - ETA: 7:47 - loss: 0.5856 - acc: 0.732 - ETA: 7:45 - loss: 0.5871 - acc: 0.728 - ETA: 7:43 - loss: 0.5825 - acc: 0.730 - ETA: 7:41 - loss: 0.5777 - acc: 0.734 - ETA: 7:39 - loss: 0.5811 - acc: 0.729 - ETA: 7:37 - loss: 0.5763 - acc: 0.731 - ETA: 7:35 - loss: 0.5770 - acc: 0.730 - ETA: 7:33 - loss: 0.5739 - acc: 0.730 - ETA: 7:31 - loss: 0.5739 - acc: 0.729 - ETA: 7:29 - loss: 0.5784 - acc: 0.726 - ETA: 7:27 - loss: 0.5788 - acc: 0.723 - ETA: 7:25 - loss: 0.5786 - acc: 0.722 - ETA: 7:23 - loss: 0.5755 - acc: 0.724 - ETA: 7:21 - loss: 0.5742 - acc: 0.725 - ETA: 7:19 - loss: 0.5733 - acc: 0.726 - ETA: 7:17 - loss: 0.5727 - acc: 0.725 - ETA: 7:15 - loss: 0.5710 - acc: 0.726 - ETA: 7:13 - loss: 0.5707 - acc: 0.725 - ETA: 7:11 - loss: 0.5693 - acc: 0.727 - ETA: 7:10 - loss: 0.5701 - acc: 0.726 - ETA: 7:09 - loss: 0.5696 - acc: 0.726 - ETA: 7:07 - loss: 0.5686 - acc: 0.728 - ETA: 7:05 - loss: 0.5680 - acc: 0.729 - ETA: 7:03 - loss: 0.5666 - acc: 0.729 - ETA: 7:02 - loss: 0.5640 - acc: 0.731 - ETA: 7:00 - loss: 0.5626 - acc: 0.731 - ETA: 6:58 - loss: 0.5614 - acc: 0.731 - ETA: 6:56 - loss: 0.5644 - acc: 0.729 - ETA: 6:55 - loss: 0.5656 - acc: 0.728 - ETA: 6:53 - loss: 0.5648 - acc: 0.728 - ETA: 6:52 - loss: 0.5637 - acc: 0.727 - ETA: 6:51 - loss: 0.5635 - acc: 0.726 - ETA: 6:49 - loss: 0.5605 - acc: 0.729 - ETA: 6:47 - loss: 0.5609 - acc: 0.728 - ETA: 6:45 - loss: 0.5623 - acc: 0.728 - ETA: 6:43 - loss: 0.5617 - acc: 0.728 - ETA: 6:41 - loss: 0.5615 - acc: 0.727 - ETA: 6:39 - loss: 0.5624 - acc: 0.727 - ETA: 6:37 - loss: 0.5612 - acc: 0.729 - ETA: 6:35 - loss: 0.5593 - acc: 0.730 - ETA: 6:33 - loss: 0.5652 - acc: 0.728 - ETA: 6:31 - loss: 0.5633 - acc: 0.729 - ETA: 6:29 - loss: 0.5606 - acc: 0.731 - ETA: 6:27 - loss: 0.5588 - acc: 0.732 - ETA: 6:25 - loss: 0.5581 - acc: 0.732 - ETA: 6:23 - loss: 0.5583 - acc: 0.732 - ETA: 6:21 - loss: 0.5675 - acc: 0.728 - ETA: 6:19 - loss: 0.5664 - acc: 0.729 - ETA: 6:17 - loss: 0.5655 - acc: 0.730 - ETA: 6:15 - loss: 0.5659 - acc: 0.729 - ETA: 6:13 - loss: 0.5652 - acc: 0.729 - ETA: 6:11 - loss: 0.5661 - acc: 0.727 - ETA: 6:09 - loss: 0.5649 - acc: 0.727 - ETA: 6:07 - loss: 0.5626 - acc: 0.727 - ETA: 6:05 - loss: 0.5600 - acc: 0.729 - ETA: 6:03 - loss: 0.5613 - acc: 0.729 - ETA: 6:00 - loss: 0.5599 - acc: 0.729 - ETA: 5:58 - loss: 0.5659 - acc: 0.729 - ETA: 5:57 - loss: 0.5649 - acc: 0.728 - ETA: 5:55 - loss: 0.5706 - acc: 0.729 - ETA: 5:53 - loss: 0.5684 - acc: 0.730 - ETA: 5:50 - loss: 0.5680 - acc: 0.730 - ETA: 5:48 - loss: 0.5668 - acc: 0.733 - ETA: 5:46 - loss: 0.5648 - acc: 0.734 - ETA: 5:44 - loss: 0.5656 - acc: 0.733 - ETA: 5:42 - loss: 0.5674 - acc: 0.731 - ETA: 5:40 - loss: 0.5673 - acc: 0.731 - ETA: 5:38 - loss: 0.5690 - acc: 0.729 - ETA: 5:36 - loss: 0.5693 - acc: 0.730 - ETA: 5:34 - loss: 0.5695 - acc: 0.730 - ETA: 5:32 - loss: 0.5721 - acc: 0.728 - ETA: 5:30 - loss: 0.5754 - acc: 0.726 - ETA: 5:28 - loss: 0.5762 - acc: 0.724 - ETA: 5:26 - loss: 0.5769 - acc: 0.723 - ETA: 5:24 - loss: 0.5776 - acc: 0.723 - ETA: 5:22 - loss: 0.5789 - acc: 0.722 - ETA: 5:20 - loss: 0.5790 - acc: 0.722 - ETA: 5:18 - loss: 0.5788 - acc: 0.721 - ETA: 5:16 - loss: 0.5802 - acc: 0.721 - ETA: 5:14 - loss: 0.5792 - acc: 0.722 - ETA: 5:12 - loss: 0.5799 - acc: 0.722 - ETA: 5:10 - loss: 0.5795 - acc: 0.722 - ETA: 5:08 - loss: 0.5789 - acc: 0.722 - ETA: 5:06 - loss: 0.5804 - acc: 0.720 - ETA: 5:04 - loss: 0.5829 - acc: 0.720 - ETA: 5:02 - loss: 0.5808 - acc: 0.721 - ETA: 5:00 - loss: 0.5822 - acc: 0.721 - ETA: 4:58 - loss: 0.5815 - acc: 0.722 - ETA: 4:56 - loss: 0.5812 - acc: 0.722 - ETA: 4:54 - loss: 0.5805 - acc: 0.723 - ETA: 4:52 - loss: 0.5796 - acc: 0.725 - ETA: 4:50 - loss: 0.5786 - acc: 0.725 - ETA: 4:48 - loss: 0.5794 - acc: 0.726 - ETA: 4:46 - loss: 0.5796 - acc: 0.724 - ETA: 4:44 - loss: 0.5779 - acc: 0.726 - ETA: 4:42 - loss: 0.5777 - acc: 0.726 - ETA: 4:40 - loss: 0.5777 - acc: 0.725 - ETA: 4:38 - loss: 0.5774 - acc: 0.725 - ETA: 4:36 - loss: 0.5773 - acc: 0.724 - ETA: 4:34 - loss: 0.5759 - acc: 0.725 - ETA: 4:32 - loss: 0.5765 - acc: 0.724 - ETA: 4:30 - loss: 0.5761 - acc: 0.723 - ETA: 4:28 - loss: 0.5750 - acc: 0.723 - ETA: 4:26 - loss: 0.5741 - acc: 0.725 - ETA: 4:24 - loss: 0.5740 - acc: 0.724 - ETA: 4:22 - loss: 0.5735 - acc: 0.725 - ETA: 4:20 - loss: 0.5755 - acc: 0.725 - ETA: 4:18 - loss: 0.5774 - acc: 0.724 - ETA: 4:16 - loss: 0.5784 - acc: 0.723 - ETA: 4:14 - loss: 0.5774 - acc: 0.724 - ETA: 4:12 - loss: 0.5770 - acc: 0.725 - ETA: 4:10 - loss: 0.5759 - acc: 0.725 - ETA: 4:08 - loss: 0.5763 - acc: 0.725 - ETA: 4:06 - loss: 0.5738 - acc: 0.726 - ETA: 4:04 - loss: 0.5731 - acc: 0.727 - ETA: 4:02 - loss: 0.5727 - acc: 0.727 - ETA: 4:00 - loss: 0.5728 - acc: 0.726 - ETA: 3:58 - loss: 0.5723 - acc: 0.726 - ETA: 3:56 - loss: 0.5732 - acc: 0.726 - ETA: 3:54 - loss: 0.5729 - acc: 0.726 - ETA: 3:52 - loss: 0.5720 - acc: 0.727 - ETA: 3:50 - loss: 0.5710 - acc: 0.728 - ETA: 3:48 - loss: 0.5721 - acc: 0.727 - ETA: 3:46 - loss: 0.5711 - acc: 0.728 - ETA: 3:44 - loss: 0.5710 - acc: 0.728 - ETA: 3:42 - loss: 0.5710 - acc: 0.728 - ETA: 3:40 - loss: 0.5697 - acc: 0.728 - ETA: 3:38 - loss: 0.5693 - acc: 0.727 - ETA: 3:36 - loss: 0.5705 - acc: 0.726 - ETA: 3:34 - loss: 0.5694 - acc: 0.728 - ETA: 3:32 - loss: 0.5689 - acc: 0.728 - ETA: 3:30 - loss: 0.5675 - acc: 0.729 - ETA: 3:28 - loss: 0.5663 - acc: 0.730 - ETA: 3:26 - loss: 0.5693 - acc: 0.729 - ETA: 3:24 - loss: 0.5680 - acc: 0.7304307/307 [==============================] - ETA: 3:22 - loss: 0.5676 - acc: 0.731 - ETA: 3:20 - loss: 0.5675 - acc: 0.731 - ETA: 3:18 - loss: 0.5665 - acc: 0.731 - ETA: 3:16 - loss: 0.5678 - acc: 0.729 - ETA: 3:14 - loss: 0.5681 - acc: 0.728 - ETA: 3:12 - loss: 0.5684 - acc: 0.727 - ETA: 3:10 - loss: 0.5678 - acc: 0.726 - ETA: 3:08 - loss: 0.5707 - acc: 0.725 - ETA: 3:06 - loss: 0.5705 - acc: 0.725 - ETA: 3:04 - loss: 0.5696 - acc: 0.726 - ETA: 3:02 - loss: 0.5705 - acc: 0.726 - ETA: 3:00 - loss: 0.5695 - acc: 0.726 - ETA: 2:58 - loss: 0.5688 - acc: 0.727 - ETA: 2:56 - loss: 0.5706 - acc: 0.727 - ETA: 2:54 - loss: 0.5697 - acc: 0.728 - ETA: 2:52 - loss: 0.5695 - acc: 0.728 - ETA: 2:50 - loss: 0.5692 - acc: 0.728 - ETA: 2:48 - loss: 0.5716 - acc: 0.728 - ETA: 2:46 - loss: 0.5700 - acc: 0.729 - ETA: 2:44 - loss: 0.5692 - acc: 0.729 - ETA: 2:42 - loss: 0.5696 - acc: 0.730 - ETA: 2:40 - loss: 0.5694 - acc: 0.730 - ETA: 2:38 - loss: 0.5679 - acc: 0.731 - ETA: 2:36 - loss: 0.5680 - acc: 0.730 - ETA: 2:34 - loss: 0.5688 - acc: 0.730 - ETA: 2:32 - loss: 0.5684 - acc: 0.729 - ETA: 2:30 - loss: 0.5676 - acc: 0.730 - ETA: 2:28 - loss: 0.5671 - acc: 0.730 - ETA: 2:26 - loss: 0.5667 - acc: 0.730 - ETA: 2:24 - loss: 0.5662 - acc: 0.731 - ETA: 2:22 - loss: 0.5668 - acc: 0.731 - ETA: 2:20 - loss: 0.5687 - acc: 0.731 - ETA: 2:18 - loss: 0.5688 - acc: 0.732 - ETA: 2:16 - loss: 0.5675 - acc: 0.732 - ETA: 2:15 - loss: 0.5666 - acc: 0.732 - ETA: 2:13 - loss: 0.5670 - acc: 0.732 - ETA: 2:11 - loss: 0.5663 - acc: 0.732 - ETA: 2:09 - loss: 0.5662 - acc: 0.733 - ETA: 2:07 - loss: 0.5670 - acc: 0.732 - ETA: 2:05 - loss: 0.5665 - acc: 0.732 - ETA: 2:03 - loss: 0.5671 - acc: 0.731 - ETA: 2:01 - loss: 0.5667 - acc: 0.731 - ETA: 1:59 - loss: 0.5667 - acc: 0.731 - ETA: 1:57 - loss: 0.5673 - acc: 0.730 - ETA: 1:55 - loss: 0.5663 - acc: 0.731 - ETA: 1:53 - loss: 0.5669 - acc: 0.730 - ETA: 1:51 - loss: 0.5660 - acc: 0.731 - ETA: 1:49 - loss: 0.5665 - acc: 0.730 - ETA: 1:47 - loss: 0.5660 - acc: 0.731 - ETA: 1:45 - loss: 0.5670 - acc: 0.730 - ETA: 1:43 - loss: 0.5672 - acc: 0.729 - ETA: 1:41 - loss: 0.5675 - acc: 0.729 - ETA: 1:39 - loss: 0.5680 - acc: 0.729 - ETA: 1:37 - loss: 0.5695 - acc: 0.730 - ETA: 1:35 - loss: 0.5693 - acc: 0.730 - ETA: 1:33 - loss: 0.5690 - acc: 0.730 - ETA: 1:31 - loss: 0.5684 - acc: 0.731 - ETA: 1:29 - loss: 0.5678 - acc: 0.731 - ETA: 1:27 - loss: 0.5685 - acc: 0.730 - ETA: 1:25 - loss: 0.5680 - acc: 0.730 - ETA: 1:23 - loss: 0.5671 - acc: 0.730 - ETA: 1:21 - loss: 0.5664 - acc: 0.730 - ETA: 1:19 - loss: 0.5660 - acc: 0.731 - ETA: 1:17 - loss: 0.5657 - acc: 0.731 - ETA: 1:15 - loss: 0.5682 - acc: 0.730 - ETA: 1:13 - loss: 0.5692 - acc: 0.730 - ETA: 1:11 - loss: 0.5688 - acc: 0.730 - ETA: 1:09 - loss: 0.5683 - acc: 0.730 - ETA: 1:07 - loss: 0.5680 - acc: 0.730 - ETA: 1:05 - loss: 0.5677 - acc: 0.730 - ETA: 1:03 - loss: 0.5668 - acc: 0.731 - ETA: 1:01 - loss: 0.5688 - acc: 0.731 - ETA: 59s - loss: 0.5687 - acc: 0.731 - ETA: 57s - loss: 0.5683 - acc: 0.73 - ETA: 55s - loss: 0.5686 - acc: 0.73 - ETA: 53s - loss: 0.5691 - acc: 0.73 - ETA: 51s - loss: 0.5681 - acc: 0.73 - ETA: 49s - loss: 0.5681 - acc: 0.73 - ETA: 47s - loss: 0.5675 - acc: 0.73 - ETA: 45s - loss: 0.5664 - acc: 0.73 - ETA: 43s - loss: 0.5654 - acc: 0.73 - ETA: 41s - loss: 0.5649 - acc: 0.73 - ETA: 39s - loss: 0.5651 - acc: 0.73 - ETA: 37s - loss: 0.5644 - acc: 0.73 - ETA: 35s - loss: 0.5638 - acc: 0.73 - ETA: 33s - loss: 0.5649 - acc: 0.73 - ETA: 31s - loss: 0.5652 - acc: 0.73 - ETA: 29s - loss: 0.5666 - acc: 0.73 - ETA: 27s - loss: 0.5661 - acc: 0.73 - ETA: 25s - loss: 0.5661 - acc: 0.73 - ETA: 23s - loss: 0.5673 - acc: 0.73 - ETA: 21s - loss: 0.5664 - acc: 0.73 - ETA: 19s - loss: 0.5665 - acc: 0.73 - ETA: 17s - loss: 0.5678 - acc: 0.73 - ETA: 15s - loss: 0.5673 - acc: 0.73 - ETA: 13s - loss: 0.5670 - acc: 0.73 - ETA: 11s - loss: 0.5675 - acc: 0.73 - ETA: 9s - loss: 0.5671 - acc: 0.7363 - ETA: 7s - loss: 0.5664 - acc: 0.736 - ETA: 5s - loss: 0.5664 - acc: 0.736 - ETA: 3s - loss: 0.5670 - acc: 0.736 - ETA: 1s - loss: 0.5674 - acc: 0.735 - 636s 2s/step - loss: 0.5669 - acc: 0.7362 - val_loss: 0.8518 - val_acc: 0.5255\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/307 [==================>...........] - ETA: 10:07 - loss: 0.3868 - acc: 0.87 - ETA: 10:05 - loss: 0.4443 - acc: 0.87 - ETA: 10:01 - loss: 0.5882 - acc: 0.79 - ETA: 10:02 - loss: 0.5827 - acc: 0.78 - ETA: 10:03 - loss: 0.5603 - acc: 0.75 - ETA: 9:58 - loss: 0.5394 - acc: 0.7917 - ETA: 10:02 - loss: 0.5434 - acc: 0.78 - ETA: 10:05 - loss: 0.5025 - acc: 0.81 - ETA: 10:02 - loss: 0.5489 - acc: 0.80 - ETA: 9:58 - loss: 0.5110 - acc: 0.8250 - ETA: 9:55 - loss: 0.5463 - acc: 0.818 - ETA: 9:53 - loss: 0.5241 - acc: 0.822 - ETA: 9:50 - loss: 0.5230 - acc: 0.817 - ETA: 9:48 - loss: 0.5229 - acc: 0.812 - ETA: 9:45 - loss: 0.5080 - acc: 0.808 - ETA: 9:43 - loss: 0.4974 - acc: 0.812 - ETA: 9:40 - loss: 0.4939 - acc: 0.816 - ETA: 9:16 - loss: 0.5393 - acc: 0.770 - ETA: 9:15 - loss: 0.5299 - acc: 0.769 - ETA: 9:14 - loss: 0.5144 - acc: 0.781 - ETA: 9:13 - loss: 0.5147 - acc: 0.773 - ETA: 9:11 - loss: 0.5262 - acc: 0.767 - ETA: 9:10 - loss: 0.5314 - acc: 0.771 - ETA: 9:09 - loss: 0.5160 - acc: 0.781 - ETA: 9:07 - loss: 0.5388 - acc: 0.780 - ETA: 9:06 - loss: 0.5418 - acc: 0.783 - ETA: 9:04 - loss: 0.5635 - acc: 0.777 - ETA: 9:03 - loss: 0.5597 - acc: 0.781 - ETA: 9:01 - loss: 0.5618 - acc: 0.775 - ETA: 9:00 - loss: 0.5660 - acc: 0.766 - ETA: 8:58 - loss: 0.5550 - acc: 0.774 - ETA: 8:56 - loss: 0.5478 - acc: 0.781 - ETA: 8:55 - loss: 0.5369 - acc: 0.787 - ETA: 8:53 - loss: 0.5332 - acc: 0.786 - ETA: 8:51 - loss: 0.5255 - acc: 0.789 - ETA: 8:49 - loss: 0.5196 - acc: 0.791 - ETA: 8:47 - loss: 0.5160 - acc: 0.797 - ETA: 8:46 - loss: 0.5131 - acc: 0.792 - ETA: 8:44 - loss: 0.5096 - acc: 0.794 - ETA: 8:42 - loss: 0.5057 - acc: 0.796 - ETA: 8:40 - loss: 0.5019 - acc: 0.795 - ETA: 8:38 - loss: 0.4937 - acc: 0.800 - ETA: 8:36 - loss: 0.4890 - acc: 0.805 - ETA: 8:35 - loss: 0.4868 - acc: 0.806 - ETA: 8:33 - loss: 0.4881 - acc: 0.805 - ETA: 8:31 - loss: 0.4838 - acc: 0.807 - ETA: 8:29 - loss: 0.4838 - acc: 0.805 - ETA: 8:27 - loss: 0.4865 - acc: 0.804 - ETA: 8:25 - loss: 0.4852 - acc: 0.806 - ETA: 8:23 - loss: 0.4794 - acc: 0.807 - ETA: 8:21 - loss: 0.4802 - acc: 0.806 - ETA: 8:20 - loss: 0.4785 - acc: 0.807 - ETA: 8:18 - loss: 0.4793 - acc: 0.809 - ETA: 8:16 - loss: 0.4812 - acc: 0.803 - ETA: 8:15 - loss: 0.4805 - acc: 0.800 - ETA: 8:13 - loss: 0.4807 - acc: 0.799 - ETA: 8:12 - loss: 0.4800 - acc: 0.798 - ETA: 8:10 - loss: 0.4817 - acc: 0.795 - ETA: 8:08 - loss: 0.4809 - acc: 0.796 - ETA: 8:06 - loss: 0.4802 - acc: 0.795 - ETA: 8:05 - loss: 0.4877 - acc: 0.791 - ETA: 8:03 - loss: 0.4860 - acc: 0.790 - ETA: 8:01 - loss: 0.4805 - acc: 0.793 - ETA: 7:59 - loss: 0.4787 - acc: 0.794 - ETA: 7:57 - loss: 0.4785 - acc: 0.794 - ETA: 7:55 - loss: 0.4932 - acc: 0.791 - ETA: 7:53 - loss: 0.4913 - acc: 0.792 - ETA: 7:51 - loss: 0.4925 - acc: 0.792 - ETA: 7:49 - loss: 0.4899 - acc: 0.793 - ETA: 7:47 - loss: 0.4849 - acc: 0.796 - ETA: 7:46 - loss: 0.4839 - acc: 0.795 - ETA: 7:44 - loss: 0.4877 - acc: 0.793 - ETA: 7:42 - loss: 0.4852 - acc: 0.794 - ETA: 7:40 - loss: 0.4848 - acc: 0.792 - ETA: 7:38 - loss: 0.4862 - acc: 0.790 - ETA: 7:36 - loss: 0.4871 - acc: 0.791 - ETA: 7:34 - loss: 0.4944 - acc: 0.790 - ETA: 7:32 - loss: 0.4941 - acc: 0.790 - ETA: 7:30 - loss: 0.4961 - acc: 0.788 - ETA: 7:28 - loss: 0.4972 - acc: 0.785 - ETA: 7:26 - loss: 0.4944 - acc: 0.788 - ETA: 7:24 - loss: 0.5004 - acc: 0.786 - ETA: 7:22 - loss: 0.4992 - acc: 0.786 - ETA: 7:20 - loss: 0.4987 - acc: 0.787 - ETA: 7:18 - loss: 0.5017 - acc: 0.783 - ETA: 7:16 - loss: 0.5017 - acc: 0.780 - ETA: 7:14 - loss: 0.5004 - acc: 0.783 - ETA: 7:12 - loss: 0.5063 - acc: 0.781 - ETA: 7:10 - loss: 0.5068 - acc: 0.779 - ETA: 7:08 - loss: 0.5104 - acc: 0.777 - ETA: 7:06 - loss: 0.5081 - acc: 0.778 - ETA: 7:04 - loss: 0.5093 - acc: 0.778 - ETA: 7:02 - loss: 0.5058 - acc: 0.780 - ETA: 7:00 - loss: 0.5042 - acc: 0.781 - ETA: 6:58 - loss: 0.5078 - acc: 0.780 - ETA: 6:57 - loss: 0.5045 - acc: 0.782 - ETA: 6:55 - loss: 0.5020 - acc: 0.783 - ETA: 6:52 - loss: 0.4989 - acc: 0.785 - ETA: 6:51 - loss: 0.4977 - acc: 0.786 - ETA: 6:49 - loss: 0.4947 - acc: 0.788 - ETA: 6:47 - loss: 0.4925 - acc: 0.790 - ETA: 6:45 - loss: 0.4930 - acc: 0.791 - ETA: 6:43 - loss: 0.4918 - acc: 0.792 - ETA: 6:41 - loss: 0.4959 - acc: 0.792 - ETA: 6:39 - loss: 0.4967 - acc: 0.791 - ETA: 6:37 - loss: 0.4950 - acc: 0.792 - ETA: 6:35 - loss: 0.4936 - acc: 0.792 - ETA: 6:33 - loss: 0.4934 - acc: 0.790 - ETA: 6:31 - loss: 0.4911 - acc: 0.792 - ETA: 6:29 - loss: 0.4897 - acc: 0.792 - ETA: 6:27 - loss: 0.4895 - acc: 0.790 - ETA: 6:25 - loss: 0.4884 - acc: 0.790 - ETA: 6:23 - loss: 0.4854 - acc: 0.790 - ETA: 6:21 - loss: 0.4845 - acc: 0.791 - ETA: 6:19 - loss: 0.4838 - acc: 0.791 - ETA: 6:17 - loss: 0.4870 - acc: 0.789 - ETA: 6:15 - loss: 0.4869 - acc: 0.790 - ETA: 6:13 - loss: 0.4851 - acc: 0.791 - ETA: 6:11 - loss: 0.4846 - acc: 0.791 - ETA: 6:09 - loss: 0.4822 - acc: 0.792 - ETA: 6:07 - loss: 0.4809 - acc: 0.793 - ETA: 6:05 - loss: 0.4826 - acc: 0.793 - ETA: 6:03 - loss: 0.4865 - acc: 0.791 - ETA: 6:01 - loss: 0.4846 - acc: 0.793 - ETA: 5:59 - loss: 0.4868 - acc: 0.793 - ETA: 5:57 - loss: 0.4843 - acc: 0.794 - ETA: 5:55 - loss: 0.4826 - acc: 0.794 - ETA: 5:53 - loss: 0.4835 - acc: 0.793 - ETA: 5:51 - loss: 0.4838 - acc: 0.791 - ETA: 5:49 - loss: 0.4857 - acc: 0.789 - ETA: 5:48 - loss: 0.4848 - acc: 0.790 - ETA: 5:46 - loss: 0.4840 - acc: 0.790 - ETA: 5:44 - loss: 0.4838 - acc: 0.789 - ETA: 5:42 - loss: 0.4827 - acc: 0.790 - ETA: 5:40 - loss: 0.4831 - acc: 0.789 - ETA: 5:38 - loss: 0.4831 - acc: 0.789 - ETA: 5:36 - loss: 0.4818 - acc: 0.790 - ETA: 5:34 - loss: 0.4828 - acc: 0.788 - ETA: 5:32 - loss: 0.4827 - acc: 0.788 - ETA: 5:30 - loss: 0.4832 - acc: 0.787 - ETA: 5:28 - loss: 0.4822 - acc: 0.788 - ETA: 5:26 - loss: 0.4814 - acc: 0.788 - ETA: 5:24 - loss: 0.4821 - acc: 0.787 - ETA: 5:22 - loss: 0.4825 - acc: 0.786 - ETA: 5:20 - loss: 0.4828 - acc: 0.786 - ETA: 5:18 - loss: 0.4836 - acc: 0.784 - ETA: 5:16 - loss: 0.4822 - acc: 0.784 - ETA: 5:14 - loss: 0.4819 - acc: 0.783 - ETA: 5:12 - loss: 0.4813 - acc: 0.784 - ETA: 5:10 - loss: 0.4810 - acc: 0.785 - ETA: 5:08 - loss: 0.4804 - acc: 0.785 - ETA: 5:06 - loss: 0.4792 - acc: 0.786 - ETA: 5:04 - loss: 0.4770 - acc: 0.787 - ETA: 5:02 - loss: 0.4766 - acc: 0.788 - ETA: 5:00 - loss: 0.4778 - acc: 0.787 - ETA: 4:58 - loss: 0.4777 - acc: 0.786 - ETA: 4:56 - loss: 0.4754 - acc: 0.788 - ETA: 4:54 - loss: 0.4744 - acc: 0.788 - ETA: 4:52 - loss: 0.4768 - acc: 0.787 - ETA: 4:51 - loss: 0.4776 - acc: 0.785 - ETA: 4:49 - loss: 0.4768 - acc: 0.785 - ETA: 4:47 - loss: 0.4757 - acc: 0.787 - ETA: 4:45 - loss: 0.4766 - acc: 0.787 - ETA: 4:43 - loss: 0.4757 - acc: 0.788 - ETA: 4:41 - loss: 0.4762 - acc: 0.787 - ETA: 4:39 - loss: 0.4750 - acc: 0.788 - ETA: 4:37 - loss: 0.4741 - acc: 0.788 - ETA: 4:35 - loss: 0.4746 - acc: 0.789 - ETA: 4:33 - loss: 0.4762 - acc: 0.789 - ETA: 4:31 - loss: 0.4760 - acc: 0.788 - ETA: 4:29 - loss: 0.4761 - acc: 0.787 - ETA: 4:27 - loss: 0.4768 - acc: 0.786 - ETA: 4:25 - loss: 0.4750 - acc: 0.786 - ETA: 4:23 - loss: 0.4774 - acc: 0.786 - ETA: 4:21 - loss: 0.4769 - acc: 0.787 - ETA: 4:19 - loss: 0.4760 - acc: 0.788 - ETA: 4:17 - loss: 0.4765 - acc: 0.788 - ETA: 4:15 - loss: 0.4764 - acc: 0.788 - ETA: 4:13 - loss: 0.4773 - acc: 0.788 - ETA: 4:11 - loss: 0.4764 - acc: 0.788 - ETA: 4:09 - loss: 0.4768 - acc: 0.788 - ETA: 4:07 - loss: 0.4772 - acc: 0.787 - ETA: 4:05 - loss: 0.4756 - acc: 0.788 - ETA: 4:03 - loss: 0.4751 - acc: 0.788 - ETA: 4:01 - loss: 0.4737 - acc: 0.789 - ETA: 3:59 - loss: 0.4731 - acc: 0.790 - ETA: 3:57 - loss: 0.4738 - acc: 0.790 - ETA: 3:55 - loss: 0.4741 - acc: 0.789 - ETA: 3:53 - loss: 0.4727 - acc: 0.789 - ETA: 3:51 - loss: 0.4715 - acc: 0.790 - ETA: 3:49 - loss: 0.4714 - acc: 0.789 - ETA: 3:47 - loss: 0.4715 - acc: 0.789 - ETA: 3:45 - loss: 0.4699 - acc: 0.790 - ETA: 3:43 - loss: 0.4689 - acc: 0.790 - ETA: 3:41 - loss: 0.4680 - acc: 0.791 - ETA: 3:39 - loss: 0.4674 - acc: 0.790 - ETA: 3:37 - loss: 0.4660 - acc: 0.791 - ETA: 3:35 - loss: 0.4655 - acc: 0.791 - ETA: 3:33 - loss: 0.4652 - acc: 0.790 - ETA: 3:31 - loss: 0.4647 - acc: 0.790 - ETA: 3:29 - loss: 0.4644 - acc: 0.790 - ETA: 3:27 - loss: 0.4646 - acc: 0.789 - ETA: 3:25 - loss: 0.4671 - acc: 0.788 - ETA: 3:23 - loss: 0.4652 - acc: 0.7898"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 3:21 - loss: 0.4650 - acc: 0.790 - ETA: 3:19 - loss: 0.4678 - acc: 0.790 - ETA: 3:17 - loss: 0.4657 - acc: 0.791 - ETA: 3:15 - loss: 0.4645 - acc: 0.792 - ETA: 3:13 - loss: 0.4633 - acc: 0.793 - ETA: 3:12 - loss: 0.4640 - acc: 0.792 - ETA: 3:10 - loss: 0.4637 - acc: 0.792 - ETA: 3:08 - loss: 0.4631 - acc: 0.791 - ETA: 3:06 - loss: 0.4629 - acc: 0.791 - ETA: 3:04 - loss: 0.4615 - acc: 0.792 - ETA: 3:02 - loss: 0.4609 - acc: 0.793 - ETA: 3:00 - loss: 0.4604 - acc: 0.793 - ETA: 2:58 - loss: 0.4603 - acc: 0.793 - ETA: 2:56 - loss: 0.4591 - acc: 0.793 - ETA: 2:54 - loss: 0.4585 - acc: 0.793 - ETA: 2:52 - loss: 0.4574 - acc: 0.793 - ETA: 2:50 - loss: 0.4574 - acc: 0.794 - ETA: 2:48 - loss: 0.4556 - acc: 0.795 - ETA: 2:46 - loss: 0.4548 - acc: 0.795 - ETA: 2:44 - loss: 0.4540 - acc: 0.795 - ETA: 2:42 - loss: 0.4532 - acc: 0.796 - ETA: 2:40 - loss: 0.4538 - acc: 0.795 - ETA: 2:38 - loss: 0.4559 - acc: 0.795 - ETA: 2:36 - loss: 0.4557 - acc: 0.795 - ETA: 2:34 - loss: 0.4629 - acc: 0.793 - ETA: 2:32 - loss: 0.4613 - acc: 0.794 - ETA: 2:30 - loss: 0.4611 - acc: 0.793 - ETA: 2:28 - loss: 0.4619 - acc: 0.793 - ETA: 2:26 - loss: 0.4620 - acc: 0.792 - ETA: 2:24 - loss: 0.4626 - acc: 0.792 - ETA: 2:22 - loss: 0.4616 - acc: 0.793 - ETA: 2:20 - loss: 0.4605 - acc: 0.793 - ETA: 2:18 - loss: 0.4594 - acc: 0.794 - ETA: 2:16 - loss: 0.4586 - acc: 0.794 - ETA: 2:14 - loss: 0.4592 - acc: 0.792 - ETA: 2:12 - loss: 0.4586 - acc: 0.793 - ETA: 2:10 - loss: 0.4583 - acc: 0.793 - ETA: 2:08 - loss: 0.4586 - acc: 0.792 - ETA: 2:06 - loss: 0.4582 - acc: 0.791 - ETA: 2:04 - loss: 0.4579 - acc: 0.792 - ETA: 2:02 - loss: 0.4643 - acc: 0.791 - ETA: 2:00 - loss: 0.4649 - acc: 0.790 - ETA: 1:58 - loss: 0.4644 - acc: 0.791 - ETA: 1:56 - loss: 0.4640 - acc: 0.792 - ETA: 1:54 - loss: 0.4632 - acc: 0.792 - ETA: 1:52 - loss: 0.4646 - acc: 0.791 - ETA: 1:50 - loss: 0.4651 - acc: 0.790 - ETA: 1:48 - loss: 0.4642 - acc: 0.791 - ETA: 1:46 - loss: 0.4646 - acc: 0.791 - ETA: 1:44 - loss: 0.4652 - acc: 0.790 - ETA: 1:42 - loss: 0.4651 - acc: 0.790 - ETA: 1:40 - loss: 0.4656 - acc: 0.790 - ETA: 1:38 - loss: 0.4664 - acc: 0.788 - ETA: 1:36 - loss: 0.4653 - acc: 0.789 - ETA: 1:34 - loss: 0.4671 - acc: 0.789 - ETA: 1:33 - loss: 0.4670 - acc: 0.789 - ETA: 1:31 - loss: 0.4664 - acc: 0.789 - ETA: 1:29 - loss: 0.4668 - acc: 0.789 - ETA: 1:27 - loss: 0.4662 - acc: 0.789 - ETA: 1:25 - loss: 0.4656 - acc: 0.789 - ETA: 1:23 - loss: 0.4656 - acc: 0.789 - ETA: 1:21 - loss: 0.4662 - acc: 0.789 - ETA: 1:19 - loss: 0.4655 - acc: 0.789 - ETA: 1:17 - loss: 0.4657 - acc: 0.788 - ETA: 1:15 - loss: 0.4663 - acc: 0.788 - ETA: 1:13 - loss: 0.4670 - acc: 0.788 - ETA: 1:11 - loss: 0.4682 - acc: 0.787 - ETA: 1:09 - loss: 0.4673 - acc: 0.788 - ETA: 1:07 - loss: 0.4673 - acc: 0.788 - ETA: 1:05 - loss: 0.4669 - acc: 0.789 - ETA: 1:03 - loss: 0.4662 - acc: 0.789 - ETA: 1:01 - loss: 0.4656 - acc: 0.790 - ETA: 59s - loss: 0.4654 - acc: 0.790 - ETA: 57s - loss: 0.4649 - acc: 0.79 - ETA: 55s - loss: 0.4654 - acc: 0.78 - ETA: 53s - loss: 0.4658 - acc: 0.78 - ETA: 51s - loss: 0.4679 - acc: 0.78 - ETA: 49s - loss: 0.4684 - acc: 0.78 - ETA: 47s - loss: 0.4676 - acc: 0.78 - ETA: 45s - loss: 0.4679 - acc: 0.78 - ETA: 43s - loss: 0.4677 - acc: 0.78 - ETA: 41s - loss: 0.4674 - acc: 0.78 - ETA: 39s - loss: 0.4680 - acc: 0.78 - ETA: 37s - loss: 0.4682 - acc: 0.78 - ETA: 35s - loss: 0.4690 - acc: 0.78 - ETA: 33s - loss: 0.4709 - acc: 0.78 - ETA: 31s - loss: 0.4704 - acc: 0.78 - ETA: 29s - loss: 0.4705 - acc: 0.78 - ETA: 27s - loss: 0.4714 - acc: 0.78 - ETA: 25s - loss: 0.4705 - acc: 0.78 - ETA: 23s - loss: 0.4698 - acc: 0.78 - ETA: 21s - loss: 0.4703 - acc: 0.78 - ETA: 19s - loss: 0.4694 - acc: 0.78 - ETA: 17s - loss: 0.4711 - acc: 0.78 - ETA: 15s - loss: 0.4714 - acc: 0.78 - ETA: 13s - loss: 0.4714 - acc: 0.78 - ETA: 11s - loss: 0.4729 - acc: 0.78 - ETA: 9s - loss: 0.4726 - acc: 0.7860 - ETA: 7s - loss: 0.4725 - acc: 0.785 - ETA: 5s - loss: 0.4728 - acc: 0.785 - ETA: 3s - loss: 0.4735 - acc: 0.785 - ETA: 1s - loss: 0.4732 - acc: 0.785 - 634s 2s/step - loss: 0.4741 - acc: 0.7850 - val_loss: 0.8312 - val_acc: 0.5671\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.54861 to 0.56713, saving model to best_weight6.hdf5\n",
      "Epoch 9/10\n",
      "204/307 [==================>...........] - ETA: 10:40 - loss: 0.6840 - acc: 0.62 - ETA: 10:14 - loss: 0.7589 - acc: 0.56 - ETA: 10:06 - loss: 0.6787 - acc: 0.62 - ETA: 9:57 - loss: 0.6098 - acc: 0.6875 - ETA: 9:54 - loss: 0.5885 - acc: 0.700 - ETA: 9:50 - loss: 0.6032 - acc: 0.666 - ETA: 9:45 - loss: 0.5554 - acc: 0.696 - ETA: 9:50 - loss: 0.5350 - acc: 0.718 - ETA: 9:53 - loss: 0.5073 - acc: 0.736 - ETA: 9:51 - loss: 0.4962 - acc: 0.762 - ETA: 9:48 - loss: 0.4814 - acc: 0.772 - ETA: 9:47 - loss: 0.4752 - acc: 0.770 - ETA: 9:44 - loss: 0.4809 - acc: 0.778 - ETA: 9:43 - loss: 0.4758 - acc: 0.776 - ETA: 9:40 - loss: 0.4699 - acc: 0.783 - ETA: 9:38 - loss: 0.4596 - acc: 0.789 - ETA: 9:36 - loss: 0.4445 - acc: 0.801 - ETA: 9:33 - loss: 0.4428 - acc: 0.812 - ETA: 9:30 - loss: 0.4322 - acc: 0.822 - ETA: 9:28 - loss: 0.4318 - acc: 0.818 - ETA: 9:26 - loss: 0.4197 - acc: 0.827 - ETA: 9:24 - loss: 0.4122 - acc: 0.835 - ETA: 9:22 - loss: 0.4134 - acc: 0.837 - ETA: 9:21 - loss: 0.4149 - acc: 0.833 - ETA: 9:18 - loss: 0.4148 - acc: 0.835 - ETA: 9:16 - loss: 0.4071 - acc: 0.836 - ETA: 9:14 - loss: 0.4004 - acc: 0.838 - ETA: 9:12 - loss: 0.3963 - acc: 0.839 - ETA: 9:10 - loss: 0.3911 - acc: 0.844 - ETA: 9:08 - loss: 0.3883 - acc: 0.845 - ETA: 9:06 - loss: 0.3884 - acc: 0.842 - ETA: 9:04 - loss: 0.3867 - acc: 0.839 - ETA: 9:02 - loss: 0.3825 - acc: 0.840 - ETA: 9:00 - loss: 0.3879 - acc: 0.838 - ETA: 8:58 - loss: 0.3858 - acc: 0.839 - ETA: 8:56 - loss: 0.3813 - acc: 0.843 - ETA: 8:54 - loss: 0.3767 - acc: 0.848 - ETA: 8:52 - loss: 0.3802 - acc: 0.845 - ETA: 8:50 - loss: 0.3877 - acc: 0.846 - ETA: 8:48 - loss: 0.3820 - acc: 0.850 - ETA: 8:46 - loss: 0.3799 - acc: 0.847 - ETA: 8:44 - loss: 0.3830 - acc: 0.845 - ETA: 8:42 - loss: 0.3848 - acc: 0.845 - ETA: 8:40 - loss: 0.3799 - acc: 0.849 - ETA: 8:39 - loss: 0.3760 - acc: 0.850 - ETA: 8:37 - loss: 0.3755 - acc: 0.850 - ETA: 8:35 - loss: 0.3785 - acc: 0.848 - ETA: 8:34 - loss: 0.3833 - acc: 0.849 - ETA: 8:32 - loss: 0.3777 - acc: 0.852 - ETA: 8:31 - loss: 0.3741 - acc: 0.855 - ETA: 8:29 - loss: 0.3737 - acc: 0.855 - ETA: 8:27 - loss: 0.3790 - acc: 0.855 - ETA: 8:25 - loss: 0.3787 - acc: 0.853 - ETA: 8:23 - loss: 0.3738 - acc: 0.856 - ETA: 8:20 - loss: 0.3786 - acc: 0.852 - ETA: 8:18 - loss: 0.3787 - acc: 0.852 - ETA: 8:16 - loss: 0.3835 - acc: 0.850 - ETA: 8:14 - loss: 0.3797 - acc: 0.853 - ETA: 8:12 - loss: 0.3749 - acc: 0.855 - ETA: 8:10 - loss: 0.3769 - acc: 0.856 - ETA: 8:08 - loss: 0.3788 - acc: 0.856 - ETA: 8:06 - loss: 0.3765 - acc: 0.856 - ETA: 8:04 - loss: 0.3801 - acc: 0.857 - ETA: 8:02 - loss: 0.3771 - acc: 0.859 - ETA: 8:00 - loss: 0.3785 - acc: 0.857 - ETA: 7:58 - loss: 0.3760 - acc: 0.858 - ETA: 7:56 - loss: 0.3719 - acc: 0.860 - ETA: 7:54 - loss: 0.3755 - acc: 0.860 - ETA: 7:52 - loss: 0.3857 - acc: 0.860 - ETA: 7:50 - loss: 0.3884 - acc: 0.858 - ETA: 7:48 - loss: 0.3930 - acc: 0.855 - ETA: 7:46 - loss: 0.3913 - acc: 0.855 - ETA: 7:44 - loss: 0.3904 - acc: 0.857 - ETA: 7:42 - loss: 0.3922 - acc: 0.856 - ETA: 7:40 - loss: 0.3943 - acc: 0.855 - ETA: 7:38 - loss: 0.3929 - acc: 0.855 - ETA: 7:35 - loss: 0.3896 - acc: 0.857 - ETA: 7:34 - loss: 0.3897 - acc: 0.857 - ETA: 7:32 - loss: 0.3914 - acc: 0.856 - ETA: 7:30 - loss: 0.3916 - acc: 0.854 - ETA: 7:28 - loss: 0.3904 - acc: 0.856 - ETA: 7:26 - loss: 0.3897 - acc: 0.855 - ETA: 7:24 - loss: 0.3901 - acc: 0.855 - ETA: 7:22 - loss: 0.3864 - acc: 0.857 - ETA: 7:20 - loss: 0.3934 - acc: 0.857 - ETA: 7:18 - loss: 0.3925 - acc: 0.857 - ETA: 7:16 - loss: 0.3915 - acc: 0.857 - ETA: 7:14 - loss: 0.3891 - acc: 0.859 - ETA: 7:12 - loss: 0.3934 - acc: 0.856 - ETA: 7:10 - loss: 0.3946 - acc: 0.856 - ETA: 7:08 - loss: 0.3947 - acc: 0.855 - ETA: 7:06 - loss: 0.4036 - acc: 0.853 - ETA: 7:04 - loss: 0.4009 - acc: 0.854 - ETA: 7:02 - loss: 0.3981 - acc: 0.856 - ETA: 7:00 - loss: 0.3998 - acc: 0.856 - ETA: 6:58 - loss: 0.3979 - acc: 0.858 - ETA: 6:56 - loss: 0.3968 - acc: 0.859 - ETA: 6:54 - loss: 0.3985 - acc: 0.857 - ETA: 6:52 - loss: 0.3980 - acc: 0.857 - ETA: 6:50 - loss: 0.3956 - acc: 0.858 - ETA: 6:48 - loss: 0.3965 - acc: 0.858 - ETA: 6:46 - loss: 0.3969 - acc: 0.857 - ETA: 6:44 - loss: 0.3982 - acc: 0.856 - ETA: 6:42 - loss: 0.3982 - acc: 0.855 - ETA: 6:40 - loss: 0.3982 - acc: 0.856 - ETA: 6:38 - loss: 0.3962 - acc: 0.857 - ETA: 6:36 - loss: 0.4009 - acc: 0.854 - ETA: 6:34 - loss: 0.3995 - acc: 0.854 - ETA: 6:32 - loss: 0.3988 - acc: 0.853 - ETA: 6:30 - loss: 0.3987 - acc: 0.852 - ETA: 6:28 - loss: 0.3981 - acc: 0.852 - ETA: 6:26 - loss: 0.4154 - acc: 0.851 - ETA: 6:24 - loss: 0.4146 - acc: 0.851 - ETA: 6:22 - loss: 0.4169 - acc: 0.850 - ETA: 6:20 - loss: 0.4173 - acc: 0.850 - ETA: 6:18 - loss: 0.4164 - acc: 0.849 - ETA: 6:16 - loss: 0.4144 - acc: 0.850 - ETA: 6:14 - loss: 0.4128 - acc: 0.851 - ETA: 6:12 - loss: 0.4152 - acc: 0.850 - ETA: 6:10 - loss: 0.4154 - acc: 0.850 - ETA: 6:08 - loss: 0.4141 - acc: 0.850 - ETA: 6:06 - loss: 0.4133 - acc: 0.850 - ETA: 6:04 - loss: 0.4152 - acc: 0.849 - ETA: 6:02 - loss: 0.4141 - acc: 0.850 - ETA: 6:00 - loss: 0.4157 - acc: 0.849 - ETA: 5:58 - loss: 0.4178 - acc: 0.849 - ETA: 5:56 - loss: 0.4165 - acc: 0.850 - ETA: 5:54 - loss: 0.4156 - acc: 0.851 - ETA: 5:52 - loss: 0.4136 - acc: 0.852 - ETA: 5:50 - loss: 0.4124 - acc: 0.852 - ETA: 5:48 - loss: 0.4117 - acc: 0.853 - ETA: 5:46 - loss: 0.4103 - acc: 0.854 - ETA: 5:44 - loss: 0.4096 - acc: 0.854 - ETA: 5:42 - loss: 0.4076 - acc: 0.855 - ETA: 5:40 - loss: 0.4072 - acc: 0.856 - ETA: 5:38 - loss: 0.4103 - acc: 0.856 - ETA: 5:36 - loss: 0.4100 - acc: 0.856 - ETA: 5:34 - loss: 0.4131 - acc: 0.854 - ETA: 5:32 - loss: 0.4123 - acc: 0.854 - ETA: 5:30 - loss: 0.4112 - acc: 0.855 - ETA: 5:28 - loss: 0.4105 - acc: 0.855 - ETA: 5:26 - loss: 0.4104 - acc: 0.855 - ETA: 5:24 - loss: 0.4104 - acc: 0.854 - ETA: 5:22 - loss: 0.4116 - acc: 0.854 - ETA: 5:20 - loss: 0.4095 - acc: 0.855 - ETA: 5:18 - loss: 0.4092 - acc: 0.855 - ETA: 5:16 - loss: 0.4087 - acc: 0.854 - ETA: 5:14 - loss: 0.4069 - acc: 0.855 - ETA: 5:12 - loss: 0.4054 - acc: 0.856 - ETA: 5:10 - loss: 0.4046 - acc: 0.856 - ETA: 5:08 - loss: 0.4039 - acc: 0.856 - ETA: 5:06 - loss: 0.4023 - acc: 0.856 - ETA: 5:04 - loss: 0.4055 - acc: 0.856 - ETA: 5:02 - loss: 0.4038 - acc: 0.857 - ETA: 5:00 - loss: 0.4026 - acc: 0.857 - ETA: 4:58 - loss: 0.4020 - acc: 0.857 - ETA: 4:56 - loss: 0.4018 - acc: 0.856 - ETA: 4:54 - loss: 0.4025 - acc: 0.856 - ETA: 4:53 - loss: 0.4011 - acc: 0.856 - ETA: 4:50 - loss: 0.4000 - acc: 0.857 - ETA: 4:49 - loss: 0.3989 - acc: 0.857 - ETA: 4:47 - loss: 0.4010 - acc: 0.855 - ETA: 4:45 - loss: 0.4002 - acc: 0.855 - ETA: 4:43 - loss: 0.3995 - acc: 0.856 - ETA: 4:41 - loss: 0.3993 - acc: 0.856 - ETA: 4:39 - loss: 0.4001 - acc: 0.856 - ETA: 4:37 - loss: 0.3990 - acc: 0.856 - ETA: 4:35 - loss: 0.3977 - acc: 0.856 - ETA: 4:33 - loss: 0.3963 - acc: 0.857 - ETA: 4:31 - loss: 0.3950 - acc: 0.858 - ETA: 4:29 - loss: 0.3945 - acc: 0.858 - ETA: 4:27 - loss: 0.3940 - acc: 0.857 - ETA: 4:25 - loss: 0.3950 - acc: 0.857 - ETA: 4:23 - loss: 0.3944 - acc: 0.857 - ETA: 4:21 - loss: 0.3955 - acc: 0.857 - ETA: 4:19 - loss: 0.3948 - acc: 0.857 - ETA: 4:17 - loss: 0.3937 - acc: 0.857 - ETA: 4:15 - loss: 0.3935 - acc: 0.856 - ETA: 4:13 - loss: 0.3937 - acc: 0.856 - ETA: 4:11 - loss: 0.3924 - acc: 0.856 - ETA: 4:09 - loss: 0.3921 - acc: 0.855 - ETA: 4:07 - loss: 0.3918 - acc: 0.855 - ETA: 4:05 - loss: 0.3915 - acc: 0.855 - ETA: 4:03 - loss: 0.3920 - acc: 0.854 - ETA: 4:01 - loss: 0.3917 - acc: 0.854 - ETA: 3:59 - loss: 0.3923 - acc: 0.854 - ETA: 3:57 - loss: 0.3904 - acc: 0.855 - ETA: 3:55 - loss: 0.3891 - acc: 0.856 - ETA: 3:53 - loss: 0.3883 - acc: 0.857 - ETA: 3:51 - loss: 0.3873 - acc: 0.857 - ETA: 3:49 - loss: 0.3871 - acc: 0.858 - ETA: 3:47 - loss: 0.3870 - acc: 0.858 - ETA: 3:45 - loss: 0.3860 - acc: 0.858 - ETA: 3:43 - loss: 0.3878 - acc: 0.857 - ETA: 3:41 - loss: 0.3878 - acc: 0.858 - ETA: 3:39 - loss: 0.3884 - acc: 0.857 - ETA: 3:37 - loss: 0.3876 - acc: 0.858 - ETA: 3:35 - loss: 0.3860 - acc: 0.859 - ETA: 3:33 - loss: 0.3852 - acc: 0.859 - ETA: 3:31 - loss: 0.3859 - acc: 0.859 - ETA: 3:29 - loss: 0.3849 - acc: 0.860 - ETA: 3:27 - loss: 0.3854 - acc: 0.858 - ETA: 3:25 - loss: 0.3842 - acc: 0.859 - ETA: 3:23 - loss: 0.3839 - acc: 0.8597307/307 [==============================] - ETA: 3:21 - loss: 0.3824 - acc: 0.860 - ETA: 3:19 - loss: 0.3813 - acc: 0.861 - ETA: 3:17 - loss: 0.3814 - acc: 0.861 - ETA: 3:15 - loss: 0.3816 - acc: 0.860 - ETA: 3:13 - loss: 0.3828 - acc: 0.859 - ETA: 3:11 - loss: 0.3816 - acc: 0.860 - ETA: 3:09 - loss: 0.3815 - acc: 0.859 - ETA: 3:07 - loss: 0.3809 - acc: 0.859 - ETA: 3:05 - loss: 0.3805 - acc: 0.859 - ETA: 3:03 - loss: 0.3805 - acc: 0.859 - ETA: 3:01 - loss: 0.3789 - acc: 0.860 - ETA: 3:00 - loss: 0.3773 - acc: 0.861 - ETA: 2:58 - loss: 0.3782 - acc: 0.860 - ETA: 2:56 - loss: 0.3771 - acc: 0.860 - ETA: 2:54 - loss: 0.3775 - acc: 0.860 - ETA: 2:52 - loss: 0.3775 - acc: 0.859 - ETA: 2:50 - loss: 0.3770 - acc: 0.859 - ETA: 2:48 - loss: 0.3762 - acc: 0.859 - ETA: 2:46 - loss: 0.3751 - acc: 0.860 - ETA: 2:44 - loss: 0.3743 - acc: 0.860 - ETA: 2:42 - loss: 0.3748 - acc: 0.860 - ETA: 2:40 - loss: 0.3784 - acc: 0.860 - ETA: 2:38 - loss: 0.3777 - acc: 0.860 - ETA: 2:36 - loss: 0.3771 - acc: 0.860 - ETA: 2:34 - loss: 0.3772 - acc: 0.859 - ETA: 2:32 - loss: 0.3774 - acc: 0.859 - ETA: 2:30 - loss: 0.3766 - acc: 0.859 - ETA: 2:28 - loss: 0.3765 - acc: 0.859 - ETA: 2:26 - loss: 0.3776 - acc: 0.859 - ETA: 2:24 - loss: 0.3773 - acc: 0.859 - ETA: 2:22 - loss: 0.3783 - acc: 0.859 - ETA: 2:20 - loss: 0.3776 - acc: 0.859 - ETA: 2:18 - loss: 0.3780 - acc: 0.858 - ETA: 2:16 - loss: 0.3778 - acc: 0.858 - ETA: 2:14 - loss: 0.3768 - acc: 0.859 - ETA: 2:12 - loss: 0.3766 - acc: 0.858 - ETA: 2:10 - loss: 0.3769 - acc: 0.858 - ETA: 2:08 - loss: 0.3761 - acc: 0.859 - ETA: 2:06 - loss: 0.3785 - acc: 0.858 - ETA: 2:04 - loss: 0.3796 - acc: 0.858 - ETA: 2:02 - loss: 0.3785 - acc: 0.858 - ETA: 2:00 - loss: 0.3792 - acc: 0.858 - ETA: 1:58 - loss: 0.3789 - acc: 0.858 - ETA: 1:56 - loss: 0.3793 - acc: 0.858 - ETA: 1:54 - loss: 0.3790 - acc: 0.858 - ETA: 1:52 - loss: 0.3778 - acc: 0.859 - ETA: 1:50 - loss: 0.3778 - acc: 0.859 - ETA: 1:48 - loss: 0.3774 - acc: 0.859 - ETA: 1:46 - loss: 0.3763 - acc: 0.859 - ETA: 1:44 - loss: 0.3753 - acc: 0.859 - ETA: 1:42 - loss: 0.3750 - acc: 0.859 - ETA: 1:40 - loss: 0.3748 - acc: 0.859 - ETA: 1:38 - loss: 0.3759 - acc: 0.858 - ETA: 1:36 - loss: 0.3768 - acc: 0.858 - ETA: 1:34 - loss: 0.3784 - acc: 0.858 - ETA: 1:32 - loss: 0.3778 - acc: 0.858 - ETA: 1:30 - loss: 0.3771 - acc: 0.858 - ETA: 1:28 - loss: 0.3766 - acc: 0.858 - ETA: 1:26 - loss: 0.3774 - acc: 0.857 - ETA: 1:25 - loss: 0.3768 - acc: 0.858 - ETA: 1:23 - loss: 0.3773 - acc: 0.857 - ETA: 1:21 - loss: 0.3783 - acc: 0.857 - ETA: 1:19 - loss: 0.3779 - acc: 0.857 - ETA: 1:17 - loss: 0.3776 - acc: 0.857 - ETA: 1:15 - loss: 0.3771 - acc: 0.857 - ETA: 1:13 - loss: 0.3787 - acc: 0.856 - ETA: 1:11 - loss: 0.3816 - acc: 0.855 - ETA: 1:09 - loss: 0.3806 - acc: 0.855 - ETA: 1:07 - loss: 0.3810 - acc: 0.855 - ETA: 1:05 - loss: 0.3810 - acc: 0.854 - ETA: 1:03 - loss: 0.3814 - acc: 0.855 - ETA: 1:01 - loss: 0.3813 - acc: 0.854 - ETA: 59s - loss: 0.3817 - acc: 0.854 - ETA: 57s - loss: 0.3819 - acc: 0.85 - ETA: 55s - loss: 0.3811 - acc: 0.85 - ETA: 53s - loss: 0.3829 - acc: 0.85 - ETA: 51s - loss: 0.3823 - acc: 0.85 - ETA: 49s - loss: 0.3821 - acc: 0.85 - ETA: 47s - loss: 0.3823 - acc: 0.85 - ETA: 45s - loss: 0.3817 - acc: 0.85 - ETA: 43s - loss: 0.3807 - acc: 0.85 - ETA: 41s - loss: 0.3804 - acc: 0.85 - ETA: 39s - loss: 0.3800 - acc: 0.85 - ETA: 37s - loss: 0.3797 - acc: 0.85 - ETA: 35s - loss: 0.3793 - acc: 0.85 - ETA: 33s - loss: 0.3789 - acc: 0.85 - ETA: 31s - loss: 0.3791 - acc: 0.85 - ETA: 29s - loss: 0.3793 - acc: 0.85 - ETA: 27s - loss: 0.3795 - acc: 0.85 - ETA: 25s - loss: 0.3785 - acc: 0.85 - ETA: 23s - loss: 0.3783 - acc: 0.85 - ETA: 21s - loss: 0.3782 - acc: 0.85 - ETA: 19s - loss: 0.3779 - acc: 0.85 - ETA: 17s - loss: 0.3774 - acc: 0.85 - ETA: 15s - loss: 0.3771 - acc: 0.85 - ETA: 13s - loss: 0.3805 - acc: 0.85 - ETA: 11s - loss: 0.3808 - acc: 0.85 - ETA: 9s - loss: 0.3806 - acc: 0.8539 - ETA: 7s - loss: 0.3811 - acc: 0.853 - ETA: 5s - loss: 0.3808 - acc: 0.853 - ETA: 3s - loss: 0.3816 - acc: 0.853 - ETA: 1s - loss: 0.3820 - acc: 0.852 - 633s 2s/step - loss: 0.3815 - acc: 0.8530 - val_loss: 1.1953 - val_acc: 0.5278\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204/307 [==================>...........] - ETA: 10:02 - loss: 0.4190 - acc: 0.75 - ETA: 9:58 - loss: 0.5824 - acc: 0.6250 - ETA: 7:37 - loss: 0.6114 - acc: 0.750 - ETA: 8:07 - loss: 0.4896 - acc: 0.812 - ETA: 8:26 - loss: 0.5561 - acc: 0.775 - ETA: 8:38 - loss: 0.5653 - acc: 0.770 - ETA: 8:44 - loss: 0.5893 - acc: 0.732 - ETA: 8:49 - loss: 0.5683 - acc: 0.750 - ETA: 8:58 - loss: 0.5259 - acc: 0.777 - ETA: 9:06 - loss: 0.5231 - acc: 0.775 - ETA: 9:08 - loss: 0.5007 - acc: 0.784 - ETA: 9:09 - loss: 0.4983 - acc: 0.770 - ETA: 9:10 - loss: 0.4820 - acc: 0.788 - ETA: 9:10 - loss: 0.4582 - acc: 0.803 - ETA: 9:09 - loss: 0.4425 - acc: 0.816 - ETA: 9:09 - loss: 0.4270 - acc: 0.820 - ETA: 9:09 - loss: 0.4162 - acc: 0.823 - ETA: 9:09 - loss: 0.4076 - acc: 0.826 - ETA: 9:08 - loss: 0.3939 - acc: 0.835 - ETA: 9:07 - loss: 0.3799 - acc: 0.843 - ETA: 9:07 - loss: 0.3762 - acc: 0.839 - ETA: 8:48 - loss: 0.3599 - acc: 0.846 - ETA: 8:48 - loss: 0.3472 - acc: 0.853 - ETA: 8:47 - loss: 0.3473 - acc: 0.849 - ETA: 8:47 - loss: 0.3389 - acc: 0.855 - ETA: 8:46 - loss: 0.3332 - acc: 0.855 - ETA: 8:45 - loss: 0.3387 - acc: 0.851 - ETA: 8:44 - loss: 0.3291 - acc: 0.857 - ETA: 8:43 - loss: 0.3263 - acc: 0.857 - ETA: 8:42 - loss: 0.3171 - acc: 0.862 - ETA: 8:41 - loss: 0.3102 - acc: 0.866 - ETA: 8:40 - loss: 0.3139 - acc: 0.867 - ETA: 8:39 - loss: 0.3111 - acc: 0.867 - ETA: 8:38 - loss: 0.3105 - acc: 0.867 - ETA: 8:37 - loss: 0.3329 - acc: 0.860 - ETA: 8:36 - loss: 0.3276 - acc: 0.861 - ETA: 8:35 - loss: 0.3233 - acc: 0.861 - ETA: 8:34 - loss: 0.3221 - acc: 0.861 - ETA: 8:33 - loss: 0.3224 - acc: 0.859 - ETA: 8:32 - loss: 0.3206 - acc: 0.862 - ETA: 8:31 - loss: 0.3140 - acc: 0.865 - ETA: 8:30 - loss: 0.3273 - acc: 0.866 - ETA: 8:29 - loss: 0.3220 - acc: 0.869 - ETA: 8:27 - loss: 0.3214 - acc: 0.869 - ETA: 8:25 - loss: 0.3220 - acc: 0.866 - ETA: 8:24 - loss: 0.3176 - acc: 0.869 - ETA: 8:22 - loss: 0.3177 - acc: 0.872 - ETA: 8:20 - loss: 0.3155 - acc: 0.875 - ETA: 8:18 - loss: 0.3131 - acc: 0.875 - ETA: 8:17 - loss: 0.3091 - acc: 0.875 - ETA: 8:15 - loss: 0.3200 - acc: 0.870 - ETA: 8:13 - loss: 0.3239 - acc: 0.870 - ETA: 8:11 - loss: 0.3199 - acc: 0.872 - ETA: 8:09 - loss: 0.3183 - acc: 0.872 - ETA: 8:08 - loss: 0.3170 - acc: 0.872 - ETA: 8:06 - loss: 0.3155 - acc: 0.875 - ETA: 8:04 - loss: 0.3146 - acc: 0.875 - ETA: 8:02 - loss: 0.3095 - acc: 0.877 - ETA: 8:01 - loss: 0.3049 - acc: 0.879 - ETA: 7:59 - loss: 0.3088 - acc: 0.877 - ETA: 7:57 - loss: 0.3052 - acc: 0.879 - ETA: 7:55 - loss: 0.3040 - acc: 0.879 - ETA: 7:53 - loss: 0.3029 - acc: 0.877 - ETA: 7:51 - loss: 0.3012 - acc: 0.877 - ETA: 7:50 - loss: 0.3044 - acc: 0.876 - ETA: 7:48 - loss: 0.3223 - acc: 0.871 - ETA: 7:46 - loss: 0.3208 - acc: 0.873 - ETA: 7:44 - loss: 0.3219 - acc: 0.873 - ETA: 7:42 - loss: 0.3213 - acc: 0.873 - ETA: 7:40 - loss: 0.3229 - acc: 0.871 - ETA: 7:38 - loss: 0.3186 - acc: 0.873 - ETA: 7:36 - loss: 0.3148 - acc: 0.875 - ETA: 7:35 - loss: 0.3162 - acc: 0.875 - ETA: 7:33 - loss: 0.3204 - acc: 0.875 - ETA: 7:31 - loss: 0.3194 - acc: 0.875 - ETA: 7:29 - loss: 0.3153 - acc: 0.876 - ETA: 7:27 - loss: 0.3151 - acc: 0.875 - ETA: 7:25 - loss: 0.3148 - acc: 0.875 - ETA: 7:23 - loss: 0.3132 - acc: 0.875 - ETA: 7:21 - loss: 0.3457 - acc: 0.873 - ETA: 7:20 - loss: 0.3443 - acc: 0.873 - ETA: 7:18 - loss: 0.3425 - acc: 0.875 - ETA: 7:16 - loss: 0.3542 - acc: 0.873 - ETA: 7:14 - loss: 0.3528 - acc: 0.875 - ETA: 7:12 - loss: 0.3502 - acc: 0.876 - ETA: 7:10 - loss: 0.3507 - acc: 0.875 - ETA: 7:08 - loss: 0.3483 - acc: 0.876 - ETA: 7:07 - loss: 0.3480 - acc: 0.875 - ETA: 7:05 - loss: 0.3530 - acc: 0.873 - ETA: 7:03 - loss: 0.3556 - acc: 0.872 - ETA: 7:01 - loss: 0.3646 - acc: 0.868 - ETA: 6:59 - loss: 0.3629 - acc: 0.868 - ETA: 6:57 - loss: 0.3614 - acc: 0.868 - ETA: 6:55 - loss: 0.3601 - acc: 0.868 - ETA: 6:53 - loss: 0.3589 - acc: 0.869 - ETA: 6:51 - loss: 0.3581 - acc: 0.869 - ETA: 6:50 - loss: 0.3557 - acc: 0.871 - ETA: 6:48 - loss: 0.3545 - acc: 0.872 - ETA: 6:46 - loss: 0.3525 - acc: 0.873 - ETA: 6:44 - loss: 0.3521 - acc: 0.873 - ETA: 6:42 - loss: 0.3628 - acc: 0.871 - ETA: 6:40 - loss: 0.3612 - acc: 0.872 - ETA: 6:38 - loss: 0.3622 - acc: 0.872 - ETA: 6:36 - loss: 0.3612 - acc: 0.873 - ETA: 6:34 - loss: 0.3594 - acc: 0.875 - ETA: 6:32 - loss: 0.3614 - acc: 0.873 - ETA: 6:30 - loss: 0.3607 - acc: 0.873 - ETA: 6:28 - loss: 0.3586 - acc: 0.873 - ETA: 6:26 - loss: 0.3580 - acc: 0.873 - ETA: 6:24 - loss: 0.3565 - acc: 0.875 - ETA: 6:23 - loss: 0.3605 - acc: 0.873 - ETA: 6:21 - loss: 0.3599 - acc: 0.873 - ETA: 6:19 - loss: 0.3590 - acc: 0.873 - ETA: 6:17 - loss: 0.3581 - acc: 0.875 - ETA: 6:15 - loss: 0.3573 - acc: 0.875 - ETA: 6:13 - loss: 0.3559 - acc: 0.876 - ETA: 6:11 - loss: 0.3538 - acc: 0.877 - ETA: 6:09 - loss: 0.3522 - acc: 0.878 - ETA: 6:07 - loss: 0.3577 - acc: 0.878 - ETA: 6:05 - loss: 0.3572 - acc: 0.878 - ETA: 6:03 - loss: 0.3574 - acc: 0.877 - ETA: 6:01 - loss: 0.3573 - acc: 0.877 - ETA: 5:59 - loss: 0.3566 - acc: 0.877 - ETA: 5:57 - loss: 0.3566 - acc: 0.876 - ETA: 5:55 - loss: 0.3566 - acc: 0.876 - ETA: 5:53 - loss: 0.3550 - acc: 0.877 - ETA: 5:51 - loss: 0.3537 - acc: 0.877 - ETA: 5:50 - loss: 0.3523 - acc: 0.877 - ETA: 5:48 - loss: 0.3506 - acc: 0.877 - ETA: 5:46 - loss: 0.3537 - acc: 0.877 - ETA: 5:44 - loss: 0.3526 - acc: 0.877 - ETA: 5:42 - loss: 0.3506 - acc: 0.878 - ETA: 5:40 - loss: 0.3495 - acc: 0.879 - ETA: 5:38 - loss: 0.3484 - acc: 0.879 - ETA: 5:36 - loss: 0.3477 - acc: 0.879 - ETA: 5:34 - loss: 0.3512 - acc: 0.878 - ETA: 5:32 - loss: 0.3515 - acc: 0.877 - ETA: 5:30 - loss: 0.3529 - acc: 0.877 - ETA: 5:28 - loss: 0.3523 - acc: 0.878 - ETA: 5:26 - loss: 0.3519 - acc: 0.878 - ETA: 5:24 - loss: 0.3510 - acc: 0.878 - ETA: 5:22 - loss: 0.3522 - acc: 0.876 - ETA: 5:20 - loss: 0.3529 - acc: 0.876 - ETA: 5:19 - loss: 0.3521 - acc: 0.876 - ETA: 5:17 - loss: 0.3504 - acc: 0.877 - ETA: 5:15 - loss: 0.3497 - acc: 0.878 - ETA: 5:13 - loss: 0.3476 - acc: 0.879 - ETA: 5:11 - loss: 0.3466 - acc: 0.880 - ETA: 5:09 - loss: 0.3450 - acc: 0.880 - ETA: 5:07 - loss: 0.3441 - acc: 0.881 - ETA: 5:05 - loss: 0.3430 - acc: 0.882 - ETA: 5:03 - loss: 0.3411 - acc: 0.883 - ETA: 5:01 - loss: 0.3401 - acc: 0.884 - ETA: 4:59 - loss: 0.3385 - acc: 0.884 - ETA: 4:57 - loss: 0.3381 - acc: 0.884 - ETA: 4:55 - loss: 0.3375 - acc: 0.884 - ETA: 4:53 - loss: 0.3367 - acc: 0.884 - ETA: 4:51 - loss: 0.3356 - acc: 0.885 - ETA: 4:49 - loss: 0.3346 - acc: 0.886 - ETA: 4:47 - loss: 0.3336 - acc: 0.886 - ETA: 4:45 - loss: 0.3323 - acc: 0.887 - ETA: 4:44 - loss: 0.3311 - acc: 0.888 - ETA: 4:42 - loss: 0.3306 - acc: 0.888 - ETA: 4:40 - loss: 0.3307 - acc: 0.888 - ETA: 4:38 - loss: 0.3328 - acc: 0.887 - ETA: 4:36 - loss: 0.3312 - acc: 0.887 - ETA: 4:34 - loss: 0.3295 - acc: 0.888 - ETA: 4:32 - loss: 0.3277 - acc: 0.889 - ETA: 4:30 - loss: 0.3262 - acc: 0.889 - ETA: 4:28 - loss: 0.3253 - acc: 0.890 - ETA: 4:26 - loss: 0.3240 - acc: 0.891 - ETA: 4:24 - loss: 0.3260 - acc: 0.890 - ETA: 4:22 - loss: 0.3252 - acc: 0.890 - ETA: 4:20 - loss: 0.3247 - acc: 0.890 - ETA: 4:18 - loss: 0.3239 - acc: 0.891 - ETA: 4:16 - loss: 0.3249 - acc: 0.890 - ETA: 4:14 - loss: 0.3259 - acc: 0.890 - ETA: 4:12 - loss: 0.3246 - acc: 0.891 - ETA: 4:10 - loss: 0.3236 - acc: 0.891 - ETA: 4:08 - loss: 0.3235 - acc: 0.891 - ETA: 4:06 - loss: 0.3236 - acc: 0.890 - ETA: 4:04 - loss: 0.3232 - acc: 0.890 - ETA: 4:02 - loss: 0.3226 - acc: 0.890 - ETA: 4:00 - loss: 0.3226 - acc: 0.890 - ETA: 3:59 - loss: 0.3217 - acc: 0.891 - ETA: 3:57 - loss: 0.3216 - acc: 0.890 - ETA: 3:55 - loss: 0.3212 - acc: 0.890 - ETA: 3:53 - loss: 0.3204 - acc: 0.891 - ETA: 3:51 - loss: 0.3210 - acc: 0.890 - ETA: 3:49 - loss: 0.3200 - acc: 0.890 - ETA: 3:47 - loss: 0.3195 - acc: 0.890 - ETA: 3:45 - loss: 0.3187 - acc: 0.891 - ETA: 3:43 - loss: 0.3186 - acc: 0.891 - ETA: 3:41 - loss: 0.3178 - acc: 0.891 - ETA: 3:39 - loss: 0.3171 - acc: 0.891 - ETA: 3:37 - loss: 0.3158 - acc: 0.892 - ETA: 3:35 - loss: 0.3161 - acc: 0.892 - ETA: 3:33 - loss: 0.3156 - acc: 0.892 - ETA: 3:31 - loss: 0.3169 - acc: 0.891 - ETA: 3:29 - loss: 0.3163 - acc: 0.891 - ETA: 3:27 - loss: 0.3168 - acc: 0.891 - ETA: 3:25 - loss: 0.3154 - acc: 0.892 - ETA: 3:23 - loss: 0.3161 - acc: 0.891 - ETA: 3:21 - loss: 0.3152 - acc: 0.8922"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/307 [==============================] - ETA: 3:19 - loss: 0.3144 - acc: 0.892 - ETA: 3:17 - loss: 0.3164 - acc: 0.892 - ETA: 3:16 - loss: 0.3153 - acc: 0.892 - ETA: 3:14 - loss: 0.3159 - acc: 0.892 - ETA: 3:12 - loss: 0.3192 - acc: 0.890 - ETA: 3:10 - loss: 0.3197 - acc: 0.889 - ETA: 3:08 - loss: 0.3187 - acc: 0.890 - ETA: 3:06 - loss: 0.3182 - acc: 0.890 - ETA: 3:04 - loss: 0.3200 - acc: 0.890 - ETA: 3:02 - loss: 0.3188 - acc: 0.890 - ETA: 3:00 - loss: 0.3181 - acc: 0.890 - ETA: 2:58 - loss: 0.3170 - acc: 0.891 - ETA: 2:56 - loss: 0.3188 - acc: 0.890 - ETA: 2:54 - loss: 0.3183 - acc: 0.891 - ETA: 2:52 - loss: 0.3198 - acc: 0.889 - ETA: 2:50 - loss: 0.3197 - acc: 0.889 - ETA: 2:48 - loss: 0.3196 - acc: 0.889 - ETA: 2:46 - loss: 0.3188 - acc: 0.890 - ETA: 2:44 - loss: 0.3177 - acc: 0.890 - ETA: 2:42 - loss: 0.3179 - acc: 0.890 - ETA: 2:40 - loss: 0.3166 - acc: 0.891 - ETA: 2:38 - loss: 0.3173 - acc: 0.890 - ETA: 2:36 - loss: 0.3161 - acc: 0.891 - ETA: 2:34 - loss: 0.3153 - acc: 0.891 - ETA: 2:33 - loss: 0.3140 - acc: 0.891 - ETA: 2:31 - loss: 0.3146 - acc: 0.891 - ETA: 2:29 - loss: 0.3142 - acc: 0.891 - ETA: 2:27 - loss: 0.3139 - acc: 0.891 - ETA: 2:25 - loss: 0.3174 - acc: 0.891 - ETA: 2:23 - loss: 0.3176 - acc: 0.891 - ETA: 2:21 - loss: 0.3165 - acc: 0.891 - ETA: 2:19 - loss: 0.3165 - acc: 0.890 - ETA: 2:17 - loss: 0.3153 - acc: 0.890 - ETA: 2:15 - loss: 0.3144 - acc: 0.891 - ETA: 2:13 - loss: 0.3142 - acc: 0.891 - ETA: 2:11 - loss: 0.3147 - acc: 0.891 - ETA: 2:09 - loss: 0.3157 - acc: 0.890 - ETA: 2:07 - loss: 0.3166 - acc: 0.890 - ETA: 2:05 - loss: 0.3171 - acc: 0.889 - ETA: 2:03 - loss: 0.3168 - acc: 0.889 - ETA: 2:01 - loss: 0.3179 - acc: 0.888 - ETA: 1:59 - loss: 0.3171 - acc: 0.889 - ETA: 1:57 - loss: 0.3184 - acc: 0.888 - ETA: 1:55 - loss: 0.3178 - acc: 0.889 - ETA: 1:53 - loss: 0.3182 - acc: 0.888 - ETA: 1:51 - loss: 0.3183 - acc: 0.888 - ETA: 1:49 - loss: 0.3179 - acc: 0.888 - ETA: 1:47 - loss: 0.3175 - acc: 0.888 - ETA: 1:45 - loss: 0.3165 - acc: 0.888 - ETA: 1:43 - loss: 0.3159 - acc: 0.889 - ETA: 1:41 - loss: 0.3147 - acc: 0.889 - ETA: 1:40 - loss: 0.3146 - acc: 0.889 - ETA: 1:38 - loss: 0.3137 - acc: 0.889 - ETA: 1:36 - loss: 0.3135 - acc: 0.889 - ETA: 1:34 - loss: 0.3125 - acc: 0.889 - ETA: 1:32 - loss: 0.3115 - acc: 0.889 - ETA: 1:30 - loss: 0.3112 - acc: 0.889 - ETA: 1:28 - loss: 0.3123 - acc: 0.888 - ETA: 1:26 - loss: 0.3116 - acc: 0.889 - ETA: 1:24 - loss: 0.3119 - acc: 0.888 - ETA: 1:22 - loss: 0.3110 - acc: 0.889 - ETA: 1:20 - loss: 0.3126 - acc: 0.888 - ETA: 1:18 - loss: 0.3131 - acc: 0.888 - ETA: 1:16 - loss: 0.3125 - acc: 0.888 - ETA: 1:14 - loss: 0.3118 - acc: 0.888 - ETA: 1:12 - loss: 0.3117 - acc: 0.888 - ETA: 1:10 - loss: 0.3108 - acc: 0.889 - ETA: 1:08 - loss: 0.3103 - acc: 0.889 - ETA: 1:06 - loss: 0.3105 - acc: 0.889 - ETA: 1:04 - loss: 0.3099 - acc: 0.889 - ETA: 1:02 - loss: 0.3092 - acc: 0.890 - ETA: 1:00 - loss: 0.3081 - acc: 0.890 - ETA: 58s - loss: 0.3094 - acc: 0.889 - ETA: 56s - loss: 0.3092 - acc: 0.88 - ETA: 54s - loss: 0.3087 - acc: 0.88 - ETA: 52s - loss: 0.3085 - acc: 0.89 - ETA: 51s - loss: 0.3079 - acc: 0.89 - ETA: 49s - loss: 0.3073 - acc: 0.89 - ETA: 47s - loss: 0.3079 - acc: 0.89 - ETA: 45s - loss: 0.3082 - acc: 0.89 - ETA: 43s - loss: 0.3083 - acc: 0.89 - ETA: 41s - loss: 0.3082 - acc: 0.89 - ETA: 39s - loss: 0.3084 - acc: 0.89 - ETA: 37s - loss: 0.3076 - acc: 0.89 - ETA: 35s - loss: 0.3074 - acc: 0.89 - ETA: 33s - loss: 0.3071 - acc: 0.89 - ETA: 31s - loss: 0.3064 - acc: 0.89 - ETA: 29s - loss: 0.3065 - acc: 0.89 - ETA: 27s - loss: 0.3057 - acc: 0.89 - ETA: 25s - loss: 0.3056 - acc: 0.89 - ETA: 23s - loss: 0.3051 - acc: 0.89 - ETA: 21s - loss: 0.3042 - acc: 0.89 - ETA: 19s - loss: 0.3033 - acc: 0.89 - ETA: 17s - loss: 0.3027 - acc: 0.89 - ETA: 15s - loss: 0.3019 - acc: 0.89 - ETA: 13s - loss: 0.3025 - acc: 0.89 - ETA: 11s - loss: 0.3020 - acc: 0.89 - ETA: 9s - loss: 0.3017 - acc: 0.8920 - ETA: 7s - loss: 0.3027 - acc: 0.891 - ETA: 5s - loss: 0.3017 - acc: 0.891 - ETA: 3s - loss: 0.3026 - acc: 0.891 - ETA: 1s - loss: 0.3027 - acc: 0.890 - 629s 2s/step - loss: 0.3020 - acc: 0.8913 - val_loss: 1.2754 - val_acc: 0.5370\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xc007746f60>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model6.fit_generator(train_generator,\n",
    "                       steps_per_epoch=2457//8,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=10, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=433//8,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmod = load_model('best_weight3_mod.hdf5') # model3 is performing well on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n"
     ]
    }
   ],
   "source": [
    "#import glob\n",
    "#cmod = load_model('best_weight4.hdf5')\n",
    "tot = 0\n",
    "cor = 0\n",
    "for cl in range(3):\n",
    "    for fn in glob.glob('test/' + str(cl) + '/*'):\n",
    "        a_p = cv2.imread(fn)\n",
    "        a_p = cv2.resize(a_p,(224,224))\n",
    "        a_p = np.ndarray.astype(a_p,dtype='float32')\n",
    "        a_p /= 255\n",
    "        a_p = np.expand_dims(a_p,0)\n",
    "        pred = cmod.predict_classes(a_p)\n",
    "        if(cl == pred[0]):\n",
    "            cor+=1\n",
    "        elif(cl == 0 and pred[0] == 1):\n",
    "            cor+=1\n",
    "        elif(cl == 1 and pred[0] == 0):\n",
    "            cor+=1\n",
    "        tot += 1\n",
    "        #print(pred)\n",
    "print(cor/tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model3 best so far, 2 class 83.3%, 2 class 66.6% \n",
    "## model3 modified 3 class 53.3, 2 class 73.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.initializers import he_normal\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## model 3 modified\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Conv2D(16, kernel_size=(5, 5),   # kernel size :\n",
    "                 input_shape=(224,224,3)))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1))) # strides up :    strides down :\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model3.add(Activation('relu')) # sigmoid :      relu :      tanh : \n",
    "\n",
    "# LeakyReLU(alpha=0.1)\n",
    "#model3.add(Dropout(0.01))\n",
    "\n",
    "model3.add(Conv2D(32, (5, 5)))     # kernel size :\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model3.add(BatchNormalization()) # BN on :      BN off : \n",
    "model3.add(Activation('relu')) # sigmoid :      relu :      tanh :\n",
    "#model3.add(Dropout(0.02))\n",
    "\n",
    "        \n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(156))\n",
    "#model3.add(BatchNormalization())\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "#model3.add(Dense(64))\n",
    "#model3.add(Activation('relu'))\n",
    "\n",
    "#model3.add(Dense(32))\n",
    "#model3.add(Activation('relu'))\n",
    "\n",
    "#model3.add(Dense(16))\n",
    "#model3.add(Activation('sigmoid'))\n",
    "\n",
    "           \n",
    "model3.add(Dense(3))\n",
    "#model2.add(BatchNormalization())\n",
    "model3.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 220, 220, 16)      1216      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 219, 219, 16)      0         \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 219, 219, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 215, 215, 32)      12832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 107, 107, 32)      0         \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 107, 107, 32)      0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 366368)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 156)               57153564  \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 156)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 156)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 471       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 57,168,083\n",
      "Trainable params: 57,168,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAINING_DIR = 'train'\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2457 images belonging to 3 classes.\n",
      "Found 433 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "optm = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model3.compile(loss=categorical_crossentropy,\n",
    "              optimizer=optm,\n",
    "              metrics=['accuracy'])\n",
    "# checkpoint\n",
    "path = \"best_weight3_mod.hdf5\"\n",
    "checkpoint = ModelCheckpoint(path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "callbacks_list = [checkpoint, tbCallBack]\n",
    "\n",
    "\n",
    "\n",
    "data_generator = ImageDataGenerator(rescale=1./255, validation_split=0.15)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=19,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=(IMAGE_SIZE, IMAGE_SIZE), shuffle=True, seed=21,\n",
    "                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "135/135 [==============================] - 532s 4s/step - loss: 3.4944 - acc: 0.4119 - val_loss: 0.9946 - val_acc: 0.4844\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.48444, saving model to best_weight3_mod.hdf5\n",
      "Epoch 2/5\n",
      "135/135 [==============================] - 481s 4s/step - loss: 1.2343 - acc: 0.5215 - val_loss: 0.8453 - val_acc: 0.5733\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.48444 to 0.57333, saving model to best_weight3_mod.hdf5\n",
      "Epoch 3/5\n",
      "135/135 [==============================] - 453s 3s/step - loss: 1.0280 - acc: 0.5615 - val_loss: 0.8699 - val_acc: 0.5244\n",
      "\n",
      "Epoch 00003: val_acc did not improve\n",
      "Epoch 4/5\n",
      "135/135 [==============================] - 464s 3s/step - loss: 0.8694 - acc: 0.5763 - val_loss: 0.8535 - val_acc: 0.4889\n",
      "\n",
      "Epoch 00004: val_acc did not improve\n",
      "Epoch 5/5\n",
      "135/135 [==============================] - 439s 3s/step - loss: 0.7693 - acc: 0.6489 - val_loss: 0.7698 - val_acc: 0.5200\n",
      "\n",
      "Epoch 00005: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb022dfcbe0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 80, 80, 40\n",
    "c_weight = {0:1, 1:1, 2:2}\n",
    "\n",
    "model3.fit_generator(train_generator,\n",
    "                       steps_per_epoch=2168//16,\n",
    "                       validation_data=validation_generator, \n",
    "                       epochs=5, \n",
    "                       verbose=1,\n",
    "                       class_weight = c_weight,\n",
    "                       callbacks = callbacks_list,\n",
    "                       validation_steps=722//16,\n",
    "                       initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet = keras.applications.resnet50.ResNet50(include_top=False, weights='imagenet', input_shape=(224,224,3), pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv1_pad\n",
      "2 conv1\n",
      "3 bn_conv1\n",
      "4 activation_1\n",
      "5 max_pooling2d_1\n",
      "6 res2a_branch2a\n",
      "7 bn2a_branch2a\n",
      "8 activation_2\n",
      "9 res2a_branch2b\n",
      "10 bn2a_branch2b\n",
      "11 activation_3\n",
      "12 res2a_branch2c\n",
      "13 res2a_branch1\n",
      "14 bn2a_branch2c\n",
      "15 bn2a_branch1\n",
      "16 add_1\n",
      "17 activation_4\n",
      "18 res2b_branch2a\n",
      "19 bn2b_branch2a\n",
      "20 activation_5\n",
      "21 res2b_branch2b\n",
      "22 bn2b_branch2b\n",
      "23 activation_6\n",
      "24 res2b_branch2c\n",
      "25 bn2b_branch2c\n",
      "26 add_2\n",
      "27 activation_7\n",
      "28 res2c_branch2a\n",
      "29 bn2c_branch2a\n",
      "30 activation_8\n",
      "31 res2c_branch2b\n",
      "32 bn2c_branch2b\n",
      "33 activation_9\n",
      "34 res2c_branch2c\n",
      "35 bn2c_branch2c\n",
      "36 add_3\n",
      "37 activation_10\n",
      "38 res3a_branch2a\n",
      "39 bn3a_branch2a\n",
      "40 activation_11\n",
      "41 res3a_branch2b\n",
      "42 bn3a_branch2b\n",
      "43 activation_12\n",
      "44 res3a_branch2c\n",
      "45 res3a_branch1\n",
      "46 bn3a_branch2c\n",
      "47 bn3a_branch1\n",
      "48 add_4\n",
      "49 activation_13\n",
      "50 res3b_branch2a\n",
      "51 bn3b_branch2a\n",
      "52 activation_14\n",
      "53 res3b_branch2b\n",
      "54 bn3b_branch2b\n",
      "55 activation_15\n",
      "56 res3b_branch2c\n",
      "57 bn3b_branch2c\n",
      "58 add_5\n",
      "59 activation_16\n",
      "60 res3c_branch2a\n",
      "61 bn3c_branch2a\n",
      "62 activation_17\n",
      "63 res3c_branch2b\n",
      "64 bn3c_branch2b\n",
      "65 activation_18\n",
      "66 res3c_branch2c\n",
      "67 bn3c_branch2c\n",
      "68 add_6\n",
      "69 activation_19\n",
      "70 res3d_branch2a\n",
      "71 bn3d_branch2a\n",
      "72 activation_20\n",
      "73 res3d_branch2b\n",
      "74 bn3d_branch2b\n",
      "75 activation_21\n",
      "76 res3d_branch2c\n",
      "77 bn3d_branch2c\n",
      "78 add_7\n",
      "79 activation_22\n",
      "80 res4a_branch2a\n",
      "81 bn4a_branch2a\n",
      "82 activation_23\n",
      "83 res4a_branch2b\n",
      "84 bn4a_branch2b\n",
      "85 activation_24\n",
      "86 res4a_branch2c\n",
      "87 res4a_branch1\n",
      "88 bn4a_branch2c\n",
      "89 bn4a_branch1\n",
      "90 add_8\n",
      "91 activation_25\n",
      "92 res4b_branch2a\n",
      "93 bn4b_branch2a\n",
      "94 activation_26\n",
      "95 res4b_branch2b\n",
      "96 bn4b_branch2b\n",
      "97 activation_27\n",
      "98 res4b_branch2c\n",
      "99 bn4b_branch2c\n",
      "100 add_9\n",
      "101 activation_28\n",
      "102 res4c_branch2a\n",
      "103 bn4c_branch2a\n",
      "104 activation_29\n",
      "105 res4c_branch2b\n",
      "106 bn4c_branch2b\n",
      "107 activation_30\n",
      "108 res4c_branch2c\n",
      "109 bn4c_branch2c\n",
      "110 add_10\n",
      "111 activation_31\n",
      "112 res4d_branch2a\n",
      "113 bn4d_branch2a\n",
      "114 activation_32\n",
      "115 res4d_branch2b\n",
      "116 bn4d_branch2b\n",
      "117 activation_33\n",
      "118 res4d_branch2c\n",
      "119 bn4d_branch2c\n",
      "120 add_11\n",
      "121 activation_34\n",
      "122 res4e_branch2a\n",
      "123 bn4e_branch2a\n",
      "124 activation_35\n",
      "125 res4e_branch2b\n",
      "126 bn4e_branch2b\n",
      "127 activation_36\n",
      "128 res4e_branch2c\n",
      "129 bn4e_branch2c\n",
      "130 add_12\n",
      "131 activation_37\n",
      "132 res4f_branch2a\n",
      "133 bn4f_branch2a\n",
      "134 activation_38\n",
      "135 res4f_branch2b\n",
      "136 bn4f_branch2b\n",
      "137 activation_39\n",
      "138 res4f_branch2c\n",
      "139 bn4f_branch2c\n",
      "140 add_13\n",
      "141 activation_40\n",
      "142 res5a_branch2a\n",
      "143 bn5a_branch2a\n",
      "144 activation_41\n",
      "145 res5a_branch2b\n",
      "146 bn5a_branch2b\n",
      "147 activation_42\n",
      "148 res5a_branch2c\n",
      "149 res5a_branch1\n",
      "150 bn5a_branch2c\n",
      "151 bn5a_branch1\n",
      "152 add_14\n",
      "153 activation_43\n",
      "154 res5b_branch2a\n",
      "155 bn5b_branch2a\n",
      "156 activation_44\n",
      "157 res5b_branch2b\n",
      "158 bn5b_branch2b\n",
      "159 activation_45\n",
      "160 res5b_branch2c\n",
      "161 bn5b_branch2c\n",
      "162 add_15\n",
      "163 activation_46\n",
      "164 res5c_branch2a\n",
      "165 bn5c_branch2a\n",
      "166 activation_47\n",
      "167 res5c_branch2b\n",
      "168 bn5c_branch2b\n",
      "169 activation_48\n",
      "170 res5c_branch2c\n",
      "171 bn5c_branch2c\n",
      "172 add_16\n",
      "173 activation_49\n",
      "174 avg_pool\n",
      "175 global_max_pooling2d_1\n"
     ]
    }
   ],
   "source": [
    "for i, layers in enumerate(resnet.layers):\n",
    "    print(i, layers.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference:**\n",
    " 1. https://datascience.stackexchange.com/questions/25122/pre-trained-cnn-for-feature-extraction\n",
    " 2. https://keras.io/applications/\n",
    " 3. https://arxiv.org/pdf/1805.08974.pdf {Do Better ImageNet Models Transfer Better?}\n",
    " 4. https://github.com/johannesu/NASNet-keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
